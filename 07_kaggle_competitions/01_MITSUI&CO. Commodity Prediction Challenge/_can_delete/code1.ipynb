{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/target_pairs.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/train_labels.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/train.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/test.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/lagged_test_labels/test_labels_lag_1.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/lagged_test_labels/test_labels_lag_4.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/lagged_test_labels/test_labels_lag_3.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/lagged_test_labels/test_labels_lag_2.csv\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/mitsui_inference_server.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/mitsui_gateway.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/__init__.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/templates.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/base_gateway.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/relay.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/kaggle_evaluation.proto\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/__init__.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
    "/kaggle/input/mitsui-commodity-prediction-challenge/kaggle_evaluation/core/generated/__init__.py\n",
    "#Code Summary â€“ Hybrid Stagewise Ensemble for Mitsui Commodity Prediction\n",
    "\n",
    "#This script implements a multi-stage ensemble pipeline for the Kaggle Mitsui & Co. Commodity Prediction Challenge. It combines classical machine learning, deep learning, and gradient boosting in a stacked architecture designed for robust time-series forecasting of 424 target variables.\n",
    "\n",
    "#1. Configuration & Setup\n",
    "\n",
    "#Global Config (CFG):\n",
    "\n",
    "#Dataset path (/kaggle/input/mitsui-commodity-prediction-challenge/)\n",
    "\n",
    "#Random seed for reproducibility\n",
    "\n",
    "#Total number of targets: 424\n",
    "\n",
    "#Null filler: 0.0\n",
    "\n",
    "#Time Neural Network (TNN) parameters: epochs, lookback window, hidden units\n",
    "\n",
    "#Stage-2 LightGBM hyperparameters (learning rate, leaves, estimators, etc.)\n",
    "\n",
    "#Device: Uses GPU if available (cuda), otherwise CPU.\n",
    "\n",
    "#2. Data Preprocessing\n",
    "\n",
    "#Cleaning: Converts object columns to numeric, handles missing/inf values, fills NaN with 0.0.\n",
    "\n",
    "#Feature Engineering:\n",
    "\n",
    "#Adds lag features (1,2,3 steps)\n",
    "\n",
    "#Adds rolling mean/std features\n",
    "\n",
    "#Stabilization: Adds small Gaussian noise to flat predictions to avoid identical row outputs.\n",
    "\n",
    "#3. Model Components\n",
    "#ðŸ”¹ Extra Trees Regressor (ETR â€“ Stage 1 classical model)\n",
    "\n",
    "#Ensemble of randomized decision trees.\n",
    "\n",
    "#Provides fast, stable base forecasts.\n",
    "\n",
    "#ðŸ”¹ Time Neural Network (TNN â€“ Stage 1 deep model)\n",
    "\n",
    "#Depthwise Separable 1D Convolutions â†’ capture local temporal patterns.\n",
    "\n",
    "#Time Attention Layer â†’ learns dependencies across time steps.\n",
    "\n",
    "#Pooling & Dense Head â†’ compresses features and predicts 424 targets simultaneously.\n",
    "\n",
    "#Optimized using AdamW and SmoothL1Loss.\n",
    "\n",
    "#ðŸ”¹ Stage 2 LightGBM (Residual Learner)\n",
    "\n",
    "#Trains one LightGBM model per target (424 total).\n",
    "\n",
    "#Input: Stage-1 predictions + original features.\n",
    "\n",
    "#Learns residual corrections, boosting final accuracy.\n",
    "\n",
    "#4. Training Pipeline\n",
    "\n",
    "#Stage 1 (Base Models):\n",
    "\n",
    "#Trains Extra Trees + TNN on lagged features and labels.\n",
    "\n",
    "#Uses multiple random seeds for robustness â†’ ensemble averaging.\n",
    "\n",
    "#Produces out-of-sample Stage-1 predictions.\n",
    "\n",
    "#Stage 2 (Stacking & Residuals):\n",
    "\n",
    "#Computes residuals between Stage-1 predictions and true labels.\n",
    "\n",
    "#Trains 424 LightGBM models (one per target) on residuals.\n",
    "\n",
    "#Combines Stage-1 and Stage-2 for final predictions.\n",
    "\n",
    "#Evaluation Metrics:\n",
    "\n",
    "#Stage-1 and Stage-2 evaluated with RMSE and MAE.\n",
    "\n",
    "#Reports improvement after residual correction.\n",
    "\n",
    "#5. Prediction Workflow\n",
    "\n",
    "#Test data is cleaned, scaled, and lag features generated.\n",
    "\n",
    "#Stage-1 predictions are computed (ETR + TNN ensemble).\n",
    "\n",
    "#Stage-2 LightGBM models adjust residuals.\n",
    "\n",
    "#Final outputs are stabilized to avoid constant rows.\n",
    "\n",
    "#Returns prediction DataFrame with all 424 targets.\n",
    "\n",
    "#6. Inference Integration\n",
    "\n",
    "#Integrated with Kaggleâ€™s inference server:\n",
    "\n",
    "#inference_server.serve() when running in competition rerun mode.\n",
    "\n",
    "#inference_server.run_local_gateway() for local testing/debugging.#\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import warnings\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# Config & Globals\n",
    "# =========================\n",
    "class CFG:\n",
    "    path = \"/kaggle/input/mitsui-commodity-prediction-challenge/\"\n",
    "    seed = 42\n",
    "    total_targets = 424\n",
    "    targets = [f\"target_{i}\" for i in range(total_targets)]\n",
    "    solution_null_filler = 0.0\n",
    "\n",
    "    tnn_epochs = int(os.getenv(\"KAGGLE_TNN_EPOCHS\", \"2\"))\n",
    "    tnn_lookback = int(os.getenv(\"KAGGLE_TNN_LOOKBACK\", \"16\"))\n",
    "    tnn_hidden = int(os.getenv(\"KAGGLE_TNN_HIDDEN\", \"256\"))\n",
    "\n",
    "    n_stage1_seeds = 3\n",
    "    stage2_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": -1,\n",
    "        \"n_estimators\": 300,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"num_leaves\": 64\n",
    "    }\n",
    "\n",
    "def _set_seed(s):\n",
    "    import random\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.cuda.manual_seed_all(s)\n",
    "\n",
    "_set_seed(CFG.seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Globals\n",
    "STAGE1_MODELS = []\n",
    "STAGE2_MODEL = None\n",
    "FEATURES = []\n",
    "MODEL_READY = False\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def clean_dataframe(df):\n",
    "    if df is None or df.empty: return df\n",
    "    obj_cols = df.select_dtypes(include=\"object\").columns\n",
    "    for col in obj_cols:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0.0, inplace=True)\n",
    "    return df\n",
    "\n",
    "def preprocess_columns(df):\n",
    "    df = clean_dataframe(df)\n",
    "    return add_lag_features(df)\n",
    "\n",
    "def add_lag_features(df, lags=[1,2,3]):\n",
    "    if \"date_id\" not in df.columns: return df\n",
    "    df = df.sort_values(\"date_id\").reset_index(drop=True)\n",
    "    for col in df.columns:\n",
    "        if col == \"date_id\": continue\n",
    "        for lag in lags:\n",
    "            df[f\"{col}_lag{lag}\"] = df[col].shift(lag).fillna(0.0)\n",
    "        df[f\"{col}_roll_mean3\"] = df[col].rolling(3).mean().fillna(0.0)\n",
    "        df[f\"{col}_roll_std3\"] = df[col].rolling(3).std().fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def _stabilize_and_detie_rows(out_df, date_ids=None):\n",
    "    out_df = out_df.astype(np.float32)  # ensure float32\n",
    "    out_df[:] = np.nan_to_num(out_df.values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    n_rows, n_cols = out_df.shape\n",
    "    if date_ids is None:\n",
    "        date_ids = np.zeros(n_rows, dtype=int)\n",
    "    vals = out_df.to_numpy(np.float32)\n",
    "    row_stds = np.std(vals, axis=1)\n",
    "    flat_mask = row_stds < 1e-15\n",
    "    if np.any(flat_mask):\n",
    "        for r_idx in np.where(flat_mask)[0]:\n",
    "            rng = np.random.default_rng(int(date_ids[r_idx]) + 131071)\n",
    "            noise = rng.normal(loc=0.0, scale=1.0, size=n_cols).astype(np.float32)\n",
    "            scale = (1.0 + abs(float(np.mean(vals[r_idx])))) * 1e-6\n",
    "            vals[r_idx] = vals[r_idx] + noise * scale\n",
    "        out_df.iloc[:, :] = vals\n",
    "    return out_df\n",
    "\n",
    "# =========================\n",
    "# TNN\n",
    "# =========================\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=5):\n",
    "        super().__init__()\n",
    "        pad = kernel_size // 2\n",
    "        self.depthwise = nn.Conv1d(in_ch, in_ch, kernel_size=kernel_size, groups=in_ch, padding=pad)\n",
    "        self.pointwise = nn.Conv1d(in_ch, out_ch, kernel_size=1)\n",
    "        self.act = nn.GELU()\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.act(x)\n",
    "\n",
    "class TimeAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        B,T,D = x.shape\n",
    "        H = self.n_heads\n",
    "        q = self.q_proj(x).view(B,T,H,self.d_k).transpose(1,2)\n",
    "        k = self.k_proj(x).view(B,T,H,self.d_k).transpose(1,2)\n",
    "        v = self.v_proj(x).view(B,T,H,self.d_k).transpose(1,2)\n",
    "        attn_logits = (q @ k.transpose(-2,-1))/math.sqrt(self.d_k)\n",
    "        attn = torch.softmax(attn_logits, dim=-1)\n",
    "        out = attn @ v\n",
    "        out = out.transpose(1,2).contiguous().view(B,T,D)\n",
    "        return self.out(out), attn\n",
    "\n",
    "class TNN(nn.Module):\n",
    "    def __init__(self, in_features, hidden, lookback, out_dim=CFG.total_targets, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.kernel = DepthwiseSeparableConv1d(in_ch=in_features, out_ch=hidden)\n",
    "        self.proj = nn.Linear(hidden, hidden)\n",
    "        self.attn = TimeAttention(d_model=hidden, n_heads=n_heads)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden//2, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_conv = x.transpose(1,2)\n",
    "        x_conv = self.kernel(x_conv)\n",
    "        x_h = x_conv.transpose(1,2)\n",
    "        x_h = self.proj(x_h)\n",
    "        x_attn,_ = self.attn(x_h)\n",
    "        pooled = self.pool(x_attn.transpose(1,2)).squeeze(-1)\n",
    "        return self.head(pooled)\n",
    "\n",
    "# =========================\n",
    "# Stage 1 Training\n",
    "# =========================\n",
    "def train_stage1(seed):\n",
    "    _set_seed(seed)\n",
    "    train = pd.read_csv(os.path.join(CFG.path, \"train.csv\")).sort_values(\"date_id\")\n",
    "    train_labels = pd.read_csv(os.path.join(CFG.path, \"train_labels.csv\"))\n",
    "\n",
    "    all_cols = [c for c in train.columns if c != \"date_id\"]\n",
    "    global FEATURES\n",
    "    if not FEATURES: FEATURES = all_cols.copy()\n",
    "\n",
    "    X_train_df = preprocess_columns(train[[\"date_id\"]+FEATURES].copy()).fillna(0.0)\n",
    "    y_train_df = train_labels[[\"date_id\"]+CFG.targets].fillna(CFG.solution_null_filler).copy()\n",
    "\n",
    "    SCALER = StandardScaler()\n",
    "    X_scaled = SCALER.fit_transform(X_train_df[FEATURES]).astype(np.float32)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=FEATURES)\n",
    "    X_scaled_df.insert(0,\"date_id\", X_train_df[\"date_id\"].values)\n",
    "\n",
    "    etr = ExtraTreesRegressor(random_state=seed, n_jobs=-1, n_estimators=384, max_features=\"sqrt\")\n",
    "    etr.fit(X_scaled_df[FEATURES], y_train_df[CFG.targets])\n",
    "\n",
    "    lookback = max(2, CFG.tnn_lookback)\n",
    "    X_seq, y_seq = build_sequences(X_scaled_df, y_train_df, lookback)\n",
    "    if X_seq is not None:\n",
    "        X_tr,X_va,y_tr,y_va = train_test_split(X_seq,y_seq,test_size=0.1,random_state=seed,shuffle=False)\n",
    "        train_loader = torch.utils.data.DataLoader(SeqDataset(X_tr,y_tr),batch_size=128,shuffle=True)\n",
    "        in_features = X_seq.shape[-1]\n",
    "        tnn = TNN(in_features, CFG.tnn_hidden, lookback).to(DEVICE)\n",
    "        opt = optim.AdamW(tnn.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "        crit = nn.SmoothL1Loss()\n",
    "        for ep in range(CFG.tnn_epochs):\n",
    "            tnn.train()\n",
    "            for xb,yb in train_loader:\n",
    "                xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                loss = crit(tnn(xb), yb)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "        tnn.eval()\n",
    "    else:\n",
    "        tnn = None\n",
    "\n",
    "    return etr, tnn, SCALER\n",
    "\n",
    "def build_sequences(df_feat, df_lbl, lookback):\n",
    "    if \"date_id\" not in df_feat.columns: return None,None\n",
    "    df_feat = df_feat.sort_values(\"date_id\").reset_index(drop=True)\n",
    "    df_lbl = df_lbl.sort_values(\"date_id\").reset_index(drop=True)\n",
    "    X_all = df_feat[FEATURES].to_numpy(np.float32)\n",
    "    y_all = df_lbl[CFG.targets].to_numpy(np.float32)\n",
    "    if len(X_all) < lookback+1: return None,None\n",
    "    X_seq,y_seq=[],[]\n",
    "    for t in range(lookback-1,len(X_all)):\n",
    "        X_seq.append(X_all[t-lookback+1:t+1])\n",
    "        y_seq.append(y_all[t])\n",
    "    return np.stack(X_seq,axis=0), np.stack(y_seq,axis=0)\n",
    "\n",
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,X,y): self.X,self.y=X.astype(np.float32),y.astype(np.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,idx): return torch.from_numpy(self.X[idx]),torch.from_numpy(self.y[idx])\n",
    "\n",
    "# =========================\n",
    "# Stage 2 Training\n",
    "# =========================\n",
    "def train_stage2(stage1_preds, X_scaled, y_true):\n",
    "    residuals = y_true - stage1_preds\n",
    "    train_data = pd.DataFrame(X_scaled.astype(np.float32), columns=FEATURES).copy()\n",
    "    for i in range(CFG.total_targets):\n",
    "        train_data[f\"stage1_pred_{i}\"] = stage1_preds[:,i]\n",
    "    models=[]\n",
    "    for i in range(CFG.total_targets):\n",
    "        dtrain = lgb.Dataset(train_data, label=residuals[:,i].astype(np.float32))\n",
    "        model = lgb.train(CFG.stage2_params, dtrain)\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "# =========================\n",
    "# Training Pipeline\n",
    "# =========================\n",
    "def load_and_train_model():\n",
    "    global STAGE1_MODELS, STAGE2_MODEL, MODEL_READY\n",
    "    STAGE1_MODELS=[]\n",
    "    for s in range(CFG.n_stage1_seeds):\n",
    "        STAGE1_MODELS.append(train_stage1(seed=CFG.seed+s))\n",
    "\n",
    "    train = pd.read_csv(os.path.join(CFG.path, \"train.csv\")).sort_values(\"date_id\")\n",
    "    train_labels = pd.read_csv(os.path.join(CFG.path, \"train_labels.csv\"))\n",
    "    X_train_df = preprocess_columns(train[[\"date_id\"]+FEATURES].copy()).fillna(0.0)\n",
    "    y_train = train_labels[CFG.targets].fillna(0.0).to_numpy(np.float32)\n",
    "\n",
    "    X_stack=[]\n",
    "    for etr,tnn,scaler in STAGE1_MODELS:\n",
    "        X_scaled = scaler.transform(X_train_df[FEATURES]).astype(np.float32)\n",
    "        preds_etr = etr.predict(X_scaled).astype(np.float32)\n",
    "        if tnn is not None and len(X_scaled)>=CFG.tnn_lookback:\n",
    "            seqs=[X_scaled[t-CFG.tnn_lookback+1:t+1] for t in range(CFG.tnn_lookback-1,len(X_scaled))]\n",
    "            X_seq=torch.from_numpy(np.stack(seqs,axis=0).astype(np.float32)).to(DEVICE)\n",
    "            with torch.no_grad(): preds_tnn=tnn(X_seq).cpu().numpy().astype(np.float32)\n",
    "            full_preds=np.zeros_like(preds_etr,dtype=np.float32); full_preds[CFG.tnn_lookback-1:]=preds_tnn\n",
    "            preds=(preds_etr+full_preds)/2\n",
    "        else: preds=preds_etr\n",
    "        X_stack.append(preds)\n",
    "    stage1_preds=np.mean(X_stack,axis=0).astype(np.float32)\n",
    "\n",
    "    rmse=np.sqrt(mean_squared_error(y_train,stage1_preds))\n",
    "    mae=mean_absolute_error(y_train,stage1_preds)\n",
    "    print(f\"[Stage 1] RMSE={rmse:.5f} MAE={mae:.5f}\")\n",
    "\n",
    "    STAGE2_MODEL=train_stage2(stage1_preds,X_train_df[FEATURES].to_numpy(),y_train)\n",
    "\n",
    "    df_features=pd.DataFrame(X_train_df[FEATURES].to_numpy(np.float32),columns=FEATURES)\n",
    "    for i in range(CFG.total_targets):\n",
    "        df_features[f\"stage1_pred_{i}\"]=stage1_preds[:,i]\n",
    "    stage2_corr=np.vstack([m.predict(df_features) for m in STAGE2_MODEL]).T.astype(np.float32)\n",
    "    final_preds=(stage1_preds+stage2_corr).astype(np.float32)\n",
    "\n",
    "    rmse2=np.sqrt(mean_squared_error(y_train,final_preds))\n",
    "    mae2=mean_absolute_error(y_train,final_preds)\n",
    "    print(f\"[Stage 2] RMSE={rmse2:.5f} MAE={mae2:.5f}\")\n",
    "\n",
    "    MODEL_READY=True\n",
    "\n",
    "# =========================\n",
    "# Predict\n",
    "# =========================\n",
    "def predict(test: pl.DataFrame,*args):\n",
    "    global MODEL_READY\n",
    "    if not MODEL_READY: load_and_train_model()\n",
    "\n",
    "    test_pd=test.to_pandas()\n",
    "    date_ids=test_pd[\"date_id\"].values if \"date_id\" in test_pd.columns else None\n",
    "    test_pd=preprocess_columns(test_pd).fillna(0.0)\n",
    "\n",
    "    # safety: cast everything to float32\n",
    "    for col in FEATURES:\n",
    "        if col in test_pd.columns:\n",
    "            test_pd[col] = test_pd[col].astype(np.float32)\n",
    "\n",
    "    preds_stage1=[]\n",
    "    for etr,tnn,scaler in STAGE1_MODELS:\n",
    "        X_scaled=scaler.transform(test_pd[FEATURES]).astype(np.float32)\n",
    "        preds_etr=etr.predict(X_scaled).astype(np.float32)\n",
    "        if tnn is not None and len(X_scaled)>=CFG.tnn_lookback:\n",
    "            seqs=[X_scaled[t-CFG.tnn_lookback+1:t+1] for t in range(CFG.tnn_lookback-1,len(X_scaled))]\n",
    "            X_seq=torch.from_numpy(np.stack(seqs,axis=0).astype(np.float32)).to(DEVICE)\n",
    "            with torch.no_grad(): preds_tnn=tnn(X_seq).cpu().numpy().astype(np.float32)\n",
    "            full_preds=np.zeros_like(preds_etr,dtype=np.float32); full_preds[CFG.tnn_lookback-1:]=preds_tnn\n",
    "            preds=(preds_etr+full_preds)/2\n",
    "        else: preds=preds_etr\n",
    "        preds_stage1.append(preds)\n",
    "        X_scaled_all=X_scaled\n",
    "    stage1_pred=np.mean(preds_stage1,axis=0).astype(np.float32)\n",
    "\n",
    "    df_features=pd.DataFrame(X_scaled_all.astype(np.float32),columns=FEATURES)\n",
    "    for i in range(CFG.total_targets):\n",
    "        df_features[f\"stage1_pred_{i}\"]=stage1_pred[:,i]\n",
    "    stage2_corr=np.vstack([m.predict(df_features) for m in STAGE2_MODEL]).T.astype(np.float32)\n",
    "    final_pred=(stage1_pred+stage2_corr).astype(np.float32)\n",
    "\n",
    "    out=pd.DataFrame(final_pred,columns=CFG.targets).astype(np.float32)\n",
    "    out=_stabilize_and_detie_rows(out,date_ids if date_ids is not None else np.zeros(len(out),dtype=int))\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# Inference Server\n",
    "# =========================\n",
    "inference_server=kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"): inference_server.serve()\n",
    "else: inference_server.run_local_gateway((CFG.path,))\n",
    "[Stage 1] RMSE=0.01493 MAE=0.00960\n",
    "[Stage 2] RMSE=0.00106 MAE=0.00013\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
