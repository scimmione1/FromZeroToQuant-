{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4f1786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Local development environment detected\n",
      "üöÄ Running local test of the enhanced pipeline...\n",
      "üß™ Starting local test of the pipeline...\n",
      "üöÄ Initializing Kaggle submission pipeline...\n",
      "üìä Loaded data: Features (1961, 558), Labels (1961, 425)\n",
      "üöÄ Fitting prediction pipeline...\n",
      "üîß Using full features for local environment\n",
      "üîß Creating advanced features...\n",
      "üîß Creating advanced features...\n",
      "‚úÖ Feature engineering completed: 11140 features added\n",
      "üîß Training fallback model...\n",
      "‚úÖ Created fallback models for 10 targets\n",
      "üìã Training models for 424 targets\n",
      "   - Stacking models: 3\n",
      "   - LightGBM models: 22\n",
      "   - Linear models: 399\n",
      "‚úÖ Feature engineering completed: 11140 features added\n",
      "üîß Training fallback model...\n",
      "‚úÖ Created fallback models for 10 targets\n",
      "üìã Training models for 424 targets\n",
      "   - Stacking models: 3\n",
      "   - LightGBM models: 22\n",
      "   - Linear models: 399\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 0.0468904\tvalid_0's l2: 0.00219871\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[99]\tvalid_0's rmse: 0.0468904\tvalid_0's l2: 0.00219871\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's rmse: 0.049342\tvalid_0's l2: 0.00243464\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's rmse: 0.049342\tvalid_0's l2: 0.00243464\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's rmse: 0.042523\tvalid_0's l2: 0.0018082\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's rmse: 0.042523\tvalid_0's l2: 0.0018082\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmse: 0.053306\tvalid_0's l2: 0.00284153\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's rmse: 0.053306\tvalid_0's l2: 0.00284153\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0566041\tvalid_0's l2: 0.00320402\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0566041\tvalid_0's l2: 0.00320402\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's rmse: 0.04557\tvalid_0's l2: 0.00207662\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[94]\tvalid_0's rmse: 0.04557\tvalid_0's l2: 0.00207662\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's rmse: 0.0495429\tvalid_0's l2: 0.00245449\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid_0's rmse: 0.0495429\tvalid_0's l2: 0.00245449\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0529053\tvalid_0's l2: 0.00279897\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.0529053\tvalid_0's l2: 0.00279897\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.055717\tvalid_0's l2: 0.00310439\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 0.055717\tvalid_0's l2: 0.00310439\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 667\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# Kaggle Inference Server (COMMENTED OUT FOR LOCAL TESTING)\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# inference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\u001b[39;00m\n\u001b[32m    659\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    664\u001b[39m \n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# Run local test\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müöÄ Running local test of the enhanced pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m test_result = \u001b[43mrun_local_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 630\u001b[39m, in \u001b[36mrun_local_test\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müß™ Starting local test of the pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# Initialize the submission manager\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43msubmission_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize_for_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# Create dummy test data\u001b[39;00m\n\u001b[32m    633\u001b[39m dummy_test = {\n\u001b[32m    634\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdate_id\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m],\n\u001b[32m    635\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_1\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m1.2\u001b[39m, \u001b[32m2.3\u001b[39m, \u001b[32m3.4\u001b[39m],\n\u001b[32m    636\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_2\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.5\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.9\u001b[39m],\n\u001b[32m    637\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_3\u001b[39m\u001b[33m'\u001b[39m: [-\u001b[32m0.1\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.4\u001b[39m]\n\u001b[32m    638\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 565\u001b[39m, in \u001b[36mKaggleSubmissionManager.initialize_for_submission\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Loaded data: Features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Labels \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    564\u001b[39m     \u001b[38;5;66;03m# Fit the complete pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Kaggle submission pipeline initialized successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 422\u001b[39m, in \u001b[36mPipelinePredictor.fit\u001b[39m\u001b[34m(self, train_df, train_labels_df)\u001b[39m\n\u001b[32m    420\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_manager.train_stacking_model(X_aligned, y_aligned, target)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m strategy == \u001b[33m'\u001b[39m\u001b[33mlightgbm\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_lightgbm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_aligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_aligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    424\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_manager.train_linear_model(X_aligned, y_aligned, target)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mAdaptiveModelManager.train_lightgbm_model\u001b[39m\u001b[34m(self, X, y, target_name)\u001b[39m\n\u001b[32m    278\u001b[39m model = lgb.LGBMRegressor(**lgbm_params)\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Fit with validation set for early stopping\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrmse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEARLY_STOPPING_ROUNDS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Silent\u001b[39;49;00m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28mself\u001b[39m.models[target_name] = model\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.feature_columns[target_name] = X.columns.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[39m, in \u001b[36mLGBMRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1383\u001b[39m     X: _LGBM_ScikitMatrixLike,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1395\u001b[39m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1396\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLGBMRegressor\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[39m, in \u001b[36mLGBMModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[39m\n\u001b[32m   1046\u001b[39m evals_result: _EvalResultDict = {}\n\u001b[32m   1047\u001b[39m callbacks.append(record_evaluation(evals_result))\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1056\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[32m   1061\u001b[39m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m._n_features = \u001b[38;5;28mself\u001b[39m._Booster.num_feature()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightgbm\\engine.py:322\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[32m    311\u001b[39m     cb(\n\u001b[32m    312\u001b[39m         callback.CallbackEnv(\n\u001b[32m    313\u001b[39m             model=booster,\n\u001b[32m   (...)\u001b[39m\u001b[32m    319\u001b[39m         )\n\u001b[32m    320\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m \u001b[43mbooster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] = []\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\lightgbm\\basic.py:4155\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, train_set, fobj)\u001b[39m\n\u001b[32m   4152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__set_objective_to_none:\n\u001b[32m   4153\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[33m\"\u001b[39m\u001b[33mCannot update due to null objective function.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4154\u001b[39m _safe_call(\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4159\u001b[39m )\n\u001b[32m   4160\u001b[39m \u001b[38;5;28mself\u001b[39m.__is_predicted_cur_iter = [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.__num_dataset)]\n\u001b[32m   4161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished.value == \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==== Cell 1: Imports & Configuration ==== #\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kaggle evaluation (COMMENTED OUT FOR LOCAL TESTING)\n",
    "# import kaggle_evaluation.mitsui_inference_server\n",
    "\n",
    "# ==== Enhanced Configuration ==== #\n",
    "class Config:\n",
    "    \"\"\"Enhanced configuration for Mitsui Commodity Prediction Challenge\"\"\"\n",
    "    NUM_TARGET_COLUMNS = 424\n",
    "    RANDOM_STATE = 42\n",
    "    CV_FOLDS = 3\n",
    "    \n",
    "    # Environment-specific model parameters with eval_set support\n",
    "    LGBM_PARAMS_KAGGLE = {\n",
    "        'n_estimators': 50,         # Reduced for faster training\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': 6,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': 1,\n",
    "        'force_row_wise': True,\n",
    "        'feature_fraction': 0.8,    # Subsampling for speed\n",
    "        'bagging_fraction': 0.8,    # Row subsampling\n",
    "        'bagging_freq': 5,\n",
    "        'min_child_samples': 20,    # Regularization\n",
    "    }\n",
    "    \n",
    "    LGBM_PARAMS_LOCAL = {\n",
    "        'n_estimators': 100,        # More thorough locally\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'force_row_wise': True,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 5,\n",
    "    }\n",
    "    \n",
    "    # Feature engineering parameters\n",
    "    ROLLING_WINDOWS = [3, 5, 10, 20]\n",
    "    LAG_PERIODS = [1, 2, 3]\n",
    "    \n",
    "    # Adaptive training configuration\n",
    "    MAX_COMPLEX_MODELS = 3      # Reduced stacking models for Kaggle\n",
    "    MAX_SIMPLE_MODELS = 25      # Reduced LightGBM for performance\n",
    "    MIN_SAMPLES_REQUIRED = 100\n",
    "    \n",
    "    # Early stopping configuration\n",
    "    EARLY_STOPPING_ROUNDS = 10\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data_path():\n",
    "        kaggle_path = Path('/kaggle/input/mitsui-commodity-prediction-challenge')\n",
    "        local_path = Path(\"dataset\")\n",
    "        \n",
    "        if kaggle_path.exists():\n",
    "            print(\"üîß Kaggle environment detected\")\n",
    "            return kaggle_path\n",
    "        else:\n",
    "            print(\"üîß Local development environment detected\")\n",
    "            return local_path\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lgbm_params():\n",
    "        \"\"\"Get environment-appropriate LightGBM parameters\"\"\"\n",
    "        if Path('/kaggle').exists():\n",
    "            return Config.LGBM_PARAMS_KAGGLE.copy()\n",
    "        return Config.LGBM_PARAMS_LOCAL.copy()\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_kaggle_environment():\n",
    "        \"\"\"Check if running in Kaggle environment\"\"\"\n",
    "        return Path('/kaggle').exists()\n",
    "\n",
    "CFG = Config()\n",
    "data_path = CFG.get_data_path()\n",
    "\n",
    "# ==== Enhanced Feature Engineering Pipeline ==== #\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Scikit-learn compatible feature engineering pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, rolling_windows=None, lag_periods=None, enable_heavy_features=True):\n",
    "        self.rolling_windows = rolling_windows or CFG.ROLLING_WINDOWS\n",
    "        self.lag_periods = lag_periods or CFG.LAG_PERIODS\n",
    "        self.enable_heavy_features = enable_heavy_features\n",
    "        self.fitted_stats_ = {}\n",
    "        self.feature_names_ = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit feature engineer on training data\"\"\"\n",
    "        X = X.copy()\n",
    "        feature_cols = [c for c in X.columns if c != 'date_id']\n",
    "        \n",
    "        self.feature_names_ = feature_cols\n",
    "        for col in feature_cols:\n",
    "            self.fitted_stats_[col] = {\n",
    "                'mean': X[col].mean(),\n",
    "                'std': X[col].std(),\n",
    "                'quantiles': X[col].quantile([0.25, 0.5, 0.75]).to_dict()\n",
    "            }\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data using fitted statistics\"\"\"\n",
    "        if not hasattr(self, 'fitted_stats_'):\n",
    "            raise ValueError(\"FeatureEngineer must be fitted before transform\")\n",
    "            \n",
    "        return self._create_features(X.copy())\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X, y).transform(X)\n",
    "    \n",
    "    def _create_features(self, df):\n",
    "        \"\"\"Create advanced features with consistent transformation\"\"\"\n",
    "        print(\"üîß Creating advanced features...\")\n",
    "        \n",
    "        feature_count_before = len(df.columns)\n",
    "        feature_cols = [c for c in df.columns if c != 'date_id']\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            if col == 'date_id':\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Rolling statistics\n",
    "                for window in self.rolling_windows:\n",
    "                    df[f'{col}_rolling_mean_{window}'] = df[col].rolling(window).mean()\n",
    "                    df[f'{col}_rolling_std_{window}'] = df[col].rolling(window).std()\n",
    "                    \n",
    "                # Volatility measures\n",
    "                df[f'{col}_annual_vol_20'] = df[col].rolling(20).std() * np.sqrt(252)\n",
    "                df[f'{col}_pct_change'] = df[col].pct_change()\n",
    "                \n",
    "                # Lag features\n",
    "                for lag in self.lag_periods:\n",
    "                    df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "                \n",
    "                if self.enable_heavy_features:\n",
    "                    # Higher order statistics\n",
    "                    df[f'{col}_rolling_skew_10'] = df[col].rolling(10).skew()\n",
    "                    df[f'{col}_rolling_kurt_10'] = df[col].rolling(10).kurt()\n",
    "                    \n",
    "                    # Autocorrelation features\n",
    "                    df[f'{col}_autocorr_1'] = df[col].rolling(20).apply(\n",
    "                        lambda x: x.autocorr(lag=1) if len(x.dropna()) > 1 else 0, raw=False\n",
    "                    )\n",
    "                    df[f'{col}_autocorr_5'] = df[col].rolling(20).apply(\n",
    "                        lambda x: x.autocorr(lag=5) if len(x.dropna()) > 5 else 0, raw=False\n",
    "                    )\n",
    "                    \n",
    "                    # Market regime indicators\n",
    "                    roll_mean = df[col].rolling(10).mean()\n",
    "                    roll_vol = df[col].rolling(10).std()\n",
    "                    df[f'{col}_regime_trend_up'] = (roll_mean > roll_mean.shift(1)).astype(int)\n",
    "                    df[f'{col}_regime_high_vol'] = (roll_vol > roll_vol.quantile(0.75)).astype(int)\n",
    "                    \n",
    "                    # Vol-of-vol (volatility clustering)\n",
    "                    rolling_vol = df[col].rolling(10).std()\n",
    "                    df[f'{col}_vol_of_vol'] = rolling_vol.rolling(5).std()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error creating features for {col}: {e}\")\n",
    "                continue\n",
    "\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        feature_count_after = len(df.columns)\n",
    "        features_added = feature_count_after - feature_count_before\n",
    "        print(f\"‚úÖ Feature engineering completed: {features_added} features added\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# ==== Enhanced Model Management ==== #\n",
    "class AdaptiveModelManager:\n",
    "    \"\"\"Manages adaptive model selection and training with proper validation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.feature_columns = {}\n",
    "        self.model_strategies = {}\n",
    "        \n",
    "    def calculate_target_importance(self, train_labels_df):\n",
    "        \"\"\"Calculate target importance for model selection\"\"\"\n",
    "        target_columns = [col for col in train_labels_df.columns if col.startswith('target_')]\n",
    "        importance_scores = []\n",
    "        \n",
    "        for target in target_columns:\n",
    "            # Use combination of variance and non-null ratio as importance\n",
    "            variance = train_labels_df[target].var()\n",
    "            non_null_ratio = train_labels_df[target].notna().sum() / len(train_labels_df)\n",
    "            combined_importance = variance * non_null_ratio\n",
    "            importance_scores.append(combined_importance)\n",
    "            \n",
    "        return importance_scores\n",
    "    \n",
    "    def select_model_strategies(self, target_columns, importance_scores):\n",
    "        \"\"\"Select appropriate model strategy for each target\"\"\"\n",
    "        target_scores = list(zip(target_columns, importance_scores))\n",
    "        target_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        strategies = {}\n",
    "        for i, (target, score) in enumerate(target_scores):\n",
    "            if i < CFG.MAX_COMPLEX_MODELS:\n",
    "                strategies[target] = 'stacking'\n",
    "            elif i < CFG.MAX_SIMPLE_MODELS:\n",
    "                strategies[target] = 'lightgbm'\n",
    "            else:\n",
    "                strategies[target] = 'linear'\n",
    "                \n",
    "        return strategies\n",
    "    \n",
    "    def train_stacking_model(self, X, y, target_name):\n",
    "        \"\"\"Train stacking ensemble model with validation\"\"\"\n",
    "        try:\n",
    "            # Split data for validation\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=CFG.VALIDATION_SPLIT, random_state=CFG.RANDOM_STATE\n",
    "            )\n",
    "            \n",
    "            # Create LightGBM with proper validation\n",
    "            lgbm_params = CFG.get_lgbm_params()\n",
    "            lgbm_model = lgb.LGBMRegressor(**lgbm_params)\n",
    "            \n",
    "            estimators = [\n",
    "                ('lr', Ridge(alpha=1.0, random_state=CFG.RANDOM_STATE)),\n",
    "                ('lgb', lgbm_model)\n",
    "            ]\n",
    "            \n",
    "            model = StackingRegressor(\n",
    "                estimators=estimators,\n",
    "                final_estimator=Ridge(alpha=1.0, random_state=CFG.RANDOM_STATE),\n",
    "                cv=CFG.CV_FOLDS,\n",
    "                n_jobs=1\n",
    "            )\n",
    "            \n",
    "            model.fit(X, y)  # Use full data for stacking (CV handles validation internally)\n",
    "            self.models[target_name] = model\n",
    "            self.feature_columns[target_name] = X.columns.tolist()\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Stacking failed for {target_name}: {e}\")\n",
    "            return self.train_linear_model(X, y, target_name)\n",
    "    \n",
    "    def train_lightgbm_model(self, X, y, target_name):\n",
    "        \"\"\"Train LightGBM model with proper early stopping\"\"\"\n",
    "        try:\n",
    "            # Split data for validation\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=CFG.VALIDATION_SPLIT, random_state=CFG.RANDOM_STATE\n",
    "            )\n",
    "            \n",
    "            lgbm_params = CFG.get_lgbm_params()\n",
    "            model = lgb.LGBMRegressor(**lgbm_params)\n",
    "            \n",
    "            # Fit with validation set for early stopping\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                eval_metric='rmse',\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(CFG.EARLY_STOPPING_ROUNDS),\n",
    "                    lgb.log_evaluation(0)  # Silent\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.models[target_name] = model\n",
    "            self.feature_columns[target_name] = X.columns.tolist()\n",
    "            \n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå LightGBM failed for {target_name}: {e}\")\n",
    "            return self.train_linear_model(X, y, target_name)\n",
    "    \n",
    "    def train_linear_model(self, X, y, target_name):\n",
    "        \"\"\"Train linear regression model as fallback\"\"\"\n",
    "        model = Ridge(alpha=1.0, random_state=CFG.RANDOM_STATE)\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        self.models[target_name] = model\n",
    "        self.feature_columns[target_name] = X.columns.tolist()\n",
    "        \n",
    "        return model\n",
    "\n",
    "def calculate_normalized_correlation_metric(y_true, y_pred):\n",
    "    \"\"\"Calculate normalized correlation metric\"\"\"\n",
    "    try:\n",
    "        base_corr, _ = spearmanr(y_true, y_pred)\n",
    "        if np.isnan(base_corr):\n",
    "            return 0.0\n",
    "            \n",
    "        residuals = y_true - y_pred\n",
    "        volatility_factor = np.std(residuals) / (np.std(y_true) + 1e-8)\n",
    "        normalized_metric = abs(base_corr) / (1 + volatility_factor)\n",
    "        \n",
    "        return normalized_metric\n",
    "        \n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# ==== Stabilization Function ==== #\n",
    "def _stabilize_and_detie_rows(out_df, date_ids=None):\n",
    "    \"\"\"Ensure no flat rows in predictions\"\"\"\n",
    "    out_df = out_df.astype(np.float32)\n",
    "    out_df[:] = np.nan_to_num(out_df.values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    n_rows, n_cols = out_df.shape\n",
    "    \n",
    "    if date_ids is None:\n",
    "        date_ids = np.zeros(n_rows, dtype=int)\n",
    "        \n",
    "    vals = out_df.to_numpy(np.float32)\n",
    "    row_stds = np.std(vals, axis=1)\n",
    "    flat_mask = row_stds < 1e-15\n",
    "    \n",
    "    if np.any(flat_mask):\n",
    "        for r_idx in np.where(flat_mask)[0]:\n",
    "            rng = np.random.default_rng(int(date_ids[r_idx]) + 131071)\n",
    "            noise = rng.normal(loc=0.0, scale=1.0, size=n_cols).astype(np.float32)\n",
    "            scale = (1.0 + abs(float(np.mean(vals[r_idx])))) * 1e-6\n",
    "            vals[r_idx] = vals[r_idx] + noise * scale\n",
    "        out_df.iloc[:, :] = vals\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "# ==== Enhanced Pipeline Predictor ==== #\n",
    "class PipelinePredictor:\n",
    "    \"\"\"Complete prediction pipeline with adaptive feature engineering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_pipeline = None\n",
    "        self.model_manager = AdaptiveModelManager()\n",
    "        self.is_fitted = False\n",
    "        self.fallback_model = None\n",
    "        self.original_data = None  # Store for fallback predictions\n",
    "    \n",
    "    def fit(self, train_df, train_labels_df):\n",
    "        \"\"\"Fit the complete pipeline on training data\"\"\"\n",
    "        print(\"üöÄ Fitting prediction pipeline...\")\n",
    "        \n",
    "        # Store original data for fallback predictions\n",
    "        self.original_data = train_df.copy()\n",
    "        \n",
    "        # Determine feature engineering complexity based on environment\n",
    "        if CFG.is_kaggle_environment():\n",
    "            # Lighter features for Kaggle\n",
    "            rolling_windows = [3, 5, 10]\n",
    "            lag_periods = [1, 2]\n",
    "            enable_heavy = False\n",
    "            print(\"üîß Using lightweight features for Kaggle environment\")\n",
    "        else:\n",
    "            # Full features for local development\n",
    "            rolling_windows = [3, 5, 10, 20]\n",
    "            lag_periods = [1, 2, 3]\n",
    "            enable_heavy = True\n",
    "            print(\"üîß Using full features for local environment\")\n",
    "        \n",
    "        # Fit feature engineering pipeline\n",
    "        self.feature_pipeline = FeatureEngineer(\n",
    "            rolling_windows=rolling_windows,\n",
    "            lag_periods=lag_periods,\n",
    "            enable_heavy_features=enable_heavy\n",
    "        )\n",
    "        \n",
    "        # Fit feature pipeline and transform training data\n",
    "        X_train = self.feature_pipeline.fit_transform(train_df)\n",
    "        X_train = X_train.drop(columns=['date_id'])\n",
    "        \n",
    "        # Create ultra-simple fallback model for better predictions\n",
    "        print(\"üîß Training fallback model...\")\n",
    "        self._create_fallback_model(train_df, train_labels_df)\n",
    "        \n",
    "        # Calculate target importance and select strategies\n",
    "        target_columns = [col for col in train_labels_df.columns if col.startswith('target_')]\n",
    "        importance_scores = self.model_manager.calculate_target_importance(train_labels_df)\n",
    "        model_strategies = self.model_manager.select_model_strategies(target_columns, importance_scores)\n",
    "        \n",
    "        print(f\"üìã Training models for {len(target_columns)} targets\")\n",
    "        print(f\"   - Stacking models: {sum(1 for s in model_strategies.values() if s == 'stacking')}\")\n",
    "        print(f\"   - LightGBM models: {sum(1 for s in model_strategies.values() if s == 'lightgbm')}\")\n",
    "        print(f\"   - Linear models: {sum(1 for s in model_strategies.values() if s == 'linear')}\")\n",
    "        \n",
    "        # Train models with adaptive strategy\n",
    "        trained_count = 0\n",
    "        for target in target_columns:\n",
    "            strategy = model_strategies.get(target, 'linear')\n",
    "            \n",
    "            y = train_labels_df[target].dropna()\n",
    "            common_idx = X_train.index.intersection(y.index)\n",
    "            X_aligned = X_train.loc[common_idx].fillna(0)\n",
    "            y_aligned = y.loc[common_idx]\n",
    "            \n",
    "            if len(X_aligned) >= CFG.MIN_SAMPLES_REQUIRED:\n",
    "                try:\n",
    "                    if strategy == 'stacking':\n",
    "                        self.model_manager.train_stacking_model(X_aligned, y_aligned, target)\n",
    "                    elif strategy == 'lightgbm':\n",
    "                        self.model_manager.train_lightgbm_model(X_aligned, y_aligned, target)\n",
    "                    else:\n",
    "                        self.model_manager.train_linear_model(X_aligned, y_aligned, target)\n",
    "                    trained_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Failed to train {target}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Successfully trained {trained_count} models\")\n",
    "        self.is_fitted = True\n",
    "        print(\"‚úÖ Pipeline fitting completed\")\n",
    "    \n",
    "    def _create_fallback_model(self, train_df, train_labels_df):\n",
    "        \"\"\"Create ultra-simple fallback model using recent observations\"\"\"\n",
    "        try:\n",
    "            # Use last 20 observations for simple linear trend\n",
    "            recent_data = train_df.tail(20).copy()\n",
    "            feature_cols = [c for c in recent_data.columns if c != 'date_id']\n",
    "            \n",
    "            # Simple features: means and trends\n",
    "            fallback_features = {}\n",
    "            for col in feature_cols:\n",
    "                fallback_features[f'{col}_mean'] = recent_data[col].mean()\n",
    "                fallback_features[f'{col}_trend'] = recent_data[col].iloc[-1] - recent_data[col].iloc[0]\n",
    "            \n",
    "            # Train simple model on these features for each target\n",
    "            self.fallback_model = {}\n",
    "            target_columns = [col for col in train_labels_df.columns if col.startswith('target_')]\n",
    "            \n",
    "            for target in target_columns[:10]:  # Only for top 10 targets\n",
    "                try:\n",
    "                    recent_targets = train_labels_df[target].tail(20).dropna()\n",
    "                    if len(recent_targets) >= 5:\n",
    "                        # Simple linear model on target trend\n",
    "                        X_simple = np.arange(len(recent_targets)).reshape(-1, 1)\n",
    "                        model = LinearRegression()\n",
    "                        model.fit(X_simple, recent_targets)\n",
    "                        self.fallback_model[target] = {\n",
    "                            'model': model,\n",
    "                            'last_value': recent_targets.iloc[-1],\n",
    "                            'mean_value': recent_targets.mean()\n",
    "                        }\n",
    "                except Exception:\n",
    "                    continue\n",
    "                    \n",
    "            print(f\"‚úÖ Created fallback models for {len(self.fallback_model)} targets\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fallback model creation failed: {e}\")\n",
    "            self.fallback_model = None\n",
    "        \n",
    "    def predict(self, test: pl.DataFrame, *label_lags) -> pl.DataFrame:\n",
    "        \"\"\"Generate predictions using fitted pipeline\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Pipeline must be fitted before prediction\")\n",
    "            \n",
    "        test_df = test.to_pandas()\n",
    "        \n",
    "        # Transform test data using fitted pipeline\n",
    "        X_test = self.feature_pipeline.transform(test_df)\n",
    "        X_test = X_test.drop(columns=['date_id'])\n",
    "        \n",
    "        # Generate predictions\n",
    "        predictions = np.zeros((len(test_df), CFG.NUM_TARGET_COLUMNS))\n",
    "        \n",
    "        for i in range(CFG.NUM_TARGET_COLUMNS):\n",
    "            target_name = f\"target_{i}\"\n",
    "            \n",
    "            try:\n",
    "                if target_name in self.model_manager.models:\n",
    "                    # Use trained model\n",
    "                    model = self.model_manager.models[target_name]\n",
    "                    feature_cols = self.model_manager.feature_columns[target_name]\n",
    "                    X_aligned = X_test[feature_cols]\n",
    "                    predictions[:, i] = model.predict(X_aligned)\n",
    "                else:\n",
    "                    # Use improved fallback prediction\n",
    "                    predictions[:, i] = self._generate_fallback_prediction(test_df, target_name)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Ultimate fallback\n",
    "                predictions[:, i] = self._generate_fallback_prediction(test_df, target_name)\n",
    "        \n",
    "        # Create and stabilize output\n",
    "        out_df = pd.DataFrame(predictions, columns=[f\"target_{i}\" for i in range(CFG.NUM_TARGET_COLUMNS)])\n",
    "        out_df = _stabilize_and_detie_rows(out_df, test_df.get('date_id'))\n",
    "        \n",
    "        return pl.DataFrame(out_df)\n",
    "    \n",
    "    def _generate_fallback_prediction(self, test_df, target_name):\n",
    "        \"\"\"Generate improved fallback predictions\"\"\"\n",
    "        n_samples = len(test_df)\n",
    "        \n",
    "        # Try using fallback model first\n",
    "        if self.fallback_model and target_name in self.fallback_model:\n",
    "            try:\n",
    "                fallback_info = self.fallback_model[target_name]\n",
    "                # Simple trend extrapolation\n",
    "                trend_pred = fallback_info['model'].predict([[n_samples]])[0]\n",
    "                # Blend with historical mean\n",
    "                prediction = 0.7 * fallback_info['last_value'] + 0.3 * trend_pred\n",
    "                return np.full(n_samples, prediction)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        # Ultimate fallback: use feature-based prediction\n",
    "        try:\n",
    "            feature_cols = [c for c in test_df.columns if c != 'date_id']\n",
    "            if feature_cols:\n",
    "                # Use normalized feature mean as prediction base\n",
    "                feature_means = test_df[feature_cols].mean(axis=1)\n",
    "                feature_std = test_df[feature_cols].std(axis=1)\n",
    "                prediction = feature_means * 0.01 + feature_std * 0.001\n",
    "                return prediction.values\n",
    "            else:\n",
    "                # Last resort: small random values\n",
    "                return np.random.normal(0, 0.001, n_samples)\n",
    "        except Exception:\n",
    "            return np.random.normal(0, 0.001, n_samples)\n",
    "\n",
    "# ==== Kaggle Submission Manager ==== #\n",
    "class KaggleSubmissionManager:\n",
    "    \"\"\"Robust Kaggle submission management\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.predictor = None\n",
    "        self.initialization_attempted = False\n",
    "        \n",
    "    def initialize_for_submission(self):\n",
    "        \"\"\"Initialize predictor with proper error handling\"\"\"\n",
    "        if self.initialization_attempted:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            print(\"üöÄ Initializing Kaggle submission pipeline...\")\n",
    "            self.predictor = PipelinePredictor()\n",
    "            \n",
    "            # Load training data\n",
    "            train_df = pd.read_csv(data_path / 'train.csv')\n",
    "            train_labels_df = pd.read_csv(data_path / 'train_labels.csv')\n",
    "            \n",
    "            print(f\"üìä Loaded data: Features {train_df.shape}, Labels {train_labels_df.shape}\")\n",
    "            \n",
    "            # Fit the complete pipeline\n",
    "            self.predictor.fit(train_df, train_labels_df)\n",
    "            \n",
    "            print(\"‚úÖ Kaggle submission pipeline initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Initialization failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Create minimal fallback\n",
    "            self.predictor = self._create_fallback_predictor()\n",
    "            \n",
    "        finally:\n",
    "            self.initialization_attempted = True\n",
    "    \n",
    "    def _create_fallback_predictor(self):\n",
    "        \"\"\"Create minimal fallback predictor\"\"\"\n",
    "        class FallbackPredictor:\n",
    "            def predict(self, test, *args):\n",
    "                n_samples = len(test)\n",
    "                # Slightly better fallback using feature statistics\n",
    "                try:\n",
    "                    test_df = test.to_pandas()\n",
    "                    feature_cols = [c for c in test_df.columns if c != 'date_id']\n",
    "                    if feature_cols:\n",
    "                        feature_means = test_df[feature_cols].mean(axis=1)\n",
    "                        fallback_preds = np.tile(feature_means.values.reshape(-1, 1) * 0.01, \n",
    "                                               (1, CFG.NUM_TARGET_COLUMNS))\n",
    "                    else:\n",
    "                        fallback_preds = np.random.normal(0, 0.001, (n_samples, CFG.NUM_TARGET_COLUMNS))\n",
    "                except Exception:\n",
    "                    fallback_preds = np.random.normal(0, 0.001, (n_samples, CFG.NUM_TARGET_COLUMNS))\n",
    "                \n",
    "                out_df = pd.DataFrame(fallback_preds, \n",
    "                                    columns=[f\"target_{i}\" for i in range(CFG.NUM_TARGET_COLUMNS)])\n",
    "                return pl.DataFrame(out_df)\n",
    "        \n",
    "        return FallbackPredictor()\n",
    "    \n",
    "    def predict(self, test, *label_lags):\n",
    "        \"\"\"Main prediction function\"\"\"\n",
    "        if not self.initialization_attempted:\n",
    "            self.initialize_for_submission()\n",
    "            \n",
    "        return self.predictor.predict(test, *label_lags)\n",
    "\n",
    "# Global submission manager\n",
    "submission_manager = KaggleSubmissionManager()\n",
    "\n",
    "def predict(test: pl.DataFrame,\n",
    "           label_lags_1_batch: pl.DataFrame,\n",
    "           label_lags_2_batch: pl.DataFrame,\n",
    "           label_lags_3_batch: pl.DataFrame,\n",
    "           label_lags_4_batch: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Kaggle submission predict function\"\"\"\n",
    "    return submission_manager.predict(test, label_lags_1_batch, \n",
    "                                    label_lags_2_batch, label_lags_3_batch, \n",
    "                                    label_lags_4_batch)\n",
    "\n",
    "# ==== LOCAL TESTING FUNCTION ==== #\n",
    "def run_local_test():\n",
    "    \"\"\"Test the pipeline locally\"\"\"\n",
    "    try:\n",
    "        print(\"üß™ Starting local test of the pipeline...\")\n",
    "        \n",
    "        # Initialize the submission manager\n",
    "        submission_manager.initialize_for_submission()\n",
    "        \n",
    "        # Create dummy test data\n",
    "        dummy_test = {\n",
    "            'date_id': [1, 2, 3],\n",
    "            'feature_1': [1.2, 2.3, 3.4],\n",
    "            'feature_2': [0.5, 0.7, 0.9],\n",
    "            'feature_3': [-0.1, 0.2, 0.4]\n",
    "        }\n",
    "        \n",
    "        test_df = pl.DataFrame(dummy_test)\n",
    "        dummy_lags = pl.DataFrame({'dummy': [0, 0, 0]})\n",
    "        \n",
    "        # Test prediction\n",
    "        predictions = predict(test_df, dummy_lags, dummy_lags, dummy_lags, dummy_lags)\n",
    "        \n",
    "        print(f\"‚úÖ Test successful! Prediction shape: {predictions.shape}\")\n",
    "        print(f\"üìä Expected shape: ({len(dummy_test['date_id'])}, {CFG.NUM_TARGET_COLUMNS})\")\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Local test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Kaggle Inference Server (COMMENTED OUT FOR LOCAL TESTING)\n",
    "# inference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "# if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "#     inference_server.serve()\n",
    "# else:\n",
    "#     inference_server.run_local_gateway((str(data_path),))\n",
    "\n",
    "# Run local test\n",
    "print(\"üöÄ Running local test of the enhanced pipeline...\")\n",
    "test_result = run_local_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
