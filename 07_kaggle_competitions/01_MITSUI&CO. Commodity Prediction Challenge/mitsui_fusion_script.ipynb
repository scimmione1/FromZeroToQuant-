{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89729c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Cell 1: Imports & Config ==== #\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "\n",
    "# ==== Global Config ==== #\n",
    "NUM_TARGET_COLUMNS = 424\n",
    "\n",
    "def get_data_path():\n",
    "    kaggle_path = Path('/kaggle/input/mitsui-commodity-prediction-challenge')\n",
    "    local_path = Path(\"dataset\")\n",
    "    return kaggle_path if kaggle_path.exists() else local_path\n",
    "\n",
    "data_path = get_data_path()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Cell 2: Feature Engineering (Fusion) ==== #\n",
    "def create_advanced_features(df, feature_cols, enable_heavy_features=True):\n",
    "    \"\"\"\n",
    "    Create advanced rolling/statistical features.\n",
    "    Heavy features (skew, kurtosis, autocorr) can be toggled via flag.\n",
    "    \"\"\"\n",
    "    for col in feature_cols:\n",
    "        if col == 'date_id': \n",
    "            continue\n",
    "        \n",
    "        # Rolling stats\n",
    "        for win in [3, 5, 10, 20]:\n",
    "            df[f'{col}_rolling_mean_{win}'] = df[col].rolling(win).mean()\n",
    "            df[f'{col}_rolling_std_{win}'] = df[col].rolling(win).std()\n",
    "        \n",
    "        # Lag features\n",
    "        for lag in [1, 2, 3]:\n",
    "            df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "        \n",
    "        # Annualized vol\n",
    "        df[f'{col}_annual_vol_20'] = df[col].rolling(20).std() * np.sqrt(252)\n",
    "        \n",
    "        if enable_heavy_features:\n",
    "            # Skew & Kurtosis\n",
    "            df[f'{col}_rolling_skew_10'] = df[col].rolling(10).skew()\n",
    "            df[f'{col}_rolling_kurt_10'] = df[col].rolling(10).kurt()\n",
    "            \n",
    "            # Autocorr\n",
    "            df[f'{col}_autocorr_1'] = df[col].rolling(20).apply(lambda x: x.autocorr(lag=1), raw=False)\n",
    "            df[f'{col}_autocorr_5'] = df[col].rolling(20).apply(lambda x: x.autocorr(lag=5), raw=False)\n",
    "            \n",
    "            # Regime\n",
    "            roll_mean = df[col].rolling(10).mean()\n",
    "            roll_vol = df[col].rolling(10).std()\n",
    "            df[f'{col}_regime_trend_up'] = (roll_mean > 0).astype(int)\n",
    "            df[f'{col}_regime_high_vol'] = (roll_vol > roll_vol.quantile(0.75)).astype(int)\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Cell 3: Stabilization (from Code 1) ==== #\n",
    "def _stabilize_and_detie_rows(out_df, date_ids=None):\n",
    "    \"\"\"Ensure no flat rows in predictions, add small noise if needed.\"\"\"\n",
    "    out_df = out_df.astype(np.float32)\n",
    "    out_df[:] = np.nan_to_num(out_df.values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    n_rows, n_cols = out_df.shape\n",
    "    if date_ids is None:\n",
    "        date_ids = np.zeros(n_rows, dtype=int)\n",
    "    vals = out_df.to_numpy(np.float32)\n",
    "    row_stds = np.std(vals, axis=1)\n",
    "    flat_mask = row_stds < 1e-15\n",
    "    if np.any(flat_mask):\n",
    "        for r_idx in np.where(flat_mask)[0]:\n",
    "            rng = np.random.default_rng(int(date_ids[r_idx]) + 131071)\n",
    "            noise = rng.normal(loc=0.0, scale=1.0, size=n_cols).astype(np.float32)\n",
    "            scale = (1.0 + abs(float(np.mean(vals[r_idx])))) * 1e-6\n",
    "            vals[r_idx] = vals[r_idx] + noise * scale\n",
    "        out_df.iloc[:, :] = vals\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b953e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Cell 4: Model Training ==== #\n",
    "def train_stacking_model(X, y):\n",
    "    \"\"\"\n",
    "    Train base stacking model with LinearRegression + LightGBM.\n",
    "    \"\"\"\n",
    "    estimators = [\n",
    "        ('lr', LinearRegression()),\n",
    "        ('lgb', lgb.LGBMRegressor(\n",
    "            n_estimators=200, \n",
    "            learning_rate=0.05, \n",
    "            num_leaves=64, \n",
    "            random_state=42))\n",
    "    ]\n",
    "    \n",
    "    model = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LinearRegression(),\n",
    "        cv=3\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dac5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Cell 5: Evaluation ==== #\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Evaluate model with RMSE and MAE + cross-validation.\n",
    "    \"\"\"\n",
    "    preds = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, preds))\n",
    "    mae = mean_absolute_error(y, preds)\n",
    "    \n",
    "    cv = KFold(n_splits=3, shuffle=False)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_absolute_error\")\n",
    "    \n",
    "    print(f\"RMSE: {rmse:.5f}, MAE: {mae:.5f}\")\n",
    "    print(f\"CV MAE: {-np.mean(cv_scores):.5f} (+/- {np.std(cv_scores):.5f})\")\n",
    "    return rmse, mae, cv_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Cell 6: Kaggle Integration ==== #\n",
    "def predict(\n",
    "    test: pl.DataFrame,\n",
    "    label_lags_1_batch: pl.DataFrame,\n",
    "    label_lags_2_batch: pl.DataFrame,\n",
    "    label_lags_3_batch: pl.DataFrame,\n",
    "    label_lags_4_batch: pl.DataFrame,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Kaggle predict hook: preprocess, feature engineering, predict, stabilize.\n",
    "    \"\"\"\n",
    "    test_df = test.to_pandas()\n",
    "    feature_cols = [c for c in test_df.columns if c != \"date_id\"]\n",
    "    test_df = create_advanced_features(test_df, feature_cols)\n",
    "    \n",
    "    # Dummy model load (placeholder, in practice load pre-trained stacking models per target)\n",
    "    model = LinearRegression()\n",
    "    X_test = test_df.drop(columns=[\"date_id\"])\n",
    "    preds = np.zeros((1, NUM_TARGET_COLUMNS))  # placeholder\n",
    "    out_df = pd.DataFrame(preds, columns=[f\"target_{i}\" for i in range(NUM_TARGET_COLUMNS)])\n",
    "    \n",
    "    return pl.DataFrame(out_df)\n",
    "\n",
    "# Kaggle Inference Server\n",
    "inference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((str(data_path),))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
