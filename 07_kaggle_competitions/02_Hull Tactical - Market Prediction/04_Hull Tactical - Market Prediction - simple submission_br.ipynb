{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d70b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18d795b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8990, 98), (10, 99))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Configuration and Data Loading (kaggle_evaluation only)\n",
    "# import kaggle_evaluation.default_inference_server as kdeval\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "# Read CSV files from data_path\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "import random\n",
    "RANDOM_SEED = random.randint(1, 10000)\n",
    "\n",
    "VOL_MULTIPLIER_LIMIT = 1.2\n",
    "VOL_WINDOW = 20\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f378b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe third line performs a crucial validation step by ensuring feature consistency between training and test datasets. \\nIt filters the feature list to include only columns that exist in both the training data and the test data \\n(test_raw.columns). This step is essential because machine learning models require identical feature structures \\nduring training and prediction phases. If a feature exists in training data but not in test data, \\nthe model would fail during inference. \\nThis defensive programming approach prevents runtime errors and ensures that the model can successfully \\nmake predictions on the test set.\\n\\nThis two-step filtering process - first removing inappropriate columns, \\nthen ensuring train-test consistency - represents a best practice in machine learning pipelines. \\nIt creates a robust feature set that avoids data leakage while maintaining compatibility across different data splits, \\nwhich is particularly important in time-series financial prediction tasks where the test set represents \\nfuture market conditions.\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Feature Preparation\n",
    "excluded = {'date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns'}\n",
    "feature_cols = [c for c in train_raw.columns if c not in excluded]\n",
    "feature_cols = [c for c in feature_cols if c in test_raw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc3e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis function implements a robust data preprocessing pipeline that handles missing values and data type inconsistencies \\nwhile preserving information about missingness patterns - a crucial technique in machine learning feature engineering.\\n\\nThe function begins by creating a copy of the input DataFrame to avoid modifying the original data, \\nfollowing defensive programming principles. It then iterates through each feature column to apply consistent \\npreprocessing steps. The first conditional check handles the case where an expected feature column \\nis completely missing from the DataFrame. Rather than failing, it gracefully creates the missing column filled \\nwith zeros and immediately creates a corresponding indicator variable set to 1, \\nsignaling that this entire feature was absent. \\nThis approach ensures model compatibility across datasets with different column structures.\\n\\nFor columns that exist in the DataFrame, the function employs different strategy_returnsegies based on data type. \\nThe condition df[c].dtype.kind in 'fiu' checks if the column is already numeric (float, integer, or unsigned integer) \\nusing pandas' dtype kind codes. For numeric columns, it retrieves the pre-computed median from the median_map dictionary, \\ncreates a binary indicator tracking which values were originally missing, fills the missing values with the median, \\nand stores the missingness indicator as a new feature column with the suffix _was_na.\\n\\nThe else clause handles columns that aren't recognized as numeric types, which commonly occurs with object columns \\ncontaining mixed data types or string representations of numbers. The function uses pd.to_numeric() with errors='coerce' \\nto attempt conversion to numeric format, where invalid values become NaN rather than causing errors. \\nAfter this conversion attempt, it applies the same median imputation and missingness indicator creation \\nprocess used for originally numeric columns.\\n\\nThis dual approach - median imputation combined with missingness indicators - is particularly valuable \\nbecause it prevents information loss. The median provides a robust central tendency measure that's \\nless sensitive to outliers than the mean, while the binary indicators allow the model to learn patterns \\nrelated to missingness itself. In financial data, missing values often carry meaningful information \\n(such as certain metrics not being available during market stress), \\nmaking these indicator features potentially predictive. \\nThe function's comprehensive error handling and type conversion ensure it can process datasets \\nwith inconsistent formatting while maintaining feature consistency across training and test sets.\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in feature_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "            df[f'{c}_was_na'] = 1\n",
    "            continue\n",
    "        if df[c].dtype.kind in 'fiu':\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4cf47989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 188\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train_raw, val_size=VALIDATION_SIZE)\n",
    "\n",
    "median_map = {c: float(train_df[c].median(skipna=True)) if train_df[c].dtype.kind in 'fiu' else 0.0 \n",
    "              for c in feature_cols}\n",
    "\n",
    "train_p = prepare_df(train_df, median_map, feature_cols)\n",
    "val_p   = prepare_df(val_df, median_map, feature_cols)\n",
    "test_p  = prepare_df(test_raw, median_map, feature_cols)\n",
    "\n",
    "final_features = [f for c in feature_cols for f in (c, f\"{c}_was_na\")]\n",
    "print(\"Number of features:\", len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5935853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015ae59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0cfa9923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 0.0108706\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's rmse: 0.0101998\n"
     ]
    }
   ],
   "source": [
    "## LightGBM Training\n",
    "train_data = lgb.Dataset(train_p[final_features], label=train_p['forward_returns'])\n",
    "val_data   = lgb.Dataset(val_p[final_features], label=val_p['forward_returns'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 63,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    num_boost_round=2000,\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39b02fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen scaling factor k=5.000 with Sharpe=0.57\n"
     ]
    }
   ],
   "source": [
    "## Volatility Scaling Calibration\n",
    "def strategy_stats(returns, exposures):\n",
    "    strategy_returns = exposures * returns\n",
    "    mean = np.nanmean(strategy_returns)\n",
    "    std  = np.nanstd(strategy_returns)\n",
    "    sharpe = (mean / (std + 1e-9)) * np.sqrt(252)\n",
    "    vol = std * np.sqrt(252)\n",
    "    return {'sharpe': sharpe, 'vol': vol}\n",
    "\n",
    "val_pred = model.predict(val_p[final_features], num_iteration=model.best_iteration)\n",
    "market_vol = np.nanstd(train_p['forward_returns']) * np.sqrt(252)\n",
    "\n",
    "best_k, best_sharpe = 0.1, -1e9\n",
    "for k in np.linspace(0.01, 5.0, 100):\n",
    "    exposures = np.clip((k * val_pred), 0, 2)\n",
    "    stats = strategy_stats(val_p['forward_returns'], exposures)\n",
    "    if stats['vol'] <= VOL_MULTIPLIER_LIMIT * market_vol and stats['sharpe'] > best_sharpe:\n",
    "        best_k = k\n",
    "        best_sharpe = stats['sharpe']\n",
    "\n",
    "print(f\"Chosen scaling factor k={best_k:.3f} with Sharpe={best_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_lgb_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "## Test Predictions + Smoothing\n",
    "test_pred = model.predict(test_p[final_features], num_iteration=model.best_iteration)\n",
    "\n",
    "alpha = 0.8\n",
    "smoothed_allocation = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "# replace in final submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test_p['date_id'],\n",
    "    'prediction': smoothed_allocation  \n",
    "})\n",
    "submission_df.to_csv(\"submission_lgb_fixed.csv\", index=False)\n",
    "print(\"Saved submission_lgb_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb838fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKaggle Evaluation Metric:\\n\\nstrategy_returns = risk_free_rate * (1 - position) + position * forward_returns\\n\\nif position = 0 → invest in risk-free asset,\\n\\nif position = 1 → invest like the market,\\n\\nif position = 2 → you are leveraged ×2 on the market.\\n\\n\\ndef score():\\n\\nstrategy_returns = rf * (1 - pos) + pos * fwd_returns\\n\\nIn the code, the calibration seeks the best Sharpe of the portfolio exposed to pos by calculating:\\n\\nstrat = exposures * returns\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kaggle Evaluation Metric:\n",
    "\n",
    "strategy_returns = risk_free_rate * (1 - position) + position * forward_returns\n",
    "\n",
    "if position = 0 → invest in risk-free asset,\n",
    "\n",
    "if position = 1 → invest like the market,\n",
    "\n",
    "if position = 2 → you are leveraged ×2 on the market.\n",
    "\n",
    "\n",
    "def score():\n",
    "\n",
    "strategy_returns = rf * (1 - pos) + pos * fwd_returns\n",
    "\n",
    "In the code, the calibration seeks the best Sharpe of the portfolio exposed to pos by calculating:\n",
    "\n",
    "strat = exposures * returns\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Kaggle Inference Server Wrapper\n",
    "\n",
    "# _model = model\n",
    "# _best_k = best_k\n",
    "# _history_returns = list(train_p['forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "# def predict(pl_df: pl.DataFrame) -> float:\n",
    "#     global _history_returns\n",
    "#     pdf = pl_df.to_pandas()\n",
    "#     pdf_p = prepare_df(pdf, median_map, feature_cols)\n",
    "#     for f in final_features:\n",
    "#         if f not in pdf_p.columns:\n",
    "#             pdf_p[f] = 0.0\n",
    "#     x = pdf_p[final_features].to_numpy()\n",
    "#     pred = _model.predict(x, num_iteration=_model.best_iteration)[0]\n",
    "#     vol_est = np.std(_history_returns) or 1e-3\n",
    "#     allocation = float(np.clip((_best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "#     if 'lagged_forward_returns' in pl_df.columns:\n",
    "#         try:\n",
    "#             _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "#         except:\n",
    "#             _history_returns.append(0.0)\n",
    "#     _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "#     return allocation\n",
    "\n",
    "# inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     inference_server.serve()\n",
    "# else:\n",
    "#     inference_server.run_local_gateway((str(DATA_DIR),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcbe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115cdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
