{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d70b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18d795b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8990, 98), (10, 99))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Configuration and Data Loading (kaggle_evaluation only)\n",
    "# import kaggle_evaluation.default_inference_server as kdeval\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "# Read CSV files from data_path\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "import random\n",
    "RANDOM_SEED = random.randint(1, 10000)\n",
    "\n",
    "VOL_MULTIPLIER_LIMIT = 1.2\n",
    "VOL_WINDOW = 20\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "941f378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Preparation\n",
    "excluded = {'date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns'}\n",
    "feature_cols = ['m4', 'm11', 'v13', 's2', 'p6', 'm12', 's1', 'v9', 'e17', 'p7']\n",
    "feature_cols = [c for c in feature_cols if c in test_raw.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9fc3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in feature_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "            df[f'{c}_was_na'] = 1\n",
    "            continue\n",
    "        if df[c].dtype.kind in 'fiu':\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cf47989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 0\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train_raw, val_size=VALIDATION_SIZE)\n",
    "\n",
    "median_map = {c: float(train_df[c].median(skipna=True)) if train_df[c].dtype.kind in 'fiu' else 0.0 \n",
    "              for c in feature_cols}\n",
    "\n",
    "train_p = prepare_df(train_df, median_map, feature_cols)\n",
    "val_p   = prepare_df(val_df, median_map, feature_cols)\n",
    "test_p  = prepare_df(test_raw, median_map, feature_cols)\n",
    "\n",
    "final_features = [f for c in feature_cols for f in (c, f\"{c}_was_na\")]\n",
    "print(\"Number of features:\", len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5935853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c9d2dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in X_train: 0.0\n",
      "NaN values in y_train: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "at least one array or dtype is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Crea e allena il modello\u001b[39;00m\n\u001b[32m     22\u001b[39m model = BayesianRidge()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Valutazione sul validation set\u001b[39;00m\n\u001b[32m     26\u001b[39m val_pred = model.predict(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\linear_model\\_bayes.py:296\u001b[39m, in \u001b[36mBayesianRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[32m    275\u001b[39m \n\u001b[32m    276\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    292\u001b[39m \u001b[33;03m    Returns the instance itself.\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m max_iter = _deprecate_n_iter(\u001b[38;5;28mself\u001b[39m.n_iter, \u001b[38;5;28mself\u001b[39m.max_iter)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m X, y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m dtype = X.dtype\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:650\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    648\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m    649\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    651\u001b[39m     out = X, y\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\utils\\validation.py:1263\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1258\u001b[39m         estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1259\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1260\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1261\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1266\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1281\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    875\u001b[39m pandas_requires_conversion = \u001b[38;5;28many\u001b[39m(\n\u001b[32m    876\u001b[39m     _pandas_dtype_needs_early_conversion(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[32m    877\u001b[39m )\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np.dtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     dtype_orig = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdtypes_orig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m pandas_requires_conversion \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(d == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Force object if any of the dtypes is an object\u001b[39;00m\n\u001b[32m    882\u001b[39m     dtype_orig = \u001b[38;5;28mobject\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: at least one array or dtype is required"
     ]
    }
   ],
   "source": [
    "# === Bayesian Ridge Training ===\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepara i dati (usando gli stessi DataFrame)\n",
    "X_train = train_p[final_features]\n",
    "y_train = train_p['forward_returns']\n",
    "X_val   = val_p[final_features]\n",
    "y_val   = val_p['forward_returns']\n",
    "\n",
    "# Check for NaN values and handle them\n",
    "print(f\"NaN values in X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"NaN values in y_train: {y_train.isnull().sum()}\")\n",
    "\n",
    "# Fill any remaining NaN values\n",
    "X_train = X_train.fillna(0)\n",
    "y_train = y_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "y_val = y_val.fillna(0)\n",
    "\n",
    "# Crea e allena il modello\n",
    "model = BayesianRidge()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Valutazione sul validation set\n",
    "val_pred = model.predict(X_val)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "print(f\"Validation RMSE: {rmse_val:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e627045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market annualized volatility: 0.1698\n",
      "Chosen scaling factor k=5.000 | Validation Sharpe=0.69\n"
     ]
    }
   ],
   "source": [
    "# === Volatility Scaling Calibration (BayesianRidge version) ===\n",
    "def strategy_stats(returns, exposures):\n",
    "    \"\"\"Compute Sharpe and volatility for a given exposure series.\"\"\"\n",
    "    strat = exposures * returns\n",
    "    mean = np.nanmean(strat)\n",
    "    std  = np.nanstd(strat)\n",
    "    sharpe = (mean / (std + 1e-9)) * np.sqrt(252)\n",
    "    vol = std * np.sqrt(252)\n",
    "    return {'sharpe': sharpe, 'vol': vol}\n",
    "\n",
    "# Ensure validation data has no NaN values\n",
    "val_features_clean = val_p[final_features].fillna(0)\n",
    "\n",
    "# Predict mean and std (Bayesian posterior uncertainty)\n",
    "val_pred_mean, val_pred_std = model.predict(val_features_clean, return_std=True)\n",
    "\n",
    "# Define market volatility for scaling reference\n",
    "market_vol = np.nanstd(train_p['forward_returns']) * np.sqrt(252)\n",
    "print(f\"Market annualized volatility: {market_vol:.4f}\")\n",
    "\n",
    "# We'll use a dynamic confidence weight: confidence = 1 / (1 + std)\n",
    "confidence = 1 / (1 + val_pred_std)\n",
    "val_conf_adj = val_pred_mean * confidence  # lower exposure when uncertainty is high\n",
    "\n",
    "# Grid search for best scaling factor k (Sharpe ratio under vol constraint)\n",
    "best_k, best_sharpe = 0.1, -1e9\n",
    "for k in np.linspace(0.01, 5.0, 100):\n",
    "    exposures = np.clip((k * val_conf_adj), 0, 2)\n",
    "    stats = strategy_stats(val_p['forward_returns'], exposures)\n",
    "    if stats['vol'] <= VOL_MULTIPLIER_LIMIT * market_vol and stats['sharpe'] > best_sharpe:\n",
    "        best_k = k\n",
    "        best_sharpe = stats['sharpe']\n",
    "\n",
    "print(f\"Chosen scaling factor k={best_k:.3f} | Validation Sharpe={best_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_br.csv\n"
     ]
    }
   ],
   "source": [
    "## Test Predictions + Smoothing\n",
    "test_features_clean = test_p[final_features].fillna(0)\n",
    "test_pred = model.predict(test_features_clean)\n",
    "\n",
    "alpha = 0.8\n",
    "smoothed_allocation = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "# replace in final submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test_p['date_id'],\n",
    "    'prediction': smoothed_allocation  \n",
    "})\n",
    "# submission_df.to_csv(\"submission_br.csv\", index=False)\n",
    "print(\"Saved submission_br.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb838fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kaggle Evaluation Metric:\n",
    "\n",
    "strategy_returns = risk_free_rate * (1 - position) + position * forward_returns\n",
    "\n",
    "if position = 0 → invest in risk-free asset,\n",
    "\n",
    "if position = 1 → invest like the market,\n",
    "\n",
    "if position = 2 → you are leveraged ×2 on the market.\n",
    "\n",
    "\n",
    "def score():\n",
    "\n",
    "strategy_returns = rf * (1 - pos) + pos * fwd_returns\n",
    "\n",
    "In the code, the calibration seeks the best Sharpe of the portfolio exposed to pos by calculating:\n",
    "\n",
    "strat = exposures * returns\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Kaggle Inference Server Wrapper (BayesianRidge version) ===\n",
    "\n",
    "# _model = model                     # BayesianRidge fitted model\n",
    "# _best_k = best_k                   # scaling factor from validation calibration\n",
    "# _history_returns = list(train_p['forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "# def predict(pl_df: pl.DataFrame) -> float:\n",
    "#     \"\"\"Predict allocation for one timestep (Kaggle competition API).\"\"\"\n",
    "#     global _history_returns\n",
    "#     # Convert Polars → Pandas\n",
    "#     pdf = pl_df.to_pandas()\n",
    "    \n",
    "#     # Apply same preprocessing\n",
    "#     pdf_p = prepare_df(pdf, median_map, feature_cols)\n",
    "#     for f in final_features:\n",
    "#         if f not in pdf_p.columns:\n",
    "#             pdf_p[f] = 0.0\n",
    "    \n",
    "#     # Convert to NumPy and predict\n",
    "#     x = pdf_p[final_features].to_numpy()\n",
    "\n",
    "#     \"\"\"\n",
    "#     added standard deviation based confidence adjustment\n",
    "#     to reduce allocation when uncertainty is high\n",
    "#     \"\"\"\n",
    "\n",
    "#     pred, std = model.predict(x, return_std=True)\n",
    "    \n",
    "#     # Compute rolling volatility estimate\n",
    "#     vol_est = np.std(_history_returns) or 1e-3\n",
    "#     confidence = 1 / (1 + std)\n",
    "#     alloc = float(np.clip(_best_k * pred * confidence / (vol_est + 1e-9), 0, 2))\n",
    "    \n",
    "#     # Update history (for volatility tracking)\n",
    "#     if 'lagged_forward_returns' in pl_df.columns:\n",
    "#         try:\n",
    "#             _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "#         except:\n",
    "#             _history_returns.append(0.0)\n",
    "#     else:\n",
    "#         _history_returns.append(0.0)\n",
    "    \n",
    "#     # Keep only the last VOL_WINDOW entries\n",
    "#     _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "#     return alloc\n",
    "\n",
    "# # Instantiate the Kaggle inference server\n",
    "# server = kdeval.DefaultInferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     server.serve()\n",
    "# else:\n",
    "#     server.run_local_gateway((str(DATA_DIR),))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcbe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115cdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
