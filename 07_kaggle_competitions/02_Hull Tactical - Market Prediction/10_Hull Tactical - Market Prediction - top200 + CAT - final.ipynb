{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b72ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment - kaggle_evaluation not available\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  HULL TACTICAL MARKET PREDICTION — 200 TOP FEATURES + CAT BOOST\n",
    "# ================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from typing import Dict \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import time\n",
    "\n",
    "# Try to import kaggle_evaluation, handle if not available\n",
    "try:\n",
    "    import kaggle_evaluation.default_inference_server as kdeval\n",
    "    KAGGLE_ENV = True\n",
    "    print(\"Running in Kaggle competition environment\")\n",
    "except ImportError:\n",
    "    KAGGLE_ENV = False\n",
    "    print(\"Running in local environment - kaggle_evaluation not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00d6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from local environment\n",
      "Data loaded successfully\n",
      "Train shape: (9021, 98) | Test shape: (10, 99)\n",
      "Base features available: 94\n",
      "Target variable: market_forward_excess_returns\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Data Loading & Initial Feature Preparation\n",
    "# ================================================================\n",
    "\n",
    "## Configuration and Data Loading\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "TARGET = \"market_forward_excess_returns\"\n",
    "drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\"]\n",
    "VOL_WINDOW = 20        # volatility window in days\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    \"\"\"Split data chronologically for time series validation.\"\"\"\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "# Load train/test data using the KAGGLE_ENV variable from cell 1\n",
    "if KAGGLE_ENV:\n",
    "    print(\"Loading data from Kaggle environment\")\n",
    "    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "    test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "else:\n",
    "    print(\"Loading data from local environment\")\n",
    "    # Try different possible local paths\n",
    "    local_paths = [\n",
    "        DATA_DIR / \"train.csv\",\n",
    "        Path(\"01_data/train.csv\"),\n",
    "        Path(\"train.csv\")\n",
    "    ]\n",
    "    \n",
    "    train_path = None\n",
    "    test_path = None\n",
    "    \n",
    "    for path in local_paths:\n",
    "        if path.exists():\n",
    "            train_path = path\n",
    "            test_path = path.parent / \"test.csv\"\n",
    "            break\n",
    "    \n",
    "    if train_path is None or not test_path.exists():\n",
    "        raise FileNotFoundError(\"Could not find train.csv and test.csv files in expected locations\")\n",
    "    \n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "train = train.sort_values(\"date_id\").reset_index(drop=True)\n",
    "test = test.sort_values(\"date_id\").reset_index(drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "train = train.fillna(0.0)\n",
    "test = test.fillna(0.0)\n",
    "\n",
    "# Base features (before advanced transformations)\n",
    "base_features = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "\n",
    "print(f\"Base features available: {len(base_features)}\")\n",
    "print(f\"Target variable: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e94f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and prepare DataFrame by handling missing values intelligently.\n",
    "    \n",
    "    Strategy:\n",
    "    - Use median imputation for numeric columns with some missing values\n",
    "    - Use zero-fill for columns with very few missing values  \n",
    "    - Only process existing columns (no synthetic data creation)\n",
    "\n",
    "    Args:\n",
    "    df: Input DataFrame\n",
    "    median_map: Dictionary mapping column names to median values\n",
    "    feature_cols: List of feature column names to process\n",
    "\n",
    "    Returns:\n",
    "    Cleaned DataFrame\n",
    "\n",
    "    Median is much less sensitive to extreme values (outliers)\n",
    "    Mean can be heavily skewed by a few very large or very small values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Only work with columns that actually exist in the DataFrame\n",
    "    existing_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    if not existing_cols:\n",
    "        print(\"Warning: No feature columns found in DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    # Calculate missing percentages for existing columns\n",
    "    missing_pct = (df[existing_cols].isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # Categorize columns by missing percentage\n",
    "    cols_fill_median = missing_pct[(missing_pct > 5) & (missing_pct <= 50)].index.tolist()\n",
    "    cols_fill_zero = missing_pct[missing_pct <= 5].index.tolist()\n",
    "    \n",
    "    # Apply median imputation for moderately missing columns\n",
    "    if cols_fill_median:\n",
    "        for col in cols_fill_median:\n",
    "            median_val = median_map.get(col, df[col].median())\n",
    "            if pd.isna(median_val):  # Handle case where median is NaN\n",
    "                median_val = 0.0\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Apply zero-fill for low missing columns\n",
    "    if cols_fill_zero:\n",
    "        df[cols_fill_zero] = df[cols_fill_zero].fillna(0)\n",
    "    \n",
    "    # Ensure all feature columns are numeric\n",
    "    for col in existing_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    # Final cleanup - ensure no inf values\n",
    "    df[existing_cols] = df[existing_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c643414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: Train 6321 | Validation 2700 rows\n",
      "Preprocessing complete\n",
      "Number of base features: 94\n",
      "Base features available: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1']...\n",
      "Target variable 'market_forward_excess_returns' extracted separately\n",
      "Features and target prepared separately to avoid data leakage\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train, val_size=VALIDATION_SIZE)\n",
    "print(f\"Data split: Train {train_df.shape[0]} | Validation {val_df.shape[0]} rows\")\n",
    "\n",
    "# Create median map from training portion only\n",
    "median_map = {}\n",
    "for c in base_features:\n",
    "    if c in train_df.columns:\n",
    "        if train_df[c].dtype.kind in 'fiu':  # numeric types\n",
    "            median_val = train_df[c].median(skipna=True)\n",
    "            median_map[c] = float(median_val) if not pd.isna(median_val) else 0.0\n",
    "        else:\n",
    "            median_map[c] = 0.0\n",
    "    else:\n",
    "        median_map[c] = 0.0\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "train_full = prepare_df(train_df, median_map, base_features)\n",
    "val_full   = prepare_df(val_df, median_map, base_features)\n",
    "test_full  = prepare_df(test, median_map, base_features)\n",
    "\n",
    "# Extract only the base features (remove drop_cols and target)\n",
    "final_features = [c for c in base_features if c in train_full.columns]\n",
    "train_p = train_full[final_features].copy()\n",
    "val_p   = val_full[final_features].copy()\n",
    "test_p  = test_full[final_features].copy()\n",
    "\n",
    "# Keep target and other columns separate for later use\n",
    "train_target = train_full[TARGET].copy()\n",
    "val_target   = val_full[TARGET].copy()\n",
    "\n",
    "# Validation check\n",
    "if not final_features:\n",
    "    raise ValueError(\"No features available after preprocessing!\")\n",
    "\n",
    "print(f\"Preprocessing complete\")\n",
    "print(f\"Number of base features: {len(final_features)}\")\n",
    "print(f\"Base features available: {final_features[:10]}...\" if len(final_features) > 10 else f\"Features: {final_features}\")\n",
    "\n",
    "print(f\"Target variable '{TARGET}' extracted separately\")\n",
    "print(\"Features and target prepared separately to avoid data leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dcb7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Advanced Feature Factory (Enhanced) =====\n",
    "def create_advanced_features(df,\n",
    "                             top_features,\n",
    "                             macro_prefixes=('M','V','P','S'),\n",
    "                             window_sizes=(5,10),\n",
    "                             shift=1,  # Added shift parameter\n",
    "                             inplace=False):\n",
    "    \"\"\"\n",
    "    Create advanced features following a two-level approach:\n",
    "      1) Lightweight Core Features (applied to `top_features`)\n",
    "      2) Macro-Context Features (applied to columns starting with macro_prefixes)\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        top_features: List of most important features for Level 1 processing\n",
    "        macro_prefixes: Tuple of prefixes for Level 2 features\n",
    "        window_sizes: Rolling window sizes\n",
    "        shift: Number of periods to shift for avoiding data leakage\n",
    "        inplace: Whether to modify DataFrame in place\n",
    "    \n",
    "    Returns:\n",
    "        df_out: DataFrame with new features (and original columns)\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # Ensure datetime-like ordering by date_id if present\n",
    "    if 'date_id' in df.columns:\n",
    "        df = df.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "    # Helper: ensure numeric dtype for selected cols\n",
    "    def _to_numeric(cols):\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # ------------- Level 1: Core Features (top_features) -------------\n",
    "    # Function to calculate rolling statistics and distance to rolling mean\n",
    "    def create_rolling_and_distance_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling statistics and distance features efficiently using shared roll object\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll = shifted_col.rolling(window=w, min_periods=1)  # Create roll object once\n",
    "            \n",
    "                # Calculate all rolling statistics from the same roll object\n",
    "                roll_mean = roll.mean()\n",
    "                roll_std = roll.std().fillna(0.0)\n",
    "                roll_median = roll.median()\n",
    "                roll_max = roll.max()\n",
    "                roll_min = roll.min()\n",
    "            \n",
    "                # Store rolling features\n",
    "                df[f\"{c}_rolling_mean_{w}\"] = roll_mean.astype('float32')\n",
    "                df[f\"{c}_rolling_std_{w}\"] = roll_std.astype('float32')\n",
    "                df[f\"{c}_rolling_median_{w}\"] = roll_median.astype('float32')\n",
    "                df[f\"{c}_rolling_max_{w}\"] = roll_max.astype('float32')\n",
    "                df[f\"{c}_rolling_min_{w}\"] = roll_min.astype('float32')\n",
    "            \n",
    "                # Calculate distance to rolling mean using the same roll_mean\n",
    "                df[f\"{c}_dist_to_rolling_mean_{w}\"] = (df[c] - roll_mean).astype('float32')\n",
    "    \n",
    "    # Function to calculate variance features\n",
    "    def create_variance_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling variance features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_var = shifted_col.rolling(window=w, min_periods=1).var().fillna(0.0)\n",
    "                \n",
    "                df[f\"{c}_rolling_var_{w}\"] = roll_var.astype('float32')\n",
    "\n",
    "    # Function to calculate z-score features\n",
    "    def create_zscore_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling z-scores with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_mean = shifted_col.rolling(window=w, min_periods=1).mean()\n",
    "                roll_std = shifted_col.rolling(window=w, min_periods=1).std().fillna(0.0)\n",
    "                \n",
    "                df[f\"{c}_z_{w}\"] = ((df[c] - roll_mean) / (roll_std + 1e-9)).astype('float32')\n",
    "\n",
    "    # function to calculate zscore from scipy.stats\n",
    "    def create_scipy_zscore_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create z-score features using scipy.stats.zscore with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            zscored = zscore(shifted_col)\n",
    "            df[f\"{c}_scipy_zscore\"] = zscored.astype('float32').fillna(0.0)\n",
    "\n",
    "    # Function to calculate spread and percentage change features\n",
    "    def create_spread_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create spread and percentage change features\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            # Use proper shift for difference calculations\n",
    "            df[f\"{c}_diff_1\"] = (df[c] - df[c].shift(shift_periods)).astype('float32')\n",
    "            df[f\"{c}_pctchg_1\"] = (df[c].pct_change(periods=shift_periods).fillna(0.0)).astype('float32')\n",
    "\n",
    "    # Function to calculate numerical PACF values to be added as additional features\n",
    "    def create_pacf_features(cols, nlags=10, shift_periods=shift):\n",
    "        \"\"\"Create PACF features for selected columns\"\"\"\n",
    "        from statsmodels.tsa.stattools import pacf\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            # Change from 'ywunbiased' to 'yw' (Yule-Walker method)\n",
    "            pacf_values = pacf(shifted_col, nlags=nlags, method='yw')\n",
    "            for lag in range(1, nlags + 1):\n",
    "                df[f\"{c}_pacf_{lag}\"] = pacf_values[lag]\n",
    "\n",
    "    # Function to calculate numerical ACF values to be added as additional features\n",
    "    def create_acf_features(cols, nlags=10, shift_periods=shift):\n",
    "        \"\"\"Create ACF features for selected columns\"\"\"\n",
    "        from statsmodels.tsa.stattools import acf\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            acf_values = acf(shifted_col, nlags=nlags, fft=False)\n",
    "            for lag in range(1, nlags + 1):\n",
    "                df[f\"{c}_acf_{lag}\"] = acf_values[lag]\n",
    "\n",
    "    # Function to calculate pandas autocorr values to be added as additional features\n",
    "    def create_autocorr_features(cols, lags=10, shift_periods=shift):\n",
    "        \"\"\"Create autocorrelation features for selected columns\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for lag in range(1, lags + 1):\n",
    "                df[f\"{c}_autocorr_{lag}\"] = df[c].autocorr(lag=lag)\n",
    "\n",
    "    # Function to calculate skewness and kurtosis features\n",
    "    def create_skewness_kurtosis_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create skewness and kurtosis features for selected columns\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods)\n",
    "            df[f\"{c}_skewness\"] = shifted_col.rolling(window=30, min_periods=1).skew().astype('float32').fillna(0.0)\n",
    "            df[f\"{c}_kurtosis\"] = shifted_col.rolling(window=30, min_periods=1).kurt().astype('float32').fillna(0.0)\n",
    "\n",
    "    # Function to calculate momentum features\n",
    "    def create_momentum_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create momentum features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                df[f\"{c}_momentum_{w}\"] = (shifted_col - shifted_col.shift(w)).astype('float32')\n",
    "\n",
    "    # Function to calculate distance from value to momentum\n",
    "    def create_distance_to_momentum_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create distance to momentum features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                momentum = shifted_col - shifted_col.shift(w)\n",
    "                df[f\"{c}_dist_to_momentum_{w}\"] = (df[c] - momentum).astype('float32')\n",
    "\n",
    "    # Function to calculate difference series\n",
    "    def create_difference_features(cols, lags=(1,5,10), shift_periods=shift):\n",
    "        \"\"\"Create difference features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for lag in lags:\n",
    "                df[f\"{c}_diff_{lag}\"] = (df[c] - df[c].shift(lag + shift_periods)).astype('float32')\n",
    "\n",
    "    # Function to calculate normalized series\n",
    "    def create_normalized_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create normalized features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods)\n",
    "            df[f\"{c}_normalized\"] = (shifted_col - shifted_col.mean()) / shifted_col.std()\n",
    "\n",
    "    # Function to calculate rolling sums features\n",
    "    def create_rolling_sum_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create rolling sums with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                df[f\"{c}_macro_rolling_sum_{w}\"] = shifted_col.rolling(window=w, min_periods=1).sum().astype('float32')            \n",
    "\n",
    "    # Function to calculate cumsum features\n",
    "    def create_cumsum_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create cumulative sum features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods)\n",
    "            df[f\"{c}_cumsum\"] = shifted_col.cumsum().astype('float32')\n",
    "\n",
    "    # Function to calculate Hurst exponent features\n",
    "    def create_hurst_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create Hurst exponent features with proper shift\"\"\"\n",
    "        \"\"\"\n",
    "        The Hurst exponent uses lags to measure the long-term memory of the time series. \n",
    "        For each lag in the range, calculate the standard deviation of the differenced series. \n",
    "        Then calculate the slope of the log lags versus the standard deviations. \n",
    "        You can do this by returning the first value from NumPy’s polyfit function \n",
    "        which fits a first-degree polynomial function.\n",
    "        \n",
    "        The Hurst exponent ranges between 0 and 1.\n",
    "\n",
    "        If the Hurst exponent is below 0.5, the market is mean reverting. \n",
    "        Reversal strategies win in these markets.\n",
    "\n",
    "        If the Hurst exponent of 0.5 means the market is random. \n",
    "        In this case, a trading strategy that relies on the market direction will lose money.\n",
    "\n",
    "        If the Hurst exponent is above 0.5 the market is trending. \n",
    "        Markets with a high Hurst exponent are perfect for trend-following strategies.\n",
    "        \"\"\"\n",
    "        def hurst_exponent(ts):\n",
    "            lags = range(2, 20)\n",
    "            tau = [np.std(np.subtract(ts[lag:], ts[:-lag])) for lag in lags]\n",
    "            poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "            return poly[0] * 2.0\n",
    "        \n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            df[f\"{c}_hurst\"] = hurst_exponent(shifted_col)\n",
    "\n",
    "    # Function to calculate lagged observations from the past\n",
    "    def create_lagged_features(cols, lags=(1,5,10), shift_periods=shift):\n",
    "        \"\"\"Create lagged features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for lag in lags:\n",
    "                df[f\"{c}_lag_{lag}\"] = df[c].shift(lag + shift_periods).astype('float32')\n",
    "\n",
    "    # ------------- Level 2: Macro Features (selective) -------------\n",
    "    # Function to calculate correlation features\n",
    "    def create_correlation_features(pairs=None, window=30, shift_periods=shift):\n",
    "        \"\"\"Create rolling correlations with proper shift\"\"\"\n",
    "        if pairs is None:\n",
    "            # Build pairs from top_features (limit to avoid explosion)\n",
    "            cand = []\n",
    "            for i in range(len(top_features)):\n",
    "                for j in range(i+1, len(top_features)):\n",
    "                    cand.append((top_features[i], top_features[j]))\n",
    "            pairs = cand[:10]  # Limit to 10 pairs\n",
    "        \n",
    "        for a, b in pairs:\n",
    "            if a not in df.columns or b not in df.columns:\n",
    "                continue\n",
    "            # Apply shift to both series\n",
    "            a_shifted = df[a].shift(shift_periods)\n",
    "            b_shifted = df[b].shift(shift_periods)\n",
    "            corr = a_shifted.rolling(window=window, min_periods=1).corr(b_shifted)\n",
    "            df[f\"macro_corr_{a}_{b}_{30}\"] = corr.astype('float32').fillna(0.0)\n",
    "\n",
    "    # Function to calculate volatility spread features\n",
    "    def create_volatility_features(cols=None, windows=(20,60), shift_periods=shift):\n",
    "        \"\"\"Create volatility spread features with proper shift\"\"\"\n",
    "        if cols is None:\n",
    "            cols = [c for c in df.columns if c.startswith('v')]\n",
    "        \n",
    "        # Limit to prevent feature explosion\n",
    "        cols = cols[:8]\n",
    "        \n",
    "        for w in windows:\n",
    "            vols = {}\n",
    "            for c in cols:\n",
    "                if c in df.columns:\n",
    "                    shifted_col = df[c].shift(shift_periods)\n",
    "                    vols[c] = shifted_col.rolling(window=w, min_periods=1).std().astype('float32').fillna(0.0)\n",
    "            \n",
    "            # Create spread between consecutive volatilities\n",
    "            vol_keys = list(vols.keys())\n",
    "            for i in range(len(vol_keys) - 1):\n",
    "                a, b = vol_keys[i], vol_keys[i + 1]\n",
    "                df[f\"macro_volspread_{a}_{b}_{w}\"] = (vols[a] - vols[b]).astype('float32')\n",
    "\n",
    "    # Function to calculate high/low ratio features\n",
    "    def create_extremes_features(cols, windows=(20,60,120), shift_periods=shift):\n",
    "        \"\"\"Create high/low ratio features with proper shift\"\"\"\n",
    "        # Limit columns to prevent explosion\n",
    "        cols = [c for c in cols if c in df.columns][:10]\n",
    "        \n",
    "        for c in cols:\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_max = shifted_col.rolling(window=w, min_periods=1).max()\n",
    "                roll_min = shifted_col.rolling(window=w, min_periods=1).min()\n",
    "\n",
    "                df[f\"{c}_macro_high_ratio_{w}\"] = (df[c] / (roll_max + 1e-9)).astype('float32')\n",
    "                df[f\"{c}_macro_low_ratio_{w}\"] = (df[c] / (roll_min + 1e-9)).astype('float32')\n",
    "\n",
    "    # Execute feature creation\n",
    "    print(\"Creating Level 1 features (Core)...\")\n",
    "    _to_numeric(top_features)\n",
    "    create_rolling_and_distance_features(top_features)\n",
    "    create_variance_features(top_features)\n",
    "    create_zscore_features(top_features)\n",
    "    create_scipy_zscore_features(top_features)\n",
    "    create_spread_features(top_features)\n",
    "    create_pacf_features(top_features)\n",
    "    create_acf_features(top_features)\n",
    "    create_autocorr_features(top_features)\n",
    "    create_skewness_kurtosis_features(top_features)\n",
    "    create_momentum_features(top_features)\n",
    "    create_distance_to_momentum_features(top_features)\n",
    "    create_difference_features(top_features)\n",
    "    create_normalized_features(top_features)\n",
    "    create_rolling_sum_features(top_features)\n",
    "    create_cumsum_features(top_features)\n",
    "    create_hurst_features(top_features)\n",
    "    create_lagged_features(top_features)\n",
    "\n",
    "    print(\"Creating Level 2 features (Macro)...\")\n",
    "    macro_cols = [c for c in df.columns if any(c.startswith(pref) for pref in macro_prefixes)]\n",
    "    _to_numeric(macro_cols)\n",
    "    print('Macro columns for Level 2 features:', macro_cols)\n",
    "\n",
    "    create_correlation_features(window=30)\n",
    "    create_volatility_features(windows=(20,60))\n",
    "    create_extremes_features([c for c in df.columns if c.startswith(('m','p'))], windows=(20,60,120))\n",
    "\n",
    "    # Clean data\n",
    "    print(\"Cleaning and selecting features...\")\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0.0, inplace=True)\n",
    "\n",
    "    # Downcast to save memory\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    if len(float_cols) > 0:\n",
    "        df[float_cols] = df[float_cols].astype('float32')\n",
    "\n",
    "    print(f\"Feature engineering complete. Created {len(df.columns)} total columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b277eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "127f5c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features for training data...\n",
      "Columns for feature engineering (count): 95\n",
      "Included columns names: ['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']\n",
      "Excluded columns (count): 3\n",
      "Excluded columns (prevent leakage) names: ['market_forward_excess_returns', 'forward_returns', 'risk_free_rate']\n",
      "Creating Level 1 features (Core)...\n",
      "Creating Level 2 features (Macro)...\n",
      "Macro columns for Level 2 features: ['M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'M4_rolling_mean_5', 'M4_rolling_std_5', 'M4_rolling_median_5', 'M4_rolling_max_5', 'M4_rolling_min_5', 'M4_dist_to_rolling_mean_5', 'M4_rolling_mean_10', 'M4_rolling_std_10', 'M4_rolling_median_10', 'M4_rolling_max_10', 'M4_rolling_min_10', 'M4_dist_to_rolling_mean_10', 'M4_rolling_mean_20', 'M4_rolling_std_20', 'M4_rolling_median_20', 'M4_rolling_max_20', 'M4_rolling_min_20', 'M4_dist_to_rolling_mean_20', 'M4_rolling_mean_60', 'M4_rolling_std_60', 'M4_rolling_median_60', 'M4_rolling_max_60', 'M4_rolling_min_60', 'M4_dist_to_rolling_mean_60', 'M4_rolling_mean_120', 'M4_rolling_std_120', 'M4_rolling_median_120', 'M4_rolling_max_120', 'M4_rolling_min_120', 'M4_dist_to_rolling_mean_120', 'V13_rolling_mean_5', 'V13_rolling_std_5', 'V13_rolling_median_5', 'V13_rolling_max_5', 'V13_rolling_min_5', 'V13_dist_to_rolling_mean_5', 'V13_rolling_mean_10', 'V13_rolling_std_10', 'V13_rolling_median_10', 'V13_rolling_max_10', 'V13_rolling_min_10', 'V13_dist_to_rolling_mean_10', 'V13_rolling_mean_20', 'V13_rolling_std_20', 'V13_rolling_median_20', 'V13_rolling_max_20', 'V13_rolling_min_20', 'V13_dist_to_rolling_mean_20', 'V13_rolling_mean_60', 'V13_rolling_std_60', 'V13_rolling_median_60', 'V13_rolling_max_60', 'V13_rolling_min_60', 'V13_dist_to_rolling_mean_60', 'V13_rolling_mean_120', 'V13_rolling_std_120', 'V13_rolling_median_120', 'V13_rolling_max_120', 'V13_rolling_min_120', 'V13_dist_to_rolling_mean_120', 'M11_rolling_mean_5', 'M11_rolling_std_5', 'M11_rolling_median_5', 'M11_rolling_max_5', 'M11_rolling_min_5', 'M11_dist_to_rolling_mean_5', 'M11_rolling_mean_10', 'M11_rolling_std_10', 'M11_rolling_median_10', 'M11_rolling_max_10', 'M11_rolling_min_10', 'M11_dist_to_rolling_mean_10', 'M11_rolling_mean_20', 'M11_rolling_std_20', 'M11_rolling_median_20', 'M11_rolling_max_20', 'M11_rolling_min_20', 'M11_dist_to_rolling_mean_20', 'M11_rolling_mean_60', 'M11_rolling_std_60', 'M11_rolling_median_60', 'M11_rolling_max_60', 'M11_rolling_min_60', 'M11_dist_to_rolling_mean_60', 'M11_rolling_mean_120', 'M11_rolling_std_120', 'M11_rolling_median_120', 'M11_rolling_max_120', 'M11_rolling_min_120', 'M11_dist_to_rolling_mean_120', 'S2_rolling_mean_5', 'S2_rolling_std_5', 'S2_rolling_median_5', 'S2_rolling_max_5', 'S2_rolling_min_5', 'S2_dist_to_rolling_mean_5', 'S2_rolling_mean_10', 'S2_rolling_std_10', 'S2_rolling_median_10', 'S2_rolling_max_10', 'S2_rolling_min_10', 'S2_dist_to_rolling_mean_10', 'S2_rolling_mean_20', 'S2_rolling_std_20', 'S2_rolling_median_20', 'S2_rolling_max_20', 'S2_rolling_min_20', 'S2_dist_to_rolling_mean_20', 'S2_rolling_mean_60', 'S2_rolling_std_60', 'S2_rolling_median_60', 'S2_rolling_max_60', 'S2_rolling_min_60', 'S2_dist_to_rolling_mean_60', 'S2_rolling_mean_120', 'S2_rolling_std_120', 'S2_rolling_median_120', 'S2_rolling_max_120', 'S2_rolling_min_120', 'S2_dist_to_rolling_mean_120', 'P6_rolling_mean_5', 'P6_rolling_std_5', 'P6_rolling_median_5', 'P6_rolling_max_5', 'P6_rolling_min_5', 'P6_dist_to_rolling_mean_5', 'P6_rolling_mean_10', 'P6_rolling_std_10', 'P6_rolling_median_10', 'P6_rolling_max_10', 'P6_rolling_min_10', 'P6_dist_to_rolling_mean_10', 'P6_rolling_mean_20', 'P6_rolling_std_20', 'P6_rolling_median_20', 'P6_rolling_max_20', 'P6_rolling_min_20', 'P6_dist_to_rolling_mean_20', 'P6_rolling_mean_60', 'P6_rolling_std_60', 'P6_rolling_median_60', 'P6_rolling_max_60', 'P6_rolling_min_60', 'P6_dist_to_rolling_mean_60', 'P6_rolling_mean_120', 'P6_rolling_std_120', 'P6_rolling_median_120', 'P6_rolling_max_120', 'P6_rolling_min_120', 'P6_dist_to_rolling_mean_120', 'M2_rolling_mean_5', 'M2_rolling_std_5', 'M2_rolling_median_5', 'M2_rolling_max_5', 'M2_rolling_min_5', 'M2_dist_to_rolling_mean_5', 'M2_rolling_mean_10', 'M2_rolling_std_10', 'M2_rolling_median_10', 'M2_rolling_max_10', 'M2_rolling_min_10', 'M2_dist_to_rolling_mean_10', 'M2_rolling_mean_20', 'M2_rolling_std_20', 'M2_rolling_median_20', 'M2_rolling_max_20', 'M2_rolling_min_20', 'M2_dist_to_rolling_mean_20', 'M2_rolling_mean_60', 'M2_rolling_std_60', 'M2_rolling_median_60', 'M2_rolling_max_60', 'M2_rolling_min_60', 'M2_dist_to_rolling_mean_60', 'M2_rolling_mean_120', 'M2_rolling_std_120', 'M2_rolling_median_120', 'M2_rolling_max_120', 'M2_rolling_min_120', 'M2_dist_to_rolling_mean_120', 'M9_rolling_mean_5', 'M9_rolling_std_5', 'M9_rolling_median_5', 'M9_rolling_max_5', 'M9_rolling_min_5', 'M9_dist_to_rolling_mean_5', 'M9_rolling_mean_10', 'M9_rolling_std_10', 'M9_rolling_median_10', 'M9_rolling_max_10', 'M9_rolling_min_10', 'M9_dist_to_rolling_mean_10', 'M9_rolling_mean_20', 'M9_rolling_std_20', 'M9_rolling_median_20', 'M9_rolling_max_20', 'M9_rolling_min_20', 'M9_dist_to_rolling_mean_20', 'M9_rolling_mean_60', 'M9_rolling_std_60', 'M9_rolling_median_60', 'M9_rolling_max_60', 'M9_rolling_min_60', 'M9_dist_to_rolling_mean_60', 'M9_rolling_mean_120', 'M9_rolling_std_120', 'M9_rolling_median_120', 'M9_rolling_max_120', 'M9_rolling_min_120', 'M9_dist_to_rolling_mean_120', 'P8_rolling_mean_5', 'P8_rolling_std_5', 'P8_rolling_median_5', 'P8_rolling_max_5', 'P8_rolling_min_5', 'P8_dist_to_rolling_mean_5', 'P8_rolling_mean_10', 'P8_rolling_std_10', 'P8_rolling_median_10', 'P8_rolling_max_10', 'P8_rolling_min_10', 'P8_dist_to_rolling_mean_10', 'P8_rolling_mean_20', 'P8_rolling_std_20', 'P8_rolling_median_20', 'P8_rolling_max_20', 'P8_rolling_min_20', 'P8_dist_to_rolling_mean_20', 'P8_rolling_mean_60', 'P8_rolling_std_60', 'P8_rolling_median_60', 'P8_rolling_max_60', 'P8_rolling_min_60', 'P8_dist_to_rolling_mean_60', 'P8_rolling_mean_120', 'P8_rolling_std_120', 'P8_rolling_median_120', 'P8_rolling_max_120', 'P8_rolling_min_120', 'P8_dist_to_rolling_mean_120', 'P7_rolling_mean_5', 'P7_rolling_std_5', 'P7_rolling_median_5', 'P7_rolling_max_5', 'P7_rolling_min_5', 'P7_dist_to_rolling_mean_5', 'P7_rolling_mean_10', 'P7_rolling_std_10', 'P7_rolling_median_10', 'P7_rolling_max_10', 'P7_rolling_min_10', 'P7_dist_to_rolling_mean_10', 'P7_rolling_mean_20', 'P7_rolling_std_20', 'P7_rolling_median_20', 'P7_rolling_max_20', 'P7_rolling_min_20', 'P7_dist_to_rolling_mean_20', 'P7_rolling_mean_60', 'P7_rolling_std_60', 'P7_rolling_median_60', 'P7_rolling_max_60', 'P7_rolling_min_60', 'P7_dist_to_rolling_mean_60', 'P7_rolling_mean_120', 'P7_rolling_std_120', 'P7_rolling_median_120', 'P7_rolling_max_120', 'P7_rolling_min_120', 'P7_dist_to_rolling_mean_120', 'S12_rolling_mean_5', 'S12_rolling_std_5', 'S12_rolling_median_5', 'S12_rolling_max_5', 'S12_rolling_min_5', 'S12_dist_to_rolling_mean_5', 'S12_rolling_mean_10', 'S12_rolling_std_10', 'S12_rolling_median_10', 'S12_rolling_max_10', 'S12_rolling_min_10', 'S12_dist_to_rolling_mean_10', 'S12_rolling_mean_20', 'S12_rolling_std_20', 'S12_rolling_median_20', 'S12_rolling_max_20', 'S12_rolling_min_20', 'S12_dist_to_rolling_mean_20', 'S12_rolling_mean_60', 'S12_rolling_std_60', 'S12_rolling_median_60', 'S12_rolling_max_60', 'S12_rolling_min_60', 'S12_dist_to_rolling_mean_60', 'S12_rolling_mean_120', 'S12_rolling_std_120', 'S12_rolling_median_120', 'S12_rolling_max_120', 'S12_rolling_min_120', 'S12_dist_to_rolling_mean_120', 'P13_rolling_mean_5', 'P13_rolling_std_5', 'P13_rolling_median_5', 'P13_rolling_max_5', 'P13_rolling_min_5', 'P13_dist_to_rolling_mean_5', 'P13_rolling_mean_10', 'P13_rolling_std_10', 'P13_rolling_median_10', 'P13_rolling_max_10', 'P13_rolling_min_10', 'P13_dist_to_rolling_mean_10', 'P13_rolling_mean_20', 'P13_rolling_std_20', 'P13_rolling_median_20', 'P13_rolling_max_20', 'P13_rolling_min_20', 'P13_dist_to_rolling_mean_20', 'P13_rolling_mean_60', 'P13_rolling_std_60', 'P13_rolling_median_60', 'P13_rolling_max_60', 'P13_rolling_min_60', 'P13_dist_to_rolling_mean_60', 'P13_rolling_mean_120', 'P13_rolling_std_120', 'P13_rolling_median_120', 'P13_rolling_max_120', 'P13_rolling_min_120', 'P13_dist_to_rolling_mean_120', 'V9_rolling_mean_5', 'V9_rolling_std_5', 'V9_rolling_median_5', 'V9_rolling_max_5', 'V9_rolling_min_5', 'V9_dist_to_rolling_mean_5', 'V9_rolling_mean_10', 'V9_rolling_std_10', 'V9_rolling_median_10', 'V9_rolling_max_10', 'V9_rolling_min_10', 'V9_dist_to_rolling_mean_10', 'V9_rolling_mean_20', 'V9_rolling_std_20', 'V9_rolling_median_20', 'V9_rolling_max_20', 'V9_rolling_min_20', 'V9_dist_to_rolling_mean_20', 'V9_rolling_mean_60', 'V9_rolling_std_60', 'V9_rolling_median_60', 'V9_rolling_max_60', 'V9_rolling_min_60', 'V9_dist_to_rolling_mean_60', 'V9_rolling_mean_120', 'V9_rolling_std_120', 'V9_rolling_median_120', 'V9_rolling_max_120', 'V9_rolling_min_120', 'V9_dist_to_rolling_mean_120', 'P1_rolling_mean_5', 'P1_rolling_std_5', 'P1_rolling_median_5', 'P1_rolling_max_5', 'P1_rolling_min_5', 'P1_dist_to_rolling_mean_5', 'P1_rolling_mean_10', 'P1_rolling_std_10', 'P1_rolling_median_10', 'P1_rolling_max_10', 'P1_rolling_min_10', 'P1_dist_to_rolling_mean_10', 'P1_rolling_mean_20', 'P1_rolling_std_20', 'P1_rolling_median_20', 'P1_rolling_max_20', 'P1_rolling_min_20', 'P1_dist_to_rolling_mean_20', 'P1_rolling_mean_60', 'P1_rolling_std_60', 'P1_rolling_median_60', 'P1_rolling_max_60', 'P1_rolling_min_60', 'P1_dist_to_rolling_mean_60', 'P1_rolling_mean_120', 'P1_rolling_std_120', 'P1_rolling_median_120', 'P1_rolling_max_120', 'P1_rolling_min_120', 'P1_dist_to_rolling_mean_120', 'S8_rolling_mean_5', 'S8_rolling_std_5', 'S8_rolling_median_5', 'S8_rolling_max_5', 'S8_rolling_min_5', 'S8_dist_to_rolling_mean_5', 'S8_rolling_mean_10', 'S8_rolling_std_10', 'S8_rolling_median_10', 'S8_rolling_max_10', 'S8_rolling_min_10', 'S8_dist_to_rolling_mean_10', 'S8_rolling_mean_20', 'S8_rolling_std_20', 'S8_rolling_median_20', 'S8_rolling_max_20', 'S8_rolling_min_20', 'S8_dist_to_rolling_mean_20', 'S8_rolling_mean_60', 'S8_rolling_std_60', 'S8_rolling_median_60', 'S8_rolling_max_60', 'S8_rolling_min_60', 'S8_dist_to_rolling_mean_60', 'S8_rolling_mean_120', 'S8_rolling_std_120', 'S8_rolling_median_120', 'S8_rolling_max_120', 'S8_rolling_min_120', 'S8_dist_to_rolling_mean_120', 'M4_rolling_var_5', 'M4_rolling_var_10', 'M4_rolling_var_20', 'M4_rolling_var_60', 'M4_rolling_var_120', 'V13_rolling_var_5', 'V13_rolling_var_10', 'V13_rolling_var_20', 'V13_rolling_var_60', 'V13_rolling_var_120', 'M11_rolling_var_5', 'M11_rolling_var_10', 'M11_rolling_var_20', 'M11_rolling_var_60', 'M11_rolling_var_120', 'S2_rolling_var_5', 'S2_rolling_var_10', 'S2_rolling_var_20', 'S2_rolling_var_60', 'S2_rolling_var_120', 'P6_rolling_var_5', 'P6_rolling_var_10', 'P6_rolling_var_20', 'P6_rolling_var_60', 'P6_rolling_var_120', 'M2_rolling_var_5', 'M2_rolling_var_10', 'M2_rolling_var_20', 'M2_rolling_var_60', 'M2_rolling_var_120', 'M9_rolling_var_5', 'M9_rolling_var_10', 'M9_rolling_var_20', 'M9_rolling_var_60', 'M9_rolling_var_120', 'P8_rolling_var_5', 'P8_rolling_var_10', 'P8_rolling_var_20', 'P8_rolling_var_60', 'P8_rolling_var_120', 'P7_rolling_var_5', 'P7_rolling_var_10', 'P7_rolling_var_20', 'P7_rolling_var_60', 'P7_rolling_var_120', 'S12_rolling_var_5', 'S12_rolling_var_10', 'S12_rolling_var_20', 'S12_rolling_var_60', 'S12_rolling_var_120', 'P13_rolling_var_5', 'P13_rolling_var_10', 'P13_rolling_var_20', 'P13_rolling_var_60', 'P13_rolling_var_120', 'V9_rolling_var_5', 'V9_rolling_var_10', 'V9_rolling_var_20', 'V9_rolling_var_60', 'V9_rolling_var_120', 'P1_rolling_var_5', 'P1_rolling_var_10', 'P1_rolling_var_20', 'P1_rolling_var_60', 'P1_rolling_var_120', 'S8_rolling_var_5', 'S8_rolling_var_10', 'S8_rolling_var_20', 'S8_rolling_var_60', 'S8_rolling_var_120', 'M4_z_5', 'M4_z_10', 'M4_z_20', 'M4_z_60', 'M4_z_120', 'V13_z_5', 'V13_z_10', 'V13_z_20', 'V13_z_60', 'V13_z_120', 'M11_z_5', 'M11_z_10', 'M11_z_20', 'M11_z_60', 'M11_z_120', 'S2_z_5', 'S2_z_10', 'S2_z_20', 'S2_z_60', 'S2_z_120', 'P6_z_5', 'P6_z_10', 'P6_z_20', 'P6_z_60', 'P6_z_120', 'M2_z_5', 'M2_z_10', 'M2_z_20', 'M2_z_60', 'M2_z_120', 'M9_z_5', 'M9_z_10', 'M9_z_20', 'M9_z_60', 'M9_z_120', 'P8_z_5', 'P8_z_10', 'P8_z_20', 'P8_z_60', 'P8_z_120', 'P7_z_5', 'P7_z_10', 'P7_z_20', 'P7_z_60', 'P7_z_120', 'S12_z_5', 'S12_z_10', 'S12_z_20', 'S12_z_60', 'S12_z_120', 'P13_z_5', 'P13_z_10', 'P13_z_20', 'P13_z_60', 'P13_z_120', 'V9_z_5', 'V9_z_10', 'V9_z_20', 'V9_z_60', 'V9_z_120', 'P1_z_5', 'P1_z_10', 'P1_z_20', 'P1_z_60', 'P1_z_120', 'S8_z_5', 'S8_z_10', 'S8_z_20', 'S8_z_60', 'S8_z_120', 'M4_scipy_zscore', 'V13_scipy_zscore', 'M11_scipy_zscore', 'S2_scipy_zscore', 'P6_scipy_zscore', 'M2_scipy_zscore', 'M9_scipy_zscore', 'P8_scipy_zscore', 'P7_scipy_zscore', 'S12_scipy_zscore', 'P13_scipy_zscore', 'V9_scipy_zscore', 'P1_scipy_zscore', 'S8_scipy_zscore', 'M4_diff_1', 'M4_pctchg_1', 'V13_diff_1', 'V13_pctchg_1', 'M11_diff_1', 'M11_pctchg_1', 'S2_diff_1', 'S2_pctchg_1', 'P6_diff_1', 'P6_pctchg_1', 'M2_diff_1', 'M2_pctchg_1', 'M9_diff_1', 'M9_pctchg_1', 'P8_diff_1', 'P8_pctchg_1', 'P7_diff_1', 'P7_pctchg_1', 'S12_diff_1', 'S12_pctchg_1', 'P13_diff_1', 'P13_pctchg_1', 'V9_diff_1', 'V9_pctchg_1', 'P1_diff_1', 'P1_pctchg_1', 'S8_diff_1', 'S8_pctchg_1', 'M4_pacf_1', 'M4_pacf_2', 'M4_pacf_3', 'M4_pacf_4', 'M4_pacf_5', 'M4_pacf_6', 'M4_pacf_7', 'M4_pacf_8', 'M4_pacf_9', 'M4_pacf_10', 'V13_pacf_1', 'V13_pacf_2', 'V13_pacf_3', 'V13_pacf_4', 'V13_pacf_5', 'V13_pacf_6', 'V13_pacf_7', 'V13_pacf_8', 'V13_pacf_9', 'V13_pacf_10', 'M11_pacf_1', 'M11_pacf_2', 'M11_pacf_3', 'M11_pacf_4', 'M11_pacf_5', 'M11_pacf_6', 'M11_pacf_7', 'M11_pacf_8', 'M11_pacf_9', 'M11_pacf_10', 'S2_pacf_1', 'S2_pacf_2', 'S2_pacf_3', 'S2_pacf_4', 'S2_pacf_5', 'S2_pacf_6', 'S2_pacf_7', 'S2_pacf_8', 'S2_pacf_9', 'S2_pacf_10', 'P6_pacf_1', 'P6_pacf_2', 'P6_pacf_3', 'P6_pacf_4', 'P6_pacf_5', 'P6_pacf_6', 'P6_pacf_7', 'P6_pacf_8', 'P6_pacf_9', 'P6_pacf_10', 'M2_pacf_1', 'M2_pacf_2', 'M2_pacf_3', 'M2_pacf_4', 'M2_pacf_5', 'M2_pacf_6', 'M2_pacf_7', 'M2_pacf_8', 'M2_pacf_9', 'M2_pacf_10', 'M9_pacf_1', 'M9_pacf_2', 'M9_pacf_3', 'M9_pacf_4', 'M9_pacf_5', 'M9_pacf_6', 'M9_pacf_7', 'M9_pacf_8', 'M9_pacf_9', 'M9_pacf_10', 'P8_pacf_1', 'P8_pacf_2', 'P8_pacf_3', 'P8_pacf_4', 'P8_pacf_5', 'P8_pacf_6', 'P8_pacf_7', 'P8_pacf_8', 'P8_pacf_9', 'P8_pacf_10', 'P7_pacf_1', 'P7_pacf_2', 'P7_pacf_3', 'P7_pacf_4', 'P7_pacf_5', 'P7_pacf_6', 'P7_pacf_7', 'P7_pacf_8', 'P7_pacf_9', 'P7_pacf_10', 'S12_pacf_1', 'S12_pacf_2', 'S12_pacf_3', 'S12_pacf_4', 'S12_pacf_5', 'S12_pacf_6', 'S12_pacf_7', 'S12_pacf_8', 'S12_pacf_9', 'S12_pacf_10', 'P13_pacf_1', 'P13_pacf_2', 'P13_pacf_3', 'P13_pacf_4', 'P13_pacf_5', 'P13_pacf_6', 'P13_pacf_7', 'P13_pacf_8', 'P13_pacf_9', 'P13_pacf_10', 'V9_pacf_1', 'V9_pacf_2', 'V9_pacf_3', 'V9_pacf_4', 'V9_pacf_5', 'V9_pacf_6', 'V9_pacf_7', 'V9_pacf_8', 'V9_pacf_9', 'V9_pacf_10', 'P1_pacf_1', 'P1_pacf_2', 'P1_pacf_3', 'P1_pacf_4', 'P1_pacf_5', 'P1_pacf_6', 'P1_pacf_7', 'P1_pacf_8', 'P1_pacf_9', 'P1_pacf_10', 'S8_pacf_1', 'S8_pacf_2', 'S8_pacf_3', 'S8_pacf_4', 'S8_pacf_5', 'S8_pacf_6', 'S8_pacf_7', 'S8_pacf_8', 'S8_pacf_9', 'S8_pacf_10', 'M4_acf_1', 'M4_acf_2', 'M4_acf_3', 'M4_acf_4', 'M4_acf_5', 'M4_acf_6', 'M4_acf_7', 'M4_acf_8', 'M4_acf_9', 'M4_acf_10', 'V13_acf_1', 'V13_acf_2', 'V13_acf_3', 'V13_acf_4', 'V13_acf_5', 'V13_acf_6', 'V13_acf_7', 'V13_acf_8', 'V13_acf_9', 'V13_acf_10', 'M11_acf_1', 'M11_acf_2', 'M11_acf_3', 'M11_acf_4', 'M11_acf_5', 'M11_acf_6', 'M11_acf_7', 'M11_acf_8', 'M11_acf_9', 'M11_acf_10', 'S2_acf_1', 'S2_acf_2', 'S2_acf_3', 'S2_acf_4', 'S2_acf_5', 'S2_acf_6', 'S2_acf_7', 'S2_acf_8', 'S2_acf_9', 'S2_acf_10', 'P6_acf_1', 'P6_acf_2', 'P6_acf_3', 'P6_acf_4', 'P6_acf_5', 'P6_acf_6', 'P6_acf_7', 'P6_acf_8', 'P6_acf_9', 'P6_acf_10', 'M2_acf_1', 'M2_acf_2', 'M2_acf_3', 'M2_acf_4', 'M2_acf_5', 'M2_acf_6', 'M2_acf_7', 'M2_acf_8', 'M2_acf_9', 'M2_acf_10', 'M9_acf_1', 'M9_acf_2', 'M9_acf_3', 'M9_acf_4', 'M9_acf_5', 'M9_acf_6', 'M9_acf_7', 'M9_acf_8', 'M9_acf_9', 'M9_acf_10', 'P8_acf_1', 'P8_acf_2', 'P8_acf_3', 'P8_acf_4', 'P8_acf_5', 'P8_acf_6', 'P8_acf_7', 'P8_acf_8', 'P8_acf_9', 'P8_acf_10', 'P7_acf_1', 'P7_acf_2', 'P7_acf_3', 'P7_acf_4', 'P7_acf_5', 'P7_acf_6', 'P7_acf_7', 'P7_acf_8', 'P7_acf_9', 'P7_acf_10', 'S12_acf_1', 'S12_acf_2', 'S12_acf_3', 'S12_acf_4', 'S12_acf_5', 'S12_acf_6', 'S12_acf_7', 'S12_acf_8', 'S12_acf_9', 'S12_acf_10', 'P13_acf_1', 'P13_acf_2', 'P13_acf_3', 'P13_acf_4', 'P13_acf_5', 'P13_acf_6', 'P13_acf_7', 'P13_acf_8', 'P13_acf_9', 'P13_acf_10', 'V9_acf_1', 'V9_acf_2', 'V9_acf_3', 'V9_acf_4', 'V9_acf_5', 'V9_acf_6', 'V9_acf_7', 'V9_acf_8', 'V9_acf_9', 'V9_acf_10', 'P1_acf_1', 'P1_acf_2', 'P1_acf_3', 'P1_acf_4', 'P1_acf_5', 'P1_acf_6', 'P1_acf_7', 'P1_acf_8', 'P1_acf_9', 'P1_acf_10', 'S8_acf_1', 'S8_acf_2', 'S8_acf_3', 'S8_acf_4', 'S8_acf_5', 'S8_acf_6', 'S8_acf_7', 'S8_acf_8', 'S8_acf_9', 'S8_acf_10', 'M4_autocorr_1', 'M4_autocorr_2', 'M4_autocorr_3', 'M4_autocorr_4', 'M4_autocorr_5', 'M4_autocorr_6', 'M4_autocorr_7', 'M4_autocorr_8', 'M4_autocorr_9', 'M4_autocorr_10', 'V13_autocorr_1', 'V13_autocorr_2', 'V13_autocorr_3', 'V13_autocorr_4', 'V13_autocorr_5', 'V13_autocorr_6', 'V13_autocorr_7', 'V13_autocorr_8', 'V13_autocorr_9', 'V13_autocorr_10', 'M11_autocorr_1', 'M11_autocorr_2', 'M11_autocorr_3', 'M11_autocorr_4', 'M11_autocorr_5', 'M11_autocorr_6', 'M11_autocorr_7', 'M11_autocorr_8', 'M11_autocorr_9', 'M11_autocorr_10', 'S2_autocorr_1', 'S2_autocorr_2', 'S2_autocorr_3', 'S2_autocorr_4', 'S2_autocorr_5', 'S2_autocorr_6', 'S2_autocorr_7', 'S2_autocorr_8', 'S2_autocorr_9', 'S2_autocorr_10', 'P6_autocorr_1', 'P6_autocorr_2', 'P6_autocorr_3', 'P6_autocorr_4', 'P6_autocorr_5', 'P6_autocorr_6', 'P6_autocorr_7', 'P6_autocorr_8', 'P6_autocorr_9', 'P6_autocorr_10', 'M2_autocorr_1', 'M2_autocorr_2', 'M2_autocorr_3', 'M2_autocorr_4', 'M2_autocorr_5', 'M2_autocorr_6', 'M2_autocorr_7', 'M2_autocorr_8', 'M2_autocorr_9', 'M2_autocorr_10', 'M9_autocorr_1', 'M9_autocorr_2', 'M9_autocorr_3', 'M9_autocorr_4', 'M9_autocorr_5', 'M9_autocorr_6', 'M9_autocorr_7', 'M9_autocorr_8', 'M9_autocorr_9', 'M9_autocorr_10', 'P8_autocorr_1', 'P8_autocorr_2', 'P8_autocorr_3', 'P8_autocorr_4', 'P8_autocorr_5', 'P8_autocorr_6', 'P8_autocorr_7', 'P8_autocorr_8', 'P8_autocorr_9', 'P8_autocorr_10', 'P7_autocorr_1', 'P7_autocorr_2', 'P7_autocorr_3', 'P7_autocorr_4', 'P7_autocorr_5', 'P7_autocorr_6', 'P7_autocorr_7', 'P7_autocorr_8', 'P7_autocorr_9', 'P7_autocorr_10', 'S12_autocorr_1', 'S12_autocorr_2', 'S12_autocorr_3', 'S12_autocorr_4', 'S12_autocorr_5', 'S12_autocorr_6', 'S12_autocorr_7', 'S12_autocorr_8', 'S12_autocorr_9', 'S12_autocorr_10', 'P13_autocorr_1', 'P13_autocorr_2', 'P13_autocorr_3', 'P13_autocorr_4', 'P13_autocorr_5', 'P13_autocorr_6', 'P13_autocorr_7', 'P13_autocorr_8', 'P13_autocorr_9', 'P13_autocorr_10', 'V9_autocorr_1', 'V9_autocorr_2', 'V9_autocorr_3', 'V9_autocorr_4', 'V9_autocorr_5', 'V9_autocorr_6', 'V9_autocorr_7', 'V9_autocorr_8', 'V9_autocorr_9', 'V9_autocorr_10', 'P1_autocorr_1', 'P1_autocorr_2', 'P1_autocorr_3', 'P1_autocorr_4', 'P1_autocorr_5', 'P1_autocorr_6', 'P1_autocorr_7', 'P1_autocorr_8', 'P1_autocorr_9', 'P1_autocorr_10', 'S8_autocorr_1', 'S8_autocorr_2', 'S8_autocorr_3', 'S8_autocorr_4', 'S8_autocorr_5', 'S8_autocorr_6', 'S8_autocorr_7', 'S8_autocorr_8', 'S8_autocorr_9', 'S8_autocorr_10', 'M4_skewness', 'M4_kurtosis', 'V13_skewness', 'V13_kurtosis', 'M11_skewness', 'M11_kurtosis', 'S2_skewness', 'S2_kurtosis', 'P6_skewness', 'P6_kurtosis', 'M2_skewness', 'M2_kurtosis', 'M9_skewness', 'M9_kurtosis', 'P8_skewness', 'P8_kurtosis', 'P7_skewness', 'P7_kurtosis', 'S12_skewness', 'S12_kurtosis', 'P13_skewness', 'P13_kurtosis', 'V9_skewness', 'V9_kurtosis', 'P1_skewness', 'P1_kurtosis', 'S8_skewness', 'S8_kurtosis', 'M4_momentum_5', 'M4_momentum_10', 'M4_momentum_20', 'V13_momentum_5', 'V13_momentum_10', 'V13_momentum_20', 'M11_momentum_5', 'M11_momentum_10', 'M11_momentum_20', 'S2_momentum_5', 'S2_momentum_10', 'S2_momentum_20', 'P6_momentum_5', 'P6_momentum_10', 'P6_momentum_20', 'M2_momentum_5', 'M2_momentum_10', 'M2_momentum_20', 'M9_momentum_5', 'M9_momentum_10', 'M9_momentum_20', 'P8_momentum_5', 'P8_momentum_10', 'P8_momentum_20', 'P7_momentum_5', 'P7_momentum_10', 'P7_momentum_20', 'S12_momentum_5', 'S12_momentum_10', 'S12_momentum_20', 'P13_momentum_5', 'P13_momentum_10', 'P13_momentum_20', 'V9_momentum_5', 'V9_momentum_10', 'V9_momentum_20', 'P1_momentum_5', 'P1_momentum_10', 'P1_momentum_20', 'S8_momentum_5', 'S8_momentum_10', 'S8_momentum_20', 'M4_dist_to_momentum_5', 'M4_dist_to_momentum_10', 'M4_dist_to_momentum_20', 'V13_dist_to_momentum_5', 'V13_dist_to_momentum_10', 'V13_dist_to_momentum_20', 'M11_dist_to_momentum_5', 'M11_dist_to_momentum_10', 'M11_dist_to_momentum_20', 'S2_dist_to_momentum_5', 'S2_dist_to_momentum_10', 'S2_dist_to_momentum_20', 'P6_dist_to_momentum_5', 'P6_dist_to_momentum_10', 'P6_dist_to_momentum_20', 'M2_dist_to_momentum_5', 'M2_dist_to_momentum_10', 'M2_dist_to_momentum_20', 'M9_dist_to_momentum_5', 'M9_dist_to_momentum_10', 'M9_dist_to_momentum_20', 'P8_dist_to_momentum_5', 'P8_dist_to_momentum_10', 'P8_dist_to_momentum_20', 'P7_dist_to_momentum_5', 'P7_dist_to_momentum_10', 'P7_dist_to_momentum_20', 'S12_dist_to_momentum_5', 'S12_dist_to_momentum_10', 'S12_dist_to_momentum_20', 'P13_dist_to_momentum_5', 'P13_dist_to_momentum_10', 'P13_dist_to_momentum_20', 'V9_dist_to_momentum_5', 'V9_dist_to_momentum_10', 'V9_dist_to_momentum_20', 'P1_dist_to_momentum_5', 'P1_dist_to_momentum_10', 'P1_dist_to_momentum_20', 'S8_dist_to_momentum_5', 'S8_dist_to_momentum_10', 'S8_dist_to_momentum_20', 'M4_diff_5', 'M4_diff_10', 'V13_diff_5', 'V13_diff_10', 'M11_diff_5', 'M11_diff_10', 'S2_diff_5', 'S2_diff_10', 'P6_diff_5', 'P6_diff_10', 'M2_diff_5', 'M2_diff_10', 'M9_diff_5', 'M9_diff_10', 'P8_diff_5', 'P8_diff_10', 'P7_diff_5', 'P7_diff_10', 'S12_diff_5', 'S12_diff_10', 'P13_diff_5', 'P13_diff_10', 'V9_diff_5', 'V9_diff_10', 'P1_diff_5', 'P1_diff_10', 'S8_diff_5', 'S8_diff_10', 'M4_normalized', 'V13_normalized', 'M11_normalized', 'S2_normalized', 'P6_normalized', 'M2_normalized', 'M9_normalized', 'P8_normalized', 'P7_normalized', 'S12_normalized', 'P13_normalized', 'V9_normalized', 'P1_normalized', 'S8_normalized', 'M4_macro_rolling_sum_5', 'M4_macro_rolling_sum_10', 'M4_macro_rolling_sum_20', 'V13_macro_rolling_sum_5', 'V13_macro_rolling_sum_10', 'V13_macro_rolling_sum_20', 'M11_macro_rolling_sum_5', 'M11_macro_rolling_sum_10', 'M11_macro_rolling_sum_20', 'S2_macro_rolling_sum_5', 'S2_macro_rolling_sum_10', 'S2_macro_rolling_sum_20', 'P6_macro_rolling_sum_5', 'P6_macro_rolling_sum_10', 'P6_macro_rolling_sum_20', 'M2_macro_rolling_sum_5', 'M2_macro_rolling_sum_10', 'M2_macro_rolling_sum_20', 'M9_macro_rolling_sum_5', 'M9_macro_rolling_sum_10', 'M9_macro_rolling_sum_20', 'P8_macro_rolling_sum_5', 'P8_macro_rolling_sum_10', 'P8_macro_rolling_sum_20', 'P7_macro_rolling_sum_5', 'P7_macro_rolling_sum_10', 'P7_macro_rolling_sum_20', 'S12_macro_rolling_sum_5', 'S12_macro_rolling_sum_10', 'S12_macro_rolling_sum_20', 'P13_macro_rolling_sum_5', 'P13_macro_rolling_sum_10', 'P13_macro_rolling_sum_20', 'V9_macro_rolling_sum_5', 'V9_macro_rolling_sum_10', 'V9_macro_rolling_sum_20', 'P1_macro_rolling_sum_5', 'P1_macro_rolling_sum_10', 'P1_macro_rolling_sum_20', 'S8_macro_rolling_sum_5', 'S8_macro_rolling_sum_10', 'S8_macro_rolling_sum_20', 'M4_cumsum', 'V13_cumsum', 'M11_cumsum', 'S2_cumsum', 'P6_cumsum', 'M2_cumsum', 'M9_cumsum', 'P8_cumsum', 'P7_cumsum', 'S12_cumsum', 'P13_cumsum', 'V9_cumsum', 'P1_cumsum', 'S8_cumsum', 'M4_hurst', 'V13_hurst', 'M11_hurst', 'S2_hurst', 'P6_hurst', 'M2_hurst', 'M9_hurst', 'P8_hurst', 'P7_hurst', 'S12_hurst', 'P13_hurst', 'V9_hurst', 'P1_hurst', 'S8_hurst', 'M4_lag_1', 'M4_lag_5', 'M4_lag_10', 'V13_lag_1', 'V13_lag_5', 'V13_lag_10', 'M11_lag_1', 'M11_lag_5', 'M11_lag_10', 'S2_lag_1', 'S2_lag_5', 'S2_lag_10', 'P6_lag_1', 'P6_lag_5', 'P6_lag_10', 'M2_lag_1', 'M2_lag_5', 'M2_lag_10', 'M9_lag_1', 'M9_lag_5', 'M9_lag_10', 'P8_lag_1', 'P8_lag_5', 'P8_lag_10', 'P7_lag_1', 'P7_lag_5', 'P7_lag_10', 'S12_lag_1', 'S12_lag_5', 'S12_lag_10', 'P13_lag_1', 'P13_lag_5', 'P13_lag_10', 'V9_lag_1', 'V9_lag_5', 'V9_lag_10', 'P1_lag_1', 'P1_lag_5', 'P1_lag_10', 'S8_lag_1', 'S8_lag_5', 'S8_lag_10']\n",
      "Cleaning and selecting features...\n",
      "Feature engineering complete. Created 2005 total columns.\n",
      "Feature columns for selection: 2004 total features available\n",
      "\n",
      "============================================================\n",
      "ENHANCED ENSEMBLE FEATURE SELECTION\n",
      "============================================================\n",
      "Using dynamic seed: 9126\n",
      "Features after variance filtering: 1354\n",
      "Method 1a: Gradient Boosting Feature Importance...\n",
      "Method 1b: Bagging Regressor Feature Importance...\n",
      "Ensemble Voting: Combining all methods...\n",
      "\n",
      "Ensemble Feature Selection Results:\n",
      "   Total unique features considered: 311\n",
      "   Selected by 1a (gradient boosting): 200\n",
      "   Selected by 1b (bagging regressor):  200\n",
      "   Final ensemble selection: 200 features\n",
      "   Final ensemble selection: 200 features\n",
      "\n",
      "Feature Engineering Results:\n",
      "Original base features available: 94\n",
      "Original features selected: 20\n",
      "New engineered features created: 180\n",
      "Total features for modeling: 200\n",
      "\n",
      "New engineered features added:\n",
      " 1. V13_diff_1\n",
      " 2. V9_diff_1\n",
      " 3. M11_diff_5\n",
      " 4. M4_dist_to_rolling_mean_60\n",
      " 5. V13_momentum_5\n",
      " 6. M4_dist_to_rolling_mean_120\n",
      " 7. V13_dist_to_rolling_mean_5\n",
      " 8. M4_dist_to_rolling_mean_20\n",
      " 9. M4_lag_10\n",
      "10. V13_rolling_min_5\n",
      "11. M11_diff_1\n",
      "12. M4_diff_10\n",
      "13. V13_dist_to_momentum_20\n",
      "14. V9_diff_10\n",
      "15. V13_pctchg_1\n",
      "16. M11_dist_to_rolling_mean_10\n",
      "17. M4_diff_5\n",
      "18. M9_diff_5\n",
      "19. S8_normalized\n",
      "20. M4_lag_1\n",
      "21. P8_dist_to_rolling_mean_60\n",
      "22. E8_cumsum\n",
      "23. V13_momentum_10\n",
      "24. V13_dist_to_momentum_5\n",
      "25. M11_momentum_5\n",
      "26. E8_dist_to_momentum_10\n",
      "27. P6_dist_to_momentum_20\n",
      "28. M11_momentum_10\n",
      "29. M4_rolling_mean_60\n",
      "30. V13_diff_5\n",
      "31. V13_dist_to_rolling_mean_120\n",
      "32. M4_momentum_20\n",
      "33. E8_momentum_20\n",
      "34. M11_dist_to_rolling_mean_5\n",
      "35. P8_diff_1\n",
      "36. M9_pctchg_1\n",
      "37. M4_rolling_median_60\n",
      "38. macro_corr_M4_P6_30_macro_high_ratio_120\n",
      "39. P13_dist_to_momentum_20\n",
      "40. M4_dist_to_rolling_mean_10\n",
      "41. P13_dist_to_momentum_10\n",
      "42. P7_momentum_10\n",
      "43. M4_diff_1\n",
      "44. V13_diff_10\n",
      "45. M4_dist_to_momentum_10\n",
      "46. P7_lag_10\n",
      "47. S2_pctchg_1\n",
      "48. P13_diff_1\n",
      "49. M9_diff_1\n",
      "50. P6_diff_1\n",
      "51. S2_dist_to_momentum_5\n",
      "52. M4_dist_to_rolling_mean_5\n",
      "53. P8_pctchg_1\n",
      "54. P13_pctchg_1\n",
      "55. M11_z_5\n",
      "56. P13_z_5\n",
      "57. P13_momentum_20\n",
      "58. P7_lag_5\n",
      "59. P6_momentum_20\n",
      "60. V13_kurtosis\n",
      "61. macro_corr_M4_D8_30_macro_low_ratio_120\n",
      "62. M9_dist_to_rolling_mean_5\n",
      "63. M4_pctchg_1\n",
      "64. P1_pctchg_1\n",
      "65. P6_dist_to_rolling_mean_5\n",
      "66. macro_corr_M4_E8_30_macro_low_ratio_20\n",
      "67. P8_momentum_20\n",
      "68. P1_momentum_10\n",
      "69. P7_lag_1\n",
      "70. M11_z_120\n",
      "71. V13_dist_to_rolling_mean_20\n",
      "72. macro_corr_M4_P6_30_macro_high_ratio_20\n",
      "73. P13_diff_10\n",
      "74. P1_z_5\n",
      "75. P13_momentum_5\n",
      "76. E8_dist_to_rolling_mean_5\n",
      "77. M4_rolling_max_5\n",
      "78. M9_dist_to_rolling_mean_10\n",
      "79. M11_skewness\n",
      "80. M4_rolling_median_5\n",
      "81. M4_rolling_mean_20\n",
      "82. D4_cumsum\n",
      "83. E8_rolling_min_5\n",
      "84. V9_dist_to_rolling_mean_120\n",
      "85. M9_cumsum\n",
      "86. D4_rolling_mean_120\n",
      "87. P7_rolling_median_5\n",
      "88. M11_scipy_zscore\n",
      "89. P7_rolling_std_120\n",
      "90. V9_lag_1\n",
      "91. S2_momentum_20\n",
      "92. macro_corr_M4_M11_30_macro_high_ratio_60\n",
      "93. M11_lag_1\n",
      "94. E8_dist_to_rolling_mean_20\n",
      "95. V13_z_5\n",
      "96. macro_corr_M4_S2_30_macro_high_ratio_60\n",
      "97. S8_rolling_min_5\n",
      "98. S2_momentum_10\n",
      "99. D4_rolling_std_120\n",
      "100. P6_pctchg_1\n",
      "101. M4_rolling_var_5\n",
      "102. S2_rolling_mean_120\n",
      "103. P6_dist_to_rolling_mean_60\n",
      "104. M11_pctchg_1\n",
      "105. P6_diff_5\n",
      "106. M11_dist_to_rolling_mean_20\n",
      "107. P7_skewness\n",
      "108. P6_z_120\n",
      "109. D4_rolling_var_120\n",
      "110. S8_dist_to_momentum_20\n",
      "111. S8_dist_to_rolling_mean_120\n",
      "112. S8_lag_1\n",
      "113. S12_z_5\n",
      "114. V9_dist_to_rolling_mean_60\n",
      "115. M4_rolling_mean_120\n",
      "116. M11_rolling_median_120\n",
      "117. D5_cumsum\n",
      "118. macro_corr_M4_D8_30_macro_low_ratio_60\n",
      "119. P8_dist_to_rolling_mean_10\n",
      "120. macro_corr_M4_P6_30_macro_high_ratio_60\n",
      "121. D4_dist_to_rolling_mean_120\n",
      "122. P8_dist_to_rolling_mean_120\n",
      "123. M11_macro_rolling_sum_5\n",
      "124. M9_rolling_std_20\n",
      "125. M2_cumsum\n",
      "126. V13_momentum_20\n",
      "127. M4_momentum_10\n",
      "128. S8_skewness\n",
      "129. P7_rolling_median_120\n",
      "130. M9_z_20\n",
      "131. macro_corr_M4_E8_30_macro_low_ratio_120\n",
      "132. M9_momentum_5\n",
      "133. P7_diff_10\n",
      "134. S8_rolling_min_10\n",
      "135. V9_momentum_20\n",
      "136. macro_corr_M4_D4_30_macro_low_ratio_60\n",
      "137. macro_corr_M4_M2_30\n",
      "138. M9_dist_to_rolling_mean_60\n",
      "139. P8_dist_to_rolling_mean_5\n",
      "140. P6_lag_5\n",
      "141. V9_dist_to_rolling_mean_10\n",
      "142. P8_rolling_var_20\n",
      "143. D8_cumsum\n",
      "144. V13_z_120\n",
      "145. S2_z_10\n",
      "146. M2_momentum_20\n",
      "147. P13_diff_5\n",
      "148. macro_corr_M4_P6_30_macro_low_ratio_120\n",
      "149. P1_dist_to_rolling_mean_5\n",
      "150. P7_rolling_median_20\n",
      "151. P7_z_10\n",
      "152. P6_lag_1\n",
      "153. P7_momentum_20\n",
      "154. P6_momentum_5\n",
      "155. M4_dist_to_momentum_5\n",
      "156. P6_z_60\n",
      "157. S2_rolling_max_20\n",
      "158. M11_rolling_var_5\n",
      "159. macro_corr_M4_S2_30_macro_high_ratio_20\n",
      "160. M4_rolling_median_10\n",
      "161. V13_lag_10\n",
      "162. V13_z_20\n",
      "163. M9_z_10\n",
      "164. S2_rolling_mean_10\n",
      "165. P6_dist_to_momentum_5\n",
      "166. E8_skewness\n",
      "167. E8_lag_5\n",
      "168. E8_normalized\n",
      "169. P8_dist_to_momentum_5\n",
      "170. M11_dist_to_rolling_mean_60\n",
      "171. P6_dist_to_momentum_10\n",
      "172. S12_dist_to_rolling_mean_60\n",
      "173. P8_z_5\n",
      "174. M4_lag_5\n",
      "175. D4_z_120\n",
      "176. P13_rolling_mean_5\n",
      "177. P8_diff_5\n",
      "178. P8_momentum_5\n",
      "179. E8_dist_to_rolling_mean_120\n",
      "180. D8_dist_to_momentum_20\n",
      "\n",
      "All 200 selected features:\n",
      " 1. V13_diff_1                [ENGINEERED]\n",
      " 2. V9_diff_1                 [ENGINEERED]\n",
      " 3. M11_diff_5                [ENGINEERED]\n",
      " 4. M4_dist_to_rolling_mean_60 [ENGINEERED]\n",
      " 5. V13_momentum_5            [ENGINEERED]\n",
      " 6. M4_dist_to_rolling_mean_120 [ENGINEERED]\n",
      " 7. V13_dist_to_rolling_mean_5 [ENGINEERED]\n",
      " 8. M4_dist_to_rolling_mean_20 [ENGINEERED]\n",
      " 9. M4_lag_10                 [ENGINEERED]\n",
      "10. V13_rolling_min_5         [ENGINEERED]\n",
      "11. E12                       [ORIGINAL]\n",
      "12. M11_diff_1                [ENGINEERED]\n",
      "13. M4_diff_10                [ENGINEERED]\n",
      "14. E19                       [ORIGINAL]\n",
      "15. V13_dist_to_momentum_20   [ENGINEERED]\n",
      "16. V9_diff_10                [ENGINEERED]\n",
      "17. V13_pctchg_1              [ENGINEERED]\n",
      "18. M11_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "19. M4_diff_5                 [ENGINEERED]\n",
      "20. M9_diff_5                 [ENGINEERED]\n",
      "21. S8_normalized             [ENGINEERED]\n",
      "22. S2                        [ORIGINAL]\n",
      "23. M4_lag_1                  [ENGINEERED]\n",
      "24. P8_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "25. E8_cumsum                 [ENGINEERED]\n",
      "26. V13_momentum_10           [ENGINEERED]\n",
      "27. V13_dist_to_momentum_5    [ENGINEERED]\n",
      "28. M11_momentum_5            [ENGINEERED]\n",
      "29. E8_dist_to_momentum_10    [ENGINEERED]\n",
      "30. P6_dist_to_momentum_20    [ENGINEERED]\n",
      "31. M11_momentum_10           [ENGINEERED]\n",
      "32. S8                        [ORIGINAL]\n",
      "33. M4_rolling_mean_60        [ENGINEERED]\n",
      "34. V13_diff_5                [ENGINEERED]\n",
      "35. V13_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "36. M4_momentum_20            [ENGINEERED]\n",
      "37. E8_momentum_20            [ENGINEERED]\n",
      "38. M11_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "39. P8_diff_1                 [ENGINEERED]\n",
      "40. M9_pctchg_1               [ENGINEERED]\n",
      "41. M4_rolling_median_60      [ENGINEERED]\n",
      "42. macro_corr_M4_P6_30_macro_high_ratio_120 [ENGINEERED]\n",
      "43. P5                        [ORIGINAL]\n",
      "44. P13_dist_to_momentum_20   [ENGINEERED]\n",
      "45. M4_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "46. P13_dist_to_momentum_10   [ENGINEERED]\n",
      "47. P7_momentum_10            [ENGINEERED]\n",
      "48. M4_diff_1                 [ENGINEERED]\n",
      "49. V13_diff_10               [ENGINEERED]\n",
      "50. I2                        [ORIGINAL]\n",
      "51. M4_dist_to_momentum_10    [ENGINEERED]\n",
      "52. P7_lag_10                 [ENGINEERED]\n",
      "53. S2_pctchg_1               [ENGINEERED]\n",
      "54. P13_diff_1                [ENGINEERED]\n",
      "55. M9_diff_1                 [ENGINEERED]\n",
      "56. P6_diff_1                 [ENGINEERED]\n",
      "57. S2_dist_to_momentum_5     [ENGINEERED]\n",
      "58. M4_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "59. P8_pctchg_1               [ENGINEERED]\n",
      "60. P13_pctchg_1              [ENGINEERED]\n",
      "61. M11_z_5                   [ENGINEERED]\n",
      "62. P13_z_5                   [ENGINEERED]\n",
      "63. P13_momentum_20           [ENGINEERED]\n",
      "64. P7_lag_5                  [ENGINEERED]\n",
      "65. P6_momentum_20            [ENGINEERED]\n",
      "66. S5                        [ORIGINAL]\n",
      "67. V13_kurtosis              [ENGINEERED]\n",
      "68. macro_corr_M4_D8_30_macro_low_ratio_120 [ENGINEERED]\n",
      "69. M9_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "70. M4_pctchg_1               [ENGINEERED]\n",
      "71. P1_pctchg_1               [ENGINEERED]\n",
      "72. P6_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "73. macro_corr_M4_E8_30_macro_low_ratio_20 [ENGINEERED]\n",
      "74. P8_momentum_20            [ENGINEERED]\n",
      "75. P1_momentum_10            [ENGINEERED]\n",
      "76. P7_lag_1                  [ENGINEERED]\n",
      "77. M11_z_120                 [ENGINEERED]\n",
      "78. V13_dist_to_rolling_mean_20 [ENGINEERED]\n",
      "79. macro_corr_M4_P6_30_macro_high_ratio_20 [ENGINEERED]\n",
      "80. E17                       [ORIGINAL]\n",
      "81. P13_diff_10               [ENGINEERED]\n",
      "82. P1_z_5                    [ENGINEERED]\n",
      "83. P13_momentum_5            [ENGINEERED]\n",
      "84. E8_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "85. M4_rolling_max_5          [ENGINEERED]\n",
      "86. M9_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "87. M11_skewness              [ENGINEERED]\n",
      "88. M4_rolling_median_5       [ENGINEERED]\n",
      "89. M4_rolling_mean_20        [ENGINEERED]\n",
      "90. D4_cumsum                 [ENGINEERED]\n",
      "91. E8_rolling_min_5          [ENGINEERED]\n",
      "92. V9_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "93. P8                        [ORIGINAL]\n",
      "94. M9_cumsum                 [ENGINEERED]\n",
      "95. D4_rolling_mean_120       [ENGINEERED]\n",
      "96. P7_rolling_median_5       [ENGINEERED]\n",
      "97. M11_scipy_zscore          [ENGINEERED]\n",
      "98. P7_rolling_std_120        [ENGINEERED]\n",
      "99. V9_lag_1                  [ENGINEERED]\n",
      "100. S2_momentum_20            [ENGINEERED]\n",
      "101. macro_corr_M4_M11_30_macro_high_ratio_60 [ENGINEERED]\n",
      "102. M4                        [ORIGINAL]\n",
      "103. V5                        [ORIGINAL]\n",
      "104. M11_lag_1                 [ENGINEERED]\n",
      "105. E8_dist_to_rolling_mean_20 [ENGINEERED]\n",
      "106. V13_z_5                   [ENGINEERED]\n",
      "107. macro_corr_M4_S2_30_macro_high_ratio_60 [ENGINEERED]\n",
      "108. E14                       [ORIGINAL]\n",
      "109. S8_rolling_min_5          [ENGINEERED]\n",
      "110. S2_momentum_10            [ENGINEERED]\n",
      "111. D4_rolling_std_120        [ENGINEERED]\n",
      "112. E11                       [ORIGINAL]\n",
      "113. P6_pctchg_1               [ENGINEERED]\n",
      "114. M4_rolling_var_5          [ENGINEERED]\n",
      "115. S2_rolling_mean_120       [ENGINEERED]\n",
      "116. P6_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "117. M11_pctchg_1              [ENGINEERED]\n",
      "118. P6_diff_5                 [ENGINEERED]\n",
      "119. M11_dist_to_rolling_mean_20 [ENGINEERED]\n",
      "120. P7_skewness               [ENGINEERED]\n",
      "121. P6_z_120                  [ENGINEERED]\n",
      "122. D4_rolling_var_120        [ENGINEERED]\n",
      "123. S8_dist_to_momentum_20    [ENGINEERED]\n",
      "124. S8_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "125. S8_lag_1                  [ENGINEERED]\n",
      "126. S12_z_5                   [ENGINEERED]\n",
      "127. V9_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "128. M4_rolling_mean_120       [ENGINEERED]\n",
      "129. M11_rolling_median_120    [ENGINEERED]\n",
      "130. D5_cumsum                 [ENGINEERED]\n",
      "131. macro_corr_M4_D8_30_macro_low_ratio_60 [ENGINEERED]\n",
      "132. P8_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "133. macro_corr_M4_P6_30_macro_high_ratio_60 [ENGINEERED]\n",
      "134. D4_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "135. M3                        [ORIGINAL]\n",
      "136. P8_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "137. M11_macro_rolling_sum_5   [ENGINEERED]\n",
      "138. E20                       [ORIGINAL]\n",
      "139. M9_rolling_std_20         [ENGINEERED]\n",
      "140. M2_cumsum                 [ENGINEERED]\n",
      "141. V13_momentum_20           [ENGINEERED]\n",
      "142. M4_momentum_10            [ENGINEERED]\n",
      "143. S8_skewness               [ENGINEERED]\n",
      "144. P7_rolling_median_120     [ENGINEERED]\n",
      "145. M9_z_20                   [ENGINEERED]\n",
      "146. macro_corr_M4_E8_30_macro_low_ratio_120 [ENGINEERED]\n",
      "147. M9_momentum_5             [ENGINEERED]\n",
      "148. P7_diff_10                [ENGINEERED]\n",
      "149. S8_rolling_min_10         [ENGINEERED]\n",
      "150. V9_momentum_20            [ENGINEERED]\n",
      "151. macro_corr_M4_D4_30_macro_low_ratio_60 [ENGINEERED]\n",
      "152. macro_corr_M4_M2_30       [ENGINEERED]\n",
      "153. M9_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "154. P8_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "155. P6_lag_5                  [ENGINEERED]\n",
      "156. V9_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "157. P8_rolling_var_20         [ENGINEERED]\n",
      "158. D8_cumsum                 [ENGINEERED]\n",
      "159. V13_z_120                 [ENGINEERED]\n",
      "160. S2_z_10                   [ENGINEERED]\n",
      "161. M2_momentum_20            [ENGINEERED]\n",
      "162. P11                       [ORIGINAL]\n",
      "163. P13_diff_5                [ENGINEERED]\n",
      "164. macro_corr_M4_P6_30_macro_low_ratio_120 [ENGINEERED]\n",
      "165. P10                       [ORIGINAL]\n",
      "166. P1_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "167. P7_rolling_median_20      [ENGINEERED]\n",
      "168. P7_z_10                   [ENGINEERED]\n",
      "169. M15                       [ORIGINAL]\n",
      "170. P6_lag_1                  [ENGINEERED]\n",
      "171. P7_momentum_20            [ENGINEERED]\n",
      "172. P6_momentum_5             [ENGINEERED]\n",
      "173. M4_dist_to_momentum_5     [ENGINEERED]\n",
      "174. P6_z_60                   [ENGINEERED]\n",
      "175. S2_rolling_max_20         [ENGINEERED]\n",
      "176. M11_rolling_var_5         [ENGINEERED]\n",
      "177. P2                        [ORIGINAL]\n",
      "178. macro_corr_M4_S2_30_macro_high_ratio_20 [ENGINEERED]\n",
      "179. M4_rolling_median_10      [ENGINEERED]\n",
      "180. V13_lag_10                [ENGINEERED]\n",
      "181. V13_z_20                  [ENGINEERED]\n",
      "182. M9_z_10                   [ENGINEERED]\n",
      "183. S2_rolling_mean_10        [ENGINEERED]\n",
      "184. P6_dist_to_momentum_5     [ENGINEERED]\n",
      "185. P3                        [ORIGINAL]\n",
      "186. E8_skewness               [ENGINEERED]\n",
      "187. E8_lag_5                  [ENGINEERED]\n",
      "188. E8_normalized             [ENGINEERED]\n",
      "189. P8_dist_to_momentum_5     [ENGINEERED]\n",
      "190. M11_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "191. P6_dist_to_momentum_10    [ENGINEERED]\n",
      "192. S12_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "193. P8_z_5                    [ENGINEERED]\n",
      "194. M4_lag_5                  [ENGINEERED]\n",
      "195. D4_z_120                  [ENGINEERED]\n",
      "196. P13_rolling_mean_5        [ENGINEERED]\n",
      "197. P8_diff_5                 [ENGINEERED]\n",
      "198. P8_momentum_5             [ENGINEERED]\n",
      "199. E8_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "200. D8_dist_to_momentum_20    [ENGINEERED]\n",
      "\n",
      "Top 10 Ensemble Features by Score:\n",
      "    1. V13_diff_1                | Votes: 2 | Score: 0.961 | [ENGINEERED]\n",
      "    2. V9_diff_1                 | Votes: 2 | Score: 0.697 | [ENGINEERED]\n",
      "    3. M11_diff_5                | Votes: 2 | Score: 0.651 | [ENGINEERED]\n",
      "    4. M4_dist_to_rolling_mean_60 | Votes: 2 | Score: 0.645 | [ENGINEERED]\n",
      "    5. V13_momentum_5            | Votes: 2 | Score: 0.583 | [ENGINEERED]\n",
      "    6. M4_dist_to_rolling_mean_120 | Votes: 2 | Score: 0.579 | [ENGINEERED]\n",
      "    7. V13_dist_to_rolling_mean_5 | Votes: 2 | Score: 0.572 | [ENGINEERED]\n",
      "    8. M4_dist_to_rolling_mean_20 | Votes: 2 | Score: 0.555 | [ENGINEERED]\n",
      "    9. M4_lag_10                 | Votes: 2 | Score: 0.547 | [ENGINEERED]\n",
      "   10. V13_rolling_min_5         | Votes: 2 | Score: 0.537 | [ENGINEERED]\n",
      "\n",
      "Final Training Data Shapes:\n",
      "Training set shape: (6321, 200)\n",
      "Target shape: (6321,)\n",
      "Features selected: 200\n",
      "\n",
      "Enhanced feature selection complete!\n",
      "Ready for model training with dynamically selected features\n"
     ]
    }
   ],
   "source": [
    "# ===== Enhanced Ensemble Feature Selection (Replaces the old selection method) =====\n",
    "from xml.sax.handler import all_features\n",
    "\n",
    "\n",
    "def enhanced_feature_selection(X_features, y_target, final_features, n_features=200, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced ensemble feature selection combining multiple methods.\n",
    "    \n",
    "    Uses dynamic random states based on current time to ensure\n",
    "    different feature selections on each run for robustness testing.\n",
    "    \n",
    "    Args:\n",
    "        X_features: Feature DataFrame (from train_enh after feature engineering)\n",
    "        y_target: Target Series (from train_enh[TARGET])\n",
    "        final_features: List of original base features for categorization\n",
    "        n_features: Number of top features to select\n",
    "        verbose: Print progress information\n",
    "    \n",
    "    Returns:\n",
    "        list: Selected feature names using ensemble voting\n",
    "        dict: Detailed results from each method\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate dynamic random state based on current time\n",
    "    dynamic_seed = int(time.time() * 1000) % 100000\n",
    "    if verbose:\n",
    "        print(f\"Using dynamic seed: {dynamic_seed}\")\n",
    "    \n",
    "    # Remove zero variance features first\n",
    "    vt = VarianceThreshold(threshold=1e-6)\n",
    "    X_filtered = X_features.loc[:, vt.fit(X_features).get_support()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Features after variance filtering: {X_filtered.shape[1]}\")\n",
    "    \n",
    "    feature_scores = {}\n",
    "    selected_features_by_method = {}\n",
    "    \n",
    "    # Method 1a: Gradient Boosting Importance (Dynamic Random State)\n",
    "    if verbose:\n",
    "        print(\"Method 1a: Gradient Boosting Feature Importance...\")\n",
    "    \n",
    "    gb = GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=3, \n",
    "        random_state=dynamic_seed,  # Dynamic instead of 42\n",
    "        subsample=0.8,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    gb.fit(X_filtered, y_target)\n",
    "    gb_scores = pd.Series(gb.feature_importances_, index=X_filtered.columns)\n",
    "    gb_top = gb_scores.nlargest(n_features).index.tolist()\n",
    "    \n",
    "    feature_scores['gradient_boosting'] = gb_scores\n",
    "    selected_features_by_method['gradient_boosting'] = gb_top\n",
    "\n",
    "    # Method 1b: Bagging Regressor Importance\n",
    "    if verbose:\n",
    "        print(\"Method 1b: Bagging Regressor Feature Importance...\")\n",
    "\n",
    "    br = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=100,\n",
    "        max_samples=0.8,\n",
    "        random_state=dynamic_seed,\n",
    "        n_jobs=-1,\n",
    "        bootstrap=True\n",
    "    )\n",
    "    br.fit(X_filtered, y_target)\n",
    "\n",
    "    # Aggregate feature importances from fitted base estimators\n",
    "    _importances = np.zeros(X_filtered.shape[1], dtype=float)\n",
    "    count = 0\n",
    "    for est in br.estimators_:\n",
    "        est_imp = getattr(est, \"feature_importances_\", None)\n",
    "        if est_imp is not None:\n",
    "            _importances += est_imp\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        _importances /= count\n",
    "    else:\n",
    "        _importances = np.zeros(X_filtered.shape[1], dtype=float)\n",
    "\n",
    "    br_scores = pd.Series(_importances, index=X_filtered.columns)\n",
    "    br_top = br_scores.nlargest(n_features).index.tolist()\n",
    "\n",
    "    feature_scores['bagging_regressor'] = br_scores\n",
    "    selected_features_by_method['bagging_regressor'] = br_top\n",
    "    \n",
    "    # Ensemble Voting: Features selected by multiple methods\n",
    "    if verbose:\n",
    "        print(\"Ensemble Voting: Combining all methods...\")\n",
    "    \n",
    "    # Count votes for each feature\n",
    "    feature_votes = {}\n",
    "    all_features = set()\n",
    "    \n",
    "    for method, features in selected_features_by_method.items():\n",
    "        all_features.update(features)\n",
    "        for feature in features:\n",
    "            feature_votes[feature] = feature_votes.get(feature, 0) + 1\n",
    "    \n",
    "    # Sort by votes, then by average score across methods\n",
    "    def get_average_score(feature):\n",
    "        scores = []\n",
    "        for method, score_series in feature_scores.items():\n",
    "            if feature in score_series.index:\n",
    "                # Normalize scores to [0,1] for fair averaging\n",
    "                normalized = (score_series[feature] - score_series.min()) / (score_series.max() - score_series.min() + 1e-10)\n",
    "                scores.append(normalized)\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # Create ensemble ranking\n",
    "    ensemble_ranking = []\n",
    "    for feature in all_features:\n",
    "        votes = feature_votes.get(feature, 0)\n",
    "        avg_score = get_average_score(feature)\n",
    "        ensemble_ranking.append({\n",
    "            'feature': feature,\n",
    "            'votes': votes,\n",
    "            'avg_score': avg_score,\n",
    "            'ensemble_score': votes + avg_score  # Hybrid scoring\n",
    "        })\n",
    "    \n",
    "    # Sort by ensemble score (votes + normalized average)\n",
    "    ensemble_ranking.sort(key=lambda x: x['ensemble_score'], reverse=True)\n",
    "    \n",
    "    # Select top features\n",
    "    ensemble_features = [item['feature'] for item in ensemble_ranking[:n_features]]\n",
    "    \n",
    "    if verbose:\n",
    "        # Separate engineered features from original base features for reporting\n",
    "        original_features_in_selection = [f for f in ensemble_features if f in final_features]\n",
    "        new_engineered_features = [f for f in ensemble_features if f not in final_features]\n",
    "        \n",
    "        print(f\"\\nEnsemble Feature Selection Results:\")\n",
    "        print(f\"   Total unique features considered: {len(all_features)}\")\n",
    "        num_1a = sum(1 for f in all_features if f in selected_features_by_method.get('gradient_boosting', []))\n",
    "        num_1b = sum(1 for f in all_features if f in selected_features_by_method.get('bagging_regressor', []))\n",
    "\n",
    "        print(f\"   Selected by 1a (gradient boosting): {num_1a}\")\n",
    "        print(f\"   Selected by 1b (bagging regressor):  {num_1b}\")\n",
    "        print(f\"   Final ensemble selection: {len(ensemble_features)} features\")\n",
    "\n",
    "        print(f\"   Final ensemble selection: {len(ensemble_features)} features\")\n",
    "        \n",
    "        print(f\"\\nFeature Engineering Results:\")\n",
    "        print(f\"Original base features available: {len(final_features)}\")\n",
    "        print(f\"Original features selected: {len(original_features_in_selection)}\")\n",
    "        print(f\"New engineered features created: {len(new_engineered_features)}\")\n",
    "        print(f\"Total features for modeling: {len(ensemble_features)}\")\n",
    "\n",
    "        print(f\"\\nNew engineered features added:\")\n",
    "        for i, feat in enumerate(new_engineered_features, 1):\n",
    "            print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "        print(f\"\\nAll {len(ensemble_features)} selected features:\")\n",
    "        for i, feat in enumerate(ensemble_features, 1):\n",
    "            feat_type = \"ORIGINAL\" if feat in final_features else \"ENGINEERED\"\n",
    "            print(f\"{i:2d}. {feat:<25} [{feat_type}]\")\n",
    "        \n",
    "        # Show top 10 features with vote details\n",
    "        print(f\"\\nTop 10 Ensemble Features by Score:\")\n",
    "        for i, item in enumerate(ensemble_ranking[:10], 1):\n",
    "            feat_type = \"ORIGINAL\" if item['feature'] in final_features else \"ENGINEERED\"\n",
    "            print(f\"   {i:2d}. {item['feature']:<25} | Votes: {item['votes']} | Score: {item['avg_score']:.3f} | [{feat_type}]\")\n",
    "    \n",
    "    results = {\n",
    "        'ensemble_features': ensemble_features,\n",
    "        'method_features': selected_features_by_method,\n",
    "        'feature_scores': feature_scores,\n",
    "        'ensemble_ranking': ensemble_ranking,\n",
    "        'dynamic_seed': dynamic_seed,\n",
    "        'original_features_selected': [f for f in ensemble_features if f in final_features],\n",
    "        'engineered_features_selected': [f for f in ensemble_features if f not in final_features]\n",
    "    }\n",
    "    \n",
    "    return ensemble_features, results\n",
    "\n",
    "# Feature Engineering & Data Preparation\n",
    "top_features = ['M4', 'V13', 'M11', 'S2', 'D4', 'D1', 'D2', 'E8', 'P6', 'M2', \n",
    "                'D8', 'M9', 'P8', 'P7', 'S12', 'P13', 'V9', 'D5', 'P1', 'S8']\n",
    "\n",
    "print(\"Creating advanced features for training data...\")\n",
    "\n",
    "# CORRECT: Create DataFrame with date_id + features but WITHOUT target columns to prevent data leakage\n",
    "columns_to_exclude = [\"market_forward_excess_returns\", \"forward_returns\", \"risk_free_rate\"]\n",
    "columns_to_include = ['date_id'] + [col for col in final_features if col in train_full.columns]\n",
    "\n",
    "train_for_engineering = train_full[columns_to_include].copy()\n",
    "\n",
    "print(f\"Columns for feature engineering (count): {len(columns_to_include)}\")\n",
    "# print name of columns included\n",
    "print(f\"Included columns names: {columns_to_include}\")\n",
    "# print length of excluded columns\n",
    "print(f\"Excluded columns (count): {len(columns_to_exclude)}\")\n",
    "print(f\"Excluded columns (prevent leakage) names: {columns_to_exclude}\")\n",
    "\n",
    "train_enh = create_advanced_features(\n",
    "    train_for_engineering,\n",
    "    top_features=top_features,\n",
    "    window_sizes=(5, 10, 20, 60, 120),\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "# Add target back AFTER feature engineering for supervised selection\n",
    "train_enh[TARGET] = train_full[TARGET].values\n",
    "\n",
    "# Now do ENHANCED supervised feature selection with target present\n",
    "feature_columns = [c for c in train_enh.columns if c not in ['date_id', TARGET]]\n",
    "print(f\"Feature columns for selection: {len(feature_columns)} total features available\")\n",
    "\n",
    "# Supervised feature selection using ENHANCED method\n",
    "X_features = train_enh[feature_columns]\n",
    "y_target = train_enh[TARGET]\n",
    "\n",
    "# Apply Enhanced Feature Selection (replaces the old single-method approach)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED ENSEMBLE FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "selected_features, selection_results = enhanced_feature_selection(\n",
    "    X_features, y_target, final_features,\n",
    "    n_features=200,  \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Final feature matrices\n",
    "X = train_enh[selected_features].astype('float32')\n",
    "y = train_enh[TARGET].astype('float32')\n",
    "\n",
    "print(f\"\\nFinal Training Data Shapes:\")\n",
    "print(f\"Training set shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features selected: {len(selected_features)}\")\n",
    "\n",
    "# Store for later use in inference\n",
    "final_selected_features = selected_features\n",
    "\n",
    "print(\"\\nEnhanced feature selection complete!\")\n",
    "print(\"Ready for model training with dynamically selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab59ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  CatBoost Base Model \n",
    "# ================================================================\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate dynamic random state for models\n",
    "model_seed = int(time.time() * 1000) % 100000\n",
    "\n",
    "# Initialize CatBoostRegressor with BEST features and dynamic random state\n",
    "ml_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=model_seed,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Fit the model on full training data\n",
    "ml_model.fit(X, y)\n",
    "\n",
    "# Create validation split for evaluation (use time series split - no shuffle)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.3, shuffle=False\n",
    ")\n",
    "\n",
    "# Get validation indices to retrieve forward_returns and risk_free_rate\n",
    "val_idx = X_val.index\n",
    "\n",
    "# Generate predictions on validation set\n",
    "val_cat = ml_model.predict(X_val)\n",
    "\n",
    "# print(f\"Model trained on {X.shape[0]} samples\")\n",
    "# print(f\"Validation split: {X_val.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abfa77f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "OFFICIAL COMPETITION METRIC RESULTS\n",
      "==================================================\n",
      "Adjusted Sharpe:      5.9732\n",
      "Raw Sharpe:           6.1268\n",
      "Vol Penalty:          1.0257\n",
      "Return Penalty:       1.0000\n",
      "Strategy Vol:         24.05%\n",
      "Market Vol:           19.62%\n",
      "Strategy Return:      147.38% ann.\n",
      "Market Return:        6.89% ann.\n",
      "Return Gap:           0.00\n"
     ]
    }
   ],
   "source": [
    "# ===== OFFICIAL Competition Metric Implementation =====\n",
    "def compute_official_score(positions, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Exact implementation of the official Kaggle competition metric.\n",
    "    Uses GEOMETRIC mean (compounding) like the competition.\n",
    "    \n",
    "    Args:\n",
    "        positions: array of allocations in [0, 2]\n",
    "        # # forward_returns: actual market returns\n",
    "        risk_free_rate: risk-free rate\n",
    "    \n",
    "    Returns:\n",
    "        dict with adjusted_sharpe and all components\n",
    "    \"\"\"\n",
    "    positions = np.asarray(positions)\n",
    "    forward_returns = np.asarray(forward_returns)\n",
    "    risk_free_rate = np.asarray(risk_free_rate)\n",
    "    \n",
    "    n = len(positions)\n",
    "    trading_days_per_yr = 252\n",
    "    \n",
    "    # Strategy returns: rf*(1-position) + position*forward_returns\n",
    "    strategy_returns = risk_free_rate * (1 - positions) + positions * forward_returns\n",
    "    \n",
    "    # Strategy excess returns and GEOMETRIC mean\n",
    "    strategy_excess_returns = strategy_returns - risk_free_rate\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod()\n",
    "    strategy_mean_excess_return = strategy_excess_cumulative ** (1 / n) - 1\n",
    "    strategy_std = np.std(strategy_returns)\n",
    "    \n",
    "    if strategy_std == 0:\n",
    "        return {'adjusted_sharpe': 0, 'error': 'strategy_std is zero'}\n",
    "    \n",
    "    # Strategy Sharpe ratio\n",
    "    sharpe = (strategy_mean_excess_return / strategy_std) * np.sqrt(trading_days_per_yr)\n",
    "    strategy_volatility = strategy_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    # Market excess returns and GEOMETRIC mean\n",
    "    market_excess_returns = forward_returns - risk_free_rate\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod()\n",
    "    market_mean_excess_return = market_excess_cumulative ** (1 / n) - 1\n",
    "    market_std = np.std(forward_returns)\n",
    "    market_volatility = market_std * np.sqrt(trading_days_per_yr) * 100\n",
    "    \n",
    "    if market_volatility == 0:\n",
    "        return {'adjusted_sharpe': 0, 'error': 'market_volatility is zero'}\n",
    "    \n",
    "    # Volatility penalty: penalize if strategy vol > 1.2 * market vol\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2)\n",
    "    vol_penalty = 1 + excess_vol\n",
    "    \n",
    "    # Return gap penalty: penalize underperforming the market (quadratic)\n",
    "    return_gap = max(0, (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr)\n",
    "    return_penalty = 1 + (return_gap ** 2) / 100\n",
    "    \n",
    "    # Adjusted Sharpe\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "    \n",
    "    return {\n",
    "        'adjusted_sharpe': float(adjusted_sharpe),\n",
    "        'raw_sharpe': float(sharpe),\n",
    "        'vol_penalty': float(vol_penalty),\n",
    "        'return_penalty': float(return_penalty),\n",
    "        'strategy_volatility': float(strategy_volatility),\n",
    "        'market_volatility': float(market_volatility),\n",
    "        'strategy_geo_return': float(strategy_mean_excess_return * 100 * trading_days_per_yr),  # annualized %\n",
    "        'market_geo_return': float(market_mean_excess_return * 100 * trading_days_per_yr),  # annualized %\n",
    "        'return_gap': float(return_gap)\n",
    "    }\n",
    "\n",
    "\n",
    "def robust_signal_to_weight(sig, lower=0.0, upper=2.0):\n",
    "    \"\"\"\n",
    "    Map raw signals to weights robustly using percentile clipping.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig)\n",
    "    lo = np.nanpercentile(sig, 5)\n",
    "    hi = np.nanpercentile(sig, 95)\n",
    "    if np.isclose(hi, lo):\n",
    "        sig_z = (sig - np.nanmean(sig)) / (np.nanstd(sig) + 1e-12)\n",
    "        w = 2.0 / (1.0 + np.exp(-sig_z))\n",
    "    else:\n",
    "        w = (sig - lo) / (hi - lo + 1e-12) * (upper - lower) + lower\n",
    "    return np.clip(w, lower, upper)\n",
    "\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_df = train.loc[val_idx].copy()\n",
    "val_weights = robust_signal_to_weight(val_cat)\n",
    "\n",
    "res = compute_official_score(\n",
    "    val_weights, \n",
    "    val_df['forward_returns'].to_numpy(), \n",
    "    val_df['risk_free_rate'].to_numpy()\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OFFICIAL COMPETITION METRIC RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Adjusted Sharpe:      {res['adjusted_sharpe']:.4f}\")\n",
    "print(f\"Raw Sharpe:           {res['raw_sharpe']:.4f}\")\n",
    "print(f\"Vol Penalty:          {res['vol_penalty']:.4f}\")\n",
    "print(f\"Return Penalty:       {res['return_penalty']:.4f}\")\n",
    "print(f\"Strategy Vol:         {res['strategy_volatility']:.2f}%\")\n",
    "print(f\"Market Vol:           {res['market_volatility']:.2f}%\")\n",
    "print(f\"Strategy Return:      {res['strategy_geo_return']:.2f}% ann.\")\n",
    "print(f\"Market Return:        {res['market_geo_return']:.2f}% ann.\")\n",
    "print(f\"Return Gap:           {res['return_gap']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "613ec4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing scaling factor k using scipy...\n",
      "Verifying with grid search...\n",
      "\n",
      "==================================================\n",
      "CALIBRATION RESULTS\n",
      "==================================================\n",
      "Optimal k:            4.4475\n",
      "Adjusted Sharpe:      4.9390\n",
      "Raw Sharpe:           4.9390\n",
      "Vol Penalty:          1.0000\n",
      "Return Penalty:       1.0000\n",
      "Strategy Vol:         1.43%\n",
      "Market Vol:           19.62%\n",
      "Vol Ratio:            0.07x\n"
     ]
    }
   ],
   "source": [
    "# ===== Volatility Scaling Calibration - OPTIMIZED FOR COMPETITION METRIC =====\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "VOL_MULTIPLIER_LIMIT = 1.19  # Slightly below 1.9 for safety buffer\n",
    "\n",
    "def competition_objective(k, predictions, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Objective function: negative adjusted sharpe (for minimization).\n",
    "    Directly optimizes the official competition metric.\n",
    "    \"\"\"\n",
    "    exposures = np.clip(k * predictions, 0, 2)\n",
    "    result = compute_official_score(exposures, forward_returns, risk_free_rate)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        return 1e9  # Return large value if error\n",
    "    \n",
    "    # Check volatility constraint\n",
    "    if result['strategy_volatility'] > VOL_MULTIPLIER_LIMIT * result['market_volatility']:\n",
    "        return 1e9  # Penalize exceeding vol limit\n",
    "    \n",
    "    return -result['adjusted_sharpe']  # Negative for minimization\n",
    "\n",
    "\n",
    "# Get validation data\n",
    "val_pred = ml_model.predict(X_val)\n",
    "val_forward_returns = train.loc[val_idx, 'forward_returns'].values\n",
    "val_risk_free_rate = train.loc[val_idx, 'risk_free_rate'].values\n",
    "\n",
    "# Method 1: Scipy optimization\n",
    "print(\"Optimizing scaling factor k using scipy...\")\n",
    "result = minimize_scalar(\n",
    "    lambda k: competition_objective(k, val_pred, val_forward_returns, val_risk_free_rate),\n",
    "    bounds=(0.01, 5.0),\n",
    "    method='bounded'\n",
    ")\n",
    "scipy_best_k = result.x\n",
    "scipy_best_score = -result.fun\n",
    "\n",
    "# Method 2: Grid search (as backup/verification)\n",
    "print(\"Verifying with grid search...\")\n",
    "best_k, best_adjusted_sharpe = 0.1, -1e9\n",
    "best_result = None\n",
    "\n",
    "for k in np.linspace(0.01, 5.0, 200):\n",
    "    exposures = np.clip(k * val_pred, 0, 2)\n",
    "    res = compute_official_score(exposures, val_forward_returns, val_risk_free_rate)\n",
    "    \n",
    "    if 'error' in res:\n",
    "        continue\n",
    "    \n",
    "    # Check volatility constraint\n",
    "    if res['strategy_volatility'] <= VOL_MULTIPLIER_LIMIT * res['market_volatility']:\n",
    "        if res['adjusted_sharpe'] > best_adjusted_sharpe:\n",
    "            best_k = k\n",
    "            best_adjusted_sharpe = res['adjusted_sharpe']\n",
    "            best_result = res\n",
    "\n",
    "# Use the better result\n",
    "if scipy_best_score > best_adjusted_sharpe:\n",
    "    best_k = scipy_best_k\n",
    "    best_adjusted_sharpe = scipy_best_score\n",
    "    exposures = np.clip(best_k * val_pred, 0, 2)\n",
    "    best_result = compute_official_score(exposures, val_forward_returns, val_risk_free_rate)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CALIBRATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Optimal k:            {best_k:.4f}\")\n",
    "print(f\"Adjusted Sharpe:      {best_adjusted_sharpe:.4f}\")\n",
    "print(f\"Raw Sharpe:           {best_result['raw_sharpe']:.4f}\")\n",
    "print(f\"Vol Penalty:          {best_result['vol_penalty']:.4f}\")\n",
    "print(f\"Return Penalty:       {best_result['return_penalty']:.4f}\")\n",
    "print(f\"Strategy Vol:         {best_result['strategy_volatility']:.2f}%\")\n",
    "print(f\"Market Vol:           {best_result['market_volatility']:.2f}%\")\n",
    "print(f\"Vol Ratio:            {best_result['strategy_volatility']/best_result['market_volatility']:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf149a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying feature engineering to test set...\n",
      "Creating Level 1 features (Core)...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can only compute partial correlations for lags up to 50% of the sample size. The requested nlags 10 must be < 5.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying feature engineering to test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m test_for_engineering = test[columns_to_include].copy()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m test_enh = \u001b[43mcreate_advanced_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_for_engineering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use SAME windows as training!\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Extract same selected features used in training\u001b[39;00m\n\u001b[32m     15\u001b[39m X_test = test_enh[selected_features].astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 323\u001b[39m, in \u001b[36mcreate_advanced_features\u001b[39m\u001b[34m(df, top_features, macro_prefixes, window_sizes, shift, inplace)\u001b[39m\n\u001b[32m    321\u001b[39m create_scipy_zscore_features(top_features)\n\u001b[32m    322\u001b[39m create_spread_features(top_features)\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m \u001b[43mcreate_pacf_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m create_acf_features(top_features)\n\u001b[32m    325\u001b[39m create_autocorr_features(top_features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mcreate_advanced_features.<locals>.create_pacf_features\u001b[39m\u001b[34m(cols, nlags, shift_periods)\u001b[39m\n\u001b[32m    120\u001b[39m shifted_col = df[c].shift(shift_periods).fillna(\u001b[32m0.0\u001b[39m)\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Change from 'ywunbiased' to 'yw' (Yule-Walker method)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m pacf_values = \u001b[43mpacf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshifted_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnlags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, nlags + \u001b[32m1\u001b[39m):\n\u001b[32m    124\u001b[39m     df[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_pacf_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = pacf_values[lag]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\statsmodels\\tsa\\stattools.py:1050\u001b[39m, in \u001b[36mpacf\u001b[39m\u001b[34m(x, nlags, method, alpha)\u001b[39m\n\u001b[32m   1048\u001b[39m nlags = \u001b[38;5;28mmax\u001b[39m(nlags, \u001b[32m1\u001b[39m)\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nlags > x.shape[\u001b[32m0\u001b[39m] // \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1050\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1051\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCan only compute partial correlations for lags up to 50\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[33mf the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1052\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msample size. The requested nlags \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnlags\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be < \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1053\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.shape[\u001b[32m0\u001b[39m]\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39m\u001b[32m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1054\u001b[39m     )\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mols\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mols-inefficient\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mols-adjusted\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1056\u001b[39m     efficient = \u001b[33m\"\u001b[39m\u001b[33minefficient\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m method\n",
      "\u001b[31mValueError\u001b[39m: Can only compute partial correlations for lags up to 50% of the sample size. The requested nlags 10 must be < 5."
     ]
    }
   ],
   "source": [
    "# ===== Test Predictions + Smoothing (OPTIMIZED) =====\n",
    "\n",
    "# Apply feature engineering to test set (same as training)\n",
    "print(\"Applying feature engineering to test set...\")\n",
    "test_for_engineering = test[columns_to_include].copy()\n",
    "\n",
    "test_enh = create_advanced_features(\n",
    "    test_for_engineering,\n",
    "    top_features=top_features,\n",
    "    window_sizes=(5, 10, 20, 60, 120),  # Use SAME windows as training!\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "# Extract same selected features used in training\n",
    "X_test = test_enh[selected_features].astype('float32')\n",
    "print(f\"Test set feature engineering complete: {X_test.shape}\")\n",
    "\n",
    "# Generate predictions\n",
    "test_pred = ml_model.predict(X_test)\n",
    "\n",
    "# Apply optimal k scaling\n",
    "scaled_pred = np.clip(best_k * test_pred, 0, 2)\n",
    "\n",
    "# Apply exponential smoothing to reduce volatility\n",
    "# Lower alpha = more smoothing = lower volatility\n",
    "alpha = 0.7  # Reduced from 0.8 for more smoothing\n",
    "\n",
    "smoothed_allocation = []\n",
    "prev = 1.0  # Start at market exposure (safe default)\n",
    "\n",
    "for x in scaled_pred:\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "# Safety check: ensure mean is close to 1.0 (market exposure)\n",
    "# This helps avoid the return penalty\n",
    "mean_alloc = smoothed_allocation.mean()\n",
    "print(f\"\\nPre-adjustment mean allocation: {mean_alloc:.4f}\")\n",
    "\n",
    "# Optional: Bias toward market exposure if model is uncertain\n",
    "# Uncomment below if you're getting penalized for underperforming market\n",
    "# if mean_alloc < 0.9:\n",
    "#     smoothed_allocation = smoothed_allocation * (1.0 / mean_alloc) * 0.95\n",
    "#     smoothed_allocation = np.clip(smoothed_allocation, 0, 2)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test['date_id'],\n",
    "    'prediction': smoothed_allocation.astype('float32')\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUBMISSION STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"File saved: submission.csv\")\n",
    "print(f\"Range:      [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")\n",
    "print(f\"Mean:       {smoothed_allocation.mean():.4f}\")\n",
    "print(f\"Median:     {np.median(smoothed_allocation):.4f}\")\n",
    "print(f\"Std:        {smoothed_allocation.std():.4f}\")\n",
    "print(f\"Total:      {len(smoothed_allocation)} predictions\")\n",
    "\n",
    "# Show distribution\n",
    "print(f\"\\nAllocation Distribution:\")\n",
    "print(f\"  < 0.5:    {(smoothed_allocation < 0.5).sum()} ({(smoothed_allocation < 0.5).mean()*100:.1f}%)\")\n",
    "print(f\"  0.5-1.0:  {((smoothed_allocation >= 0.5) & (smoothed_allocation < 1.0)).sum()} ({((smoothed_allocation >= 0.5) & (smoothed_allocation < 1.0)).mean()*100:.1f}%)\")\n",
    "print(f\"  1.0-1.5:  {((smoothed_allocation >= 1.0) & (smoothed_allocation < 1.5)).sum()} ({((smoothed_allocation >= 1.0) & (smoothed_allocation < 1.5)).mean()*100:.1f}%)\")\n",
    "print(f\"  >= 1.5:   {(smoothed_allocation >= 1.5).sum()} ({(smoothed_allocation >= 1.5).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b103351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b34b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added to stop here the running of the code\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# #  Competition-Compliant Inference Function\n",
    "# # ================================================================\n",
    "# _ml_model = ml_model  # Use the CatBoost model trained in Cell 9\n",
    "# _feat_cols = selected_features  # Use selected features from Cell 8\n",
    "\n",
    "# def predict(pl_df):\n",
    "#     \"\"\"Competition inference function - returns DataFrame with predictions.\"\"\"\n",
    "#     # Convert Polars to Pandas and handle missing values\n",
    "#     pdf = pl_df.to_pandas().fillna(0.0)\n",
    "    \n",
    "#     # Ensure all required features are present\n",
    "#     for f in _feat_cols:\n",
    "#         if f not in pdf.columns:\n",
    "#             pdf[f] = 0.0\n",
    "    \n",
    "#     # Make predictions from CatBoost model\n",
    "#     preds = _ml_model.predict(pdf[_feat_cols])\n",
    "    \n",
    "#     # Map predictions to weights [0, 2] using percentile scaling\n",
    "#     lo, hi = np.percentile(preds, [5, 95])\n",
    "#     weights = np.clip((preds - lo) / (hi - lo + 1e-9) * 2.0, 0, 2)\n",
    "    \n",
    "#     return pd.DataFrame({\"prediction\": weights.astype(\"float32\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ba005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # Kaggle Evaluation Server\n",
    "# # ================================================================\n",
    "\n",
    "# server = kdeval.DefaultInferenceServer(predict)\n",
    "\n",
    "# if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "#     server.serve()\n",
    "# else:\n",
    "#     server.run_local_gateway((DATA_DIR,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# #  Competition-Compliant Inference Function (Single Float Return)\n",
    "# # ================================================================\n",
    "# _ml_model = ml_model  # Use the CatBoost model trained in Cell 9\n",
    "# _feat_cols = selected_features  # Use selected features from Cell 8\n",
    "# _history_returns = list(train.loc[val_idx, 'forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "# def predict(pl_df: pl.DataFrame) -> float:\n",
    "#     \"\"\"Competition inference function - returns single float allocation.\"\"\"\n",
    "#     global _history_returns\n",
    "    \n",
    "#     # Convert Polars to Pandas and handle missing values\n",
    "#     pdf = pl_df.to_pandas().fillna(0.0)\n",
    "    \n",
    "#     # Ensure all required features are present\n",
    "#     for f in _feat_cols:\n",
    "#         if f not in pdf.columns:\n",
    "#             pdf[f] = 0.0\n",
    "    \n",
    "#     # Make prediction from CatBoost model\n",
    "#     pred = _ml_model.predict(pdf[_feat_cols])[0]  # Get first prediction\n",
    "    \n",
    "#     # Estimate rolling volatility for scaling\n",
    "#     vol_est = np.std(_history_returns) if len(_history_returns) > 1 else 1e-3\n",
    "    \n",
    "#     # Map prediction to weight using robust scaling\n",
    "#     lo = np.nanpercentile([pred], 5)\n",
    "#     hi = np.nanpercentile([pred], 95)\n",
    "    \n",
    "#     if np.isclose(hi, lo):\n",
    "#         # Fallback: simple clipping\n",
    "#         weight = np.clip(pred, 0, 2)\n",
    "#     else:\n",
    "#         weight = (pred - lo) / (hi - lo + 1e-9) * 2.0\n",
    "    \n",
    "#     # Apply volatility adjustment and clip to [0, 2]\n",
    "#     allocation = float(np.clip(weight / (vol_est + 1e-9), 0, 2))\n",
    "    \n",
    "#     # Update history for rolling volatility estimation\n",
    "#     if 'lagged_forward_returns' in pl_df.columns:\n",
    "#         try:\n",
    "#             _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "#         except:\n",
    "#             _history_returns.append(0.0)\n",
    "#     else:\n",
    "#         _history_returns.append(0.0)\n",
    "    \n",
    "#     # Keep only last VOL_WINDOW entries\n",
    "#     _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "    \n",
    "#     return allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edf9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total selected features for inference:\", len(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # Kaggle Evaluation Server / Local Submission\n",
    "# # ================================================================\n",
    "\n",
    "# if KAGGLE_ENV:\n",
    "#     # Kaggle competition environment\n",
    "#     server = kdeval.DefaultInferenceServer(predict)\n",
    "    \n",
    "#     if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#         server.serve()\n",
    "#     else:\n",
    "#         server.run_local_gateway((str(DATA_DIR),))\n",
    "        \n",
    "# else:\n",
    "#     # Local environment - generate submission file\n",
    "#     print(\"Local mode - generating submission file...\")\n",
    "#     print(f\"Test set size: {len(test)} rows\")\n",
    "    \n",
    "#     # Apply same feature engineering pipeline as training\n",
    "#     print(\"Applying feature engineering to test set...\")\n",
    "    \n",
    "#     # Prepare test data for feature engineering\n",
    "#     test_for_engineering = test[columns_to_include].copy()\n",
    "    \n",
    "#     # Apply same feature engineering pipeline\n",
    "#     test_enh = create_advanced_features(\n",
    "#         test_for_engineering,\n",
    "#         top_features=top_features,\n",
    "#         window_sizes=(5, 10, 20, 60, 120),\n",
    "#         shift=1\n",
    "#     )\n",
    "    \n",
    "#     # Extract same selected features\n",
    "#     X_test = test_enh[selected_features].astype('float32')\n",
    "    \n",
    "#     print(f\"Feature engineering complete: {X_test.shape[1]} features\")\n",
    "    \n",
    "#     # Generate predictions using trained CatBoost model\n",
    "#     print(\"\\nGenerating predictions with CatBoost model...\")\n",
    "#     test_pred = ml_model.predict(X_test)\n",
    "    \n",
    "#     # Map predictions to weights [0, 2] using robust scaling\n",
    "#     lo, hi = np.percentile(test_pred, [5, 95])\n",
    "#     test_weights = np.clip((test_pred - lo) / (hi - lo + 1e-9) * 2.0, 0, 2)\n",
    "    \n",
    "#     # Apply exponential smoothing\n",
    "#     alpha = 0.8\n",
    "#     smoothed_allocation = []\n",
    "#     prev = 0.0\n",
    "#     for x in test_weights:\n",
    "#         s = alpha * x + (1 - alpha) * prev\n",
    "#         smoothed_allocation.append(s)\n",
    "#         prev = s\n",
    "#     smoothed_allocation = np.array(smoothed_allocation)\n",
    "    \n",
    "#     # Create submission DataFrame\n",
    "#     submission = pd.DataFrame({\n",
    "#         'date_id': test['date_id'],\n",
    "#         'prediction': smoothed_allocation.astype('float32')\n",
    "#     })\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "#     print(\"\\nSubmission file saved: submission.csv\")\n",
    "#     print(f\"Prediction statistics:\")\n",
    "#     print(f\"  Range: [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")\n",
    "#     print(f\"  Mean: {smoothed_allocation.mean():.4f}\")\n",
    "#     print(f\"  Median: {np.median(smoothed_allocation):.4f}\")\n",
    "#     print(f\"  Std: {smoothed_allocation.std():.4f}\")\n",
    "#     print(f\"  Total predictions: {len(smoothed_allocation)}\")\n",
    "    \n",
    "#     # Display submission preview\n",
    "#     print(\"\\nSubmission preview:\")\n",
    "#     print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf902b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48212c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kaggle Submission Version\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c3abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOL_MULTIPLIER_LIMIT = 1.2\n",
    "VOL_WINDOW = 20 # volatility window in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d0450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen scaling factor k=5.000 with Sharpe=4.95\n"
     ]
    }
   ],
   "source": [
    "## Volatility Scaling Calibration\n",
    "def strategy_stats(returns, exposures):\n",
    "    strategy_returns = exposures * returns\n",
    "    mean = np.nanmean(strategy_returns)\n",
    "    std  = np.nanstd(strategy_returns)\n",
    "    sharpe = (mean / (std + 1e-9)) * np.sqrt(252)\n",
    "    vol = std * np.sqrt(252)\n",
    "    return {'sharpe': sharpe, 'vol': vol}\n",
    "\n",
    "# Remove num_iteration parameter - not needed without early stopping\n",
    "val_pred = ml_model.predict(X_val)\n",
    "\n",
    "# Get forward_returns from validation data\n",
    "val_forward_returns = train.loc[val_idx, 'forward_returns'].values\n",
    "market_vol = np.nanstd(val_forward_returns) * np.sqrt(252)\n",
    "\n",
    "best_k, best_sharpe = 0.1, -1e9\n",
    "for k in np.linspace(0.01, 5.0, 100):\n",
    "    exposures = np.clip((k * val_pred), 0, 2)\n",
    "    stats = strategy_stats(val_forward_returns, exposures)\n",
    "    if stats['vol'] <= VOL_MULTIPLIER_LIMIT * market_vol and stats['sharpe'] > best_sharpe:\n",
    "        best_k = k\n",
    "        best_sharpe = stats['sharpe']\n",
    "\n",
    "print(f\"Chosen scaling factor k={best_k:.3f} with Sharpe={best_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9abad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying feature engineering to test set...\n",
      "Creating Level 1 features (Core)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying feature engineering to test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m test_for_engineering = test[columns_to_include].copy()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m test_enh = \u001b[43mcreate_advanced_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_for_engineering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Reduced window sizes for faster inference on test set only\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshift\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Extract same selected features used in training\u001b[39;00m\n\u001b[32m     15\u001b[39m X_test = test_enh[selected_features].astype(\u001b[33m'\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 318\u001b[39m, in \u001b[36mcreate_advanced_features\u001b[39m\u001b[34m(df, top_features, macro_prefixes, window_sizes, shift, inplace)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCreating Level 1 features (Core)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    317\u001b[39m _to_numeric(top_features)\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m \u001b[43mcreate_rolling_and_distance_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m create_variance_features(top_features)\n\u001b[32m    320\u001b[39m create_zscore_features(top_features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mcreate_advanced_features.<locals>.create_rolling_and_distance_features\u001b[39m\u001b[34m(cols, windows, shift_periods)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwindows\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Apply shift BEFORE rolling operations\u001b[39;49;00m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshifted_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_periods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroll\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifted_col\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrolling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_periods\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Create roll object once\u001b[39;49;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "## Test Predictions + Smoothing\n",
    "\n",
    "# Apply feature engineering to test set (same as training)\n",
    "print(\"Applying feature engineering to test set...\")\n",
    "test_for_engineering = test[columns_to_include].copy()\n",
    "\n",
    "test_enh = create_advanced_features(\n",
    "    test_for_engineering,\n",
    "    top_features=top_features,\n",
    "    window_sizes=(5), # Reduced window sizes for faster inference on test set only\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "# Extract same selected features used in training\n",
    "X_test = test_enh[selected_features].astype('float32')\n",
    "\n",
    "print(f\"Test set feature engineering complete: {X_test.shape}\")\n",
    "\n",
    "# Generate predictions (remove num_iteration parameter)\n",
    "test_pred = ml_model.predict(X_test)\n",
    "\n",
    "# Apply exponential smoothing with best_k scaling\n",
    "alpha = 0.8\n",
    "smoothed_allocation = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test['date_id'],\n",
    "    'prediction': smoothed_allocation.astype('float32')\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(\"submission_cat_200feat.csv\", index=False)\n",
    "print(f\"\\nSaved submission_cat_200feat.csv\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Range: [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")\n",
    "print(f\"  Mean: {smoothed_allocation.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(smoothed_allocation):.4f}\")\n",
    "print(f\"  Total predictions: {len(smoothed_allocation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d08b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Change window_sizes=(5, 10) for test set ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc84b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24397300",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Predictions + Smoothing\n",
    "\n",
    "# Apply feature engineering to test set (same as training)\n",
    "print(\"Applying feature engineering to test set...\")\n",
    "test_for_engineering = test[columns_to_include].copy()\n",
    "\n",
    "test_enh = create_advanced_features(\n",
    "    test_for_engineering,\n",
    "    top_features=top_features,\n",
    "    window_sizes=(5, 10),\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "# Extract same selected features used in training\n",
    "X_test = test_enh[selected_features].astype('float32')\n",
    "\n",
    "print(f\"Test set feature engineering complete: {X_test.shape}\")\n",
    "\n",
    "# Generate predictions (remove num_iteration parameter)\n",
    "test_pred = ml_model.predict(X_test)\n",
    "\n",
    "# Apply exponential smoothing with best_k scaling\n",
    "alpha = 0.8\n",
    "smoothed_allocation = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test['date_id'],\n",
    "    'prediction': smoothed_allocation.astype('float32')\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_df.to_csv(\"submission_cat_200feat.csv\", index=False)\n",
    "print(f\"\\nSaved submission_cat_200feat.csv\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Range: [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")\n",
    "print(f\"  Mean: {smoothed_allocation.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(smoothed_allocation):.4f}\")\n",
    "print(f\"  Total predictions: {len(smoothed_allocation)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
