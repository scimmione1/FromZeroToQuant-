{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b72ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# üß† HULL TACTICAL MARKET PREDICTION ‚Äî ENSEMBLE + SHARPE PENALTY\n",
    "# ================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from typing import Tuple, Dict\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import kaggle_evaluation.default_inference_server as kdeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d614beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1Ô∏è‚É£ Data Loading\n",
    "# ================================================================\n",
    "\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "TARGET = \"market_forward_excess_returns\"\n",
    "drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\"]\n",
    "features = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "\n",
    "train = train.fillna(0.0)\n",
    "test = test.fillna(0.0)\n",
    "\n",
    "X = train[features]\n",
    "y = train[TARGET]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0de0fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "# TARGET\n",
    "# drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e08be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Training CatBoost model with TimeSeries CV...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "‚úÖ Best Params: {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 5, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2Ô∏è‚É£ CatBoost Base Model (GridSearch + TimeSeriesSplit)\n",
    "# ================================================================\n",
    "\n",
    "print(\"‚è≥ Training CatBoost model with TimeSeries CV...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cbc = CatBoostRegressor(loss_function='RMSE', verbose=0, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'iterations': [300, 500],\n",
    "    'l2_leaf_reg': [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=cbc,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X, y)\n",
    "best_cbc = grid.best_estimator_\n",
    "print(f\"‚úÖ Best Params: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ed795b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neural Network trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3Ô∏è‚É£ Neural Network Model (Feedforward Regressor)\n",
    "# ================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def build_nn(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "nn_model = build_nn(X_scaled.shape[1])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# last 20% time-based validation\n",
    "date_cut = train[\"date_id\"].quantile(0.8)\n",
    "train_idx = train[\"date_id\"] <= date_cut\n",
    "val_idx = train[\"date_id\"] > date_cut\n",
    "\n",
    "X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "\n",
    "nn_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "             epochs=100, batch_size=256, verbose=0, callbacks=[es])\n",
    "print(\"‚úÖ Neural Network trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca37f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 4Ô∏è‚É£ Ensemble Prediction (0.X √ó CatBoost + 0.XX √ó NN)\n",
    "# ================================================================\n",
    "ensemble_cat_pct = 0\n",
    "ensemble_nn_pct = 1\n",
    "\n",
    "val_cat = best_cbc.predict(X.loc[val_idx])\n",
    "val_nn = nn_model.predict(X_scaled[val_idx]).ravel()\n",
    "\n",
    "val_ensemble = ensemble_cat_pct * val_cat + ensemble_nn_pct * val_nn\n",
    "val_df = train.loc[val_idx].copy()\n",
    "val_df[\"pred\"] = val_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b696f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Validation Sharpe: -1.913 | Penalty: 0.0001 | Adjusted: -1.913\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 5Ô∏è‚É£ Sharpe Penalty Formula (Official Metric Proxy)\n",
    "# ================================================================\n",
    "def sharpe_ratio(returns):\n",
    "    mean = np.mean(returns)\n",
    "    vol = np.std(returns)\n",
    "    return (mean / (vol + 1e-9)) * np.sqrt(252)\n",
    "\n",
    "def sharpe_penalty_score(pred, fwd_ret, risk_free_rate, penalty_weight=0.1):\n",
    "    \"\"\"\n",
    "    Penalize models with excessive volatility relative to returns.\n",
    "    \"\"\"\n",
    "    strat_ret = pred * fwd_ret - risk_free_rate\n",
    "    raw_sharpe = sharpe_ratio(strat_ret)\n",
    "    vol = np.std(strat_ret)\n",
    "    penalty = penalty_weight * vol\n",
    "    adj_sharpe = raw_sharpe - penalty\n",
    "    return adj_sharpe, raw_sharpe, penalty\n",
    "\n",
    "adj_sharpe, raw_sharpe, penalty = sharpe_penalty_score(\n",
    "    val_df[\"pred\"], val_df[\"forward_returns\"], val_df[\"risk_free_rate\"]\n",
    ")\n",
    "\n",
    "print(f\"üìä Validation Sharpe: {raw_sharpe:.3f} | Penalty: {penalty:.4f} | Adjusted: {adj_sharpe:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c23c741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 6Ô∏è‚É£ Competition-Compliant Inference Function\n",
    "# ================================================================\n",
    "_cat_model = best_cbc\n",
    "_nn_model = nn_model\n",
    "_scaler = scaler\n",
    "_feat_cols = features\n",
    "\n",
    "def predict(pl_df):\n",
    "    \"\"\"Competition inference function.\"\"\"\n",
    "    pdf = pl_df.to_pandas().fillna(0.0)\n",
    "    for f in _feat_cols:\n",
    "        if f not in pdf.columns:\n",
    "            pdf[f] = 0.0\n",
    "    Xp = pdf[_feat_cols].values\n",
    "    Xp_scaled = _scaler.transform(Xp)\n",
    "    pred_cat = _cat_model.predict(pdf[_feat_cols])\n",
    "    pred_nn = _nn_model.predict(Xp_scaled, verbose=0).ravel()\n",
    "    preds = 0.4 * pred_cat + 0.6 * pred_nn\n",
    "    lo, hi = np.percentile(preds, [5, 95])\n",
    "    weights = np.clip((preds - lo) / (hi - lo + 1e-9) * 2.0, 0, 2)\n",
    "    return pd.DataFrame({\"prediction\": weights.astype(\"float32\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c2467ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped weights stats: 0.0 0.0002454032158233926 0.7244656132242924 1.9997560089382562 2.0\n",
      "Strategy raw Sharpe: 0.5348811070399059\n",
      "Adjusted Sharpe: 0.48137230953453\n",
      "Vol penalty: 1.0 Return penalty: 1.1111588607093656 Return gap: 3.334049500372867\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected evaluation: use mapped weights and official formula =====\n",
    "import numpy as np\n",
    "\n",
    "def compute_strategy_stats(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute strategy daily returns and Sharpe (annualized).\n",
    "    weights: array-like positions in [0,2]\n",
    "    forward_returns, risk_free_rate: arrays aligned\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    w = np.asarray(weights)\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "\n",
    "    # Strategy return per day: rf*(1 - w) + w * forward_returns\n",
    "    # Strategy excess over rf:\n",
    "    strat_ret = rf * (1.0 - w) + w * fr\n",
    "    strat_excess = strat_ret - rf   # == w * (fr - rf)\n",
    "    # annualized sharpe\n",
    "    mean = np.nanmean(strat_excess)\n",
    "    std = np.nanstd(strat_excess)\n",
    "    sharpe = (mean / (std + 1e-12)) * np.sqrt(252) if std > 0 else 0.0\n",
    "    # annualized vol of strategy returns\n",
    "    vol_ann = std * np.sqrt(252)\n",
    "    return {\n",
    "        'sharpe': sharpe,\n",
    "        'vol_ann': vol_ann,\n",
    "        'mean_daily_excess': mean,\n",
    "        'std_daily_excess': std,\n",
    "        'strat_ret_series': strat_ret,\n",
    "        'strat_excess_series': strat_excess\n",
    "    }\n",
    "\n",
    "def sharpe_penalty_official(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute adjusted Sharpe like the official metric:\n",
    "    - compute strategy sharpe\n",
    "    - compute market vol and strategy vol, form vol_penalty = 1 + max(0, strategy_vol/market_vol - 1.2)\n",
    "    - compute return_gap penalty like (max(0, (market_mean_excess - strat_mean_excess) * 100 * 252))**2 / 100 etc.\n",
    "    Returns adjusted_sharpe (float) and components.\n",
    "    \"\"\"\n",
    "    # strategy stats\n",
    "    stats = compute_strategy_stats(weights, forward_returns, risk_free_rate)\n",
    "    strat_excess = stats['strat_excess_series']\n",
    "    strat_sharpe = stats['sharpe']\n",
    "    strat_vol = stats['vol_ann']\n",
    "    # market stats\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "    market_excess = fr - rf\n",
    "    market_mean_excess = ( (1 + market_excess).prod() ) ** (1.0 / len(market_excess)) - 1 if len(market_excess)>0 else 0.0\n",
    "    # fallback simpler mean if product fails\n",
    "    # but safer to use mean:\n",
    "    market_mean_excess = np.nanmean(market_excess)\n",
    "    market_std = np.nanstd(fr)\n",
    "    market_vol = market_std * np.sqrt(252) if market_std>0 else 1e-9\n",
    "\n",
    "    # volatility penalty\n",
    "    excess_vol = max(0.0, (strat_vol / (market_vol + 1e-12)) - 1.2)\n",
    "    vol_penalty = 1.0 + excess_vol\n",
    "\n",
    "    # return gap penalty (use squared scaled gap similar to demo code)\n",
    "    strat_mean_excess = np.nanmean(strat_excess)\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100 * 252)  # percent annualized gap\n",
    "    return_penalty = 1.0 + (return_gap**2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = strat_sharpe / (vol_penalty * return_penalty + 1e-12)\n",
    "    return {\n",
    "        'adjusted_sharpe': adjusted_sharpe,\n",
    "        'strat_sharpe': strat_sharpe,\n",
    "        'vol_penalty': vol_penalty,\n",
    "        'return_penalty': return_penalty,\n",
    "        'strat_vol': strat_vol,\n",
    "        'market_vol': market_vol,\n",
    "        'return_gap': return_gap\n",
    "    }\n",
    "\n",
    "# ===== Use it on validation properly mapping raw preds to weights =====\n",
    "\n",
    "# val_ensemble is your raw ensemble prediction (unmapped)\n",
    "# First map to weights using your mapping function (or revised mapping)\n",
    "def robust_signal_to_weight(sig, lower=0.0, upper=2.0):\n",
    "    \"\"\"\n",
    "    Map raw signals to weights robustly using percentile clipping and stable scaling.\n",
    "    If distribution is degenerate, fallback to standard scaling.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig)\n",
    "    lo = np.nanpercentile(sig, 5)\n",
    "    hi = np.nanpercentile(sig, 95)\n",
    "    if np.isclose(hi, lo):\n",
    "        # fallback: z-score and sigmoid mapping\n",
    "        sig_z = (sig - np.nanmean(sig)) / (np.nanstd(sig) + 1e-12)\n",
    "        # map z to [0,2] via logistic\n",
    "        w = 2.0 / (1.0 + np.exp(-sig_z))\n",
    "    else:\n",
    "        w = (sig - lo) / (hi - lo + 1e-12) * (upper - lower) + lower\n",
    "    return np.clip(w, lower, upper)\n",
    "\n",
    "# compute mapped weights\n",
    "val_weights = robust_signal_to_weight(val_ensemble)   # or pass val_cat/val_nn separately\n",
    "\n",
    "# compute official adjusted sharpe and components\n",
    "res = sharpe_penalty_official(val_weights, val_df['forward_returns'].to_numpy(), val_df['risk_free_rate'].to_numpy())\n",
    "\n",
    "print(\"Mapped weights stats:\", np.nanmin(val_weights), np.nanpercentile(val_weights,5), np.nanmedian(val_weights), np.nanpercentile(val_weights,95), np.nanmax(val_weights))\n",
    "print(\"Strategy raw Sharpe:\", res['strat_sharpe'])\n",
    "print(\"Adjusted Sharpe:\", res['adjusted_sharpe'])\n",
    "print(\"Vol penalty:\", res['vol_penalty'], \"Return penalty:\", res['return_penalty'], \"Return gap:\", res['return_gap'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2cf25c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kdeval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 7Ô∏è‚É£ Kaggle Evaluation Server\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ================================================================\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m server = \u001b[43mkdeval\u001b[49m.DefaultInferenceServer(predict)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.getenv(\u001b[33m\"\u001b[39m\u001b[33mKAGGLE_IS_COMPETITION_RERUN\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      8\u001b[39m     server.serve()\n",
      "\u001b[31mNameError\u001b[39m: name 'kdeval' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 7Ô∏è‚É£ Kaggle Evaluation Server\n",
    "# ================================================================\n",
    "\n",
    "server = kdeval.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"):\n",
    "    server.serve()\n",
    "else:\n",
    "    server.run_local_gateway((DATA_DIR,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
