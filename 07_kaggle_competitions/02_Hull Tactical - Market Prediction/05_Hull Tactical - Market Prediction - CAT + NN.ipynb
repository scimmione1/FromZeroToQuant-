{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b72ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Running in local environment - kaggle_evaluation not available\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# üß† HULL TACTICAL MARKET PREDICTION ‚Äî ENSEMBLE + SHARPE PENALTY\n",
    "# ================================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use only first GPU if multiple\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# Import TensorFlow after setting environment variables\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # Only show errors\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Try to import kaggle_evaluation, handle if not available\n",
    "try:\n",
    "    import kaggle_evaluation.default_inference_server as kdeval\n",
    "    KAGGLE_ENV = True\n",
    "    print(\"‚úÖ Running in Kaggle competition environment\")\n",
    "except ImportError:\n",
    "    KAGGLE_ENV = False\n",
    "    print(\"‚ö†Ô∏è Running in local environment - kaggle_evaluation not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d614beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 1Ô∏è‚É£ Data Loading\n",
    "# ================================================================\n",
    "\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "VOL_WINDOW = 20                 # volatility window in days\n",
    "\n",
    "train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
    "test = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
    "\n",
    "TARGET = \"market_forward_excess_returns\"\n",
    "drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\"]\n",
    "features = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "\n",
    "train = train.fillna(0.0)\n",
    "test = test.fillna(0.0)\n",
    "\n",
    "X = train[features]\n",
    "y = train[TARGET]\n",
    "X_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0de0fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features\n",
    "# TARGET\n",
    "# drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e08be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Training CatBoost model with TimeSeries CV...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "‚úÖ Best Params: {'depth': 6, 'iterations': 300, 'l2_leaf_reg': 5, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2Ô∏è‚É£ CatBoost Base Model (GridSearch + TimeSeriesSplit)\n",
    "# ================================================================\n",
    "\n",
    "print(\"‚è≥ Training CatBoost model with TimeSeries CV...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# check here random_state = 42 for reproducibility!\n",
    "cbc = CatBoostRegressor(loss_function='RMSE', verbose=0, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'iterations': [300, 500],\n",
    "    'l2_leaf_reg': [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=cbc,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X, y)\n",
    "best_cbc = grid.best_estimator_\n",
    "print(f\"‚úÖ Best Params: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed795b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neural Network trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3Ô∏è‚É£ Neural Network Model (Feedforward Regressor)\n",
    "# ================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def build_nn(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "nn_model = build_nn(X_scaled.shape[1])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# last 20% time-based validation\n",
    "date_cut = train[\"date_id\"].quantile(0.8)\n",
    "train_idx = train[\"date_id\"] <= date_cut\n",
    "val_idx = train[\"date_id\"] > date_cut\n",
    "\n",
    "X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "\n",
    "nn_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "             epochs=100, batch_size=256, verbose=0, callbacks=[es])\n",
    "print(\"‚úÖ Neural Network trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca37f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 4Ô∏è‚É£ Ensemble Prediction (0.X √ó CatBoost + 0.XX √ó NN)\n",
    "# ================================================================\n",
    "ensemble_cat_pct = 0.8\n",
    "ensemble_nn_pct = 0.2\n",
    "\n",
    "val_cat = best_cbc.predict(X.loc[val_idx])\n",
    "val_nn = nn_model.predict(X_scaled[val_idx]).ravel()\n",
    "\n",
    "val_ensemble = ensemble_cat_pct * val_cat + ensemble_nn_pct * val_nn\n",
    "val_df = train.loc[val_idx].copy()\n",
    "val_df[\"pred\"] = val_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "932d6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped weights stats: 0.0 0.00011407995932442048 0.6734031891719277 1.999445759525307 2.0\n",
      "Strategy raw Sharpe: 2.5628806391535512\n",
      "Adjusted Sharpe: 2.562880639150988\n",
      "Vol penalty: 1.0 Return penalty: 1.0 Return gap: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected evaluation: use mapped weights and official formula =====\n",
    "def compute_strategy_stats(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute strategy daily returns and Sharpe (annualized).\n",
    "    weights: array-like positions in [0,2]\n",
    "    forward_returns, risk_free_rate: arrays aligned\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    w = np.asarray(weights)\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "\n",
    "    # Strategy return per day: rf*(1 - w) + w * forward_returns\n",
    "    # Strategy excess over rf:\n",
    "    strat_ret = rf * (1.0 - w) + w * fr\n",
    "    strat_excess = strat_ret - rf   # == w * (fr - rf)\n",
    "    # annualized sharpe\n",
    "    mean = np.nanmean(strat_excess)\n",
    "    std = np.nanstd(strat_excess)\n",
    "    sharpe = (mean / (std + 1e-12)) * np.sqrt(252) if std > 0 else 0.0\n",
    "    # annualized vol of strategy returns\n",
    "    vol_ann = std * np.sqrt(252)\n",
    "    return {\n",
    "        'sharpe': sharpe,\n",
    "        'vol_ann': vol_ann,\n",
    "        'mean_daily_excess': mean,\n",
    "        'std_daily_excess': std,\n",
    "        'strat_ret_series': strat_ret,\n",
    "        'strat_excess_series': strat_excess\n",
    "    }\n",
    "\n",
    "def sharpe_penalty_official(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute adjusted Sharpe like the official metric:\n",
    "    - compute strategy sharpe\n",
    "    - compute market vol and strategy vol, form vol_penalty = 1 + max(0, strategy_vol/market_vol - 1.2)\n",
    "    - compute return_gap penalty like (max(0, (market_mean_excess - strat_mean_excess) * 100 * 252))**2 / 100 etc.\n",
    "    Returns adjusted_sharpe (float) and components.\n",
    "    \"\"\"\n",
    "    # strategy stats\n",
    "    stats = compute_strategy_stats(weights, forward_returns, risk_free_rate)\n",
    "    strat_excess = stats['strat_excess_series']\n",
    "    strat_sharpe = stats['sharpe']\n",
    "    strat_vol = stats['vol_ann']\n",
    "    # market stats\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "    market_excess = fr - rf\n",
    "    market_mean_excess = ( (1 + market_excess).prod() ) ** (1.0 / len(market_excess)) - 1 if len(market_excess)>0 else 0.0\n",
    "    # fallback simpler mean if product fails\n",
    "    # but safer to use mean:\n",
    "    market_mean_excess = np.nanmean(market_excess)\n",
    "    market_std = np.nanstd(fr)\n",
    "    market_vol = market_std * np.sqrt(252) if market_std>0 else 1e-9\n",
    "\n",
    "    # volatility penalty\n",
    "    excess_vol = max(0.0, (strat_vol / (market_vol + 1e-12)) - 1.2)\n",
    "    vol_penalty = 1.0 + excess_vol\n",
    "\n",
    "    # return gap penalty (use squared scaled gap similar to demo code)\n",
    "    strat_mean_excess = np.nanmean(strat_excess)\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100 * 252)  # percent annualized gap\n",
    "    return_penalty = 1.0 + (return_gap**2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = strat_sharpe / (vol_penalty * return_penalty + 1e-12)\n",
    "    return {\n",
    "        'adjusted_sharpe': adjusted_sharpe,\n",
    "        'strat_sharpe': strat_sharpe,\n",
    "        'vol_penalty': vol_penalty,\n",
    "        'return_penalty': return_penalty,\n",
    "        'strat_vol': strat_vol,\n",
    "        'market_vol': market_vol,\n",
    "        'return_gap': return_gap\n",
    "    }\n",
    "\n",
    "# ===== Use it on validation properly mapping raw preds to weights =====\n",
    "\n",
    "# val_ensemble is your raw ensemble prediction (unmapped)\n",
    "# First map to weights using your mapping function (or revised mapping)\n",
    "def robust_signal_to_weight(sig, lower=0.0, upper=2.0):\n",
    "    \"\"\"\n",
    "    Map raw signals to weights robustly using percentile clipping and stable scaling.\n",
    "    If distribution is degenerate, fallback to standard scaling.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig)\n",
    "    lo = np.nanpercentile(sig, 5)\n",
    "    hi = np.nanpercentile(sig, 95)\n",
    "    if np.isclose(hi, lo):\n",
    "        # fallback: z-score and sigmoid mapping\n",
    "        sig_z = (sig - np.nanmean(sig)) / (np.nanstd(sig) + 1e-12)\n",
    "        # map z to [0,2] via logistic\n",
    "        w = 2.0 / (1.0 + np.exp(-sig_z))\n",
    "    else:\n",
    "        w = (sig - lo) / (hi - lo + 1e-12) * (upper - lower) + lower\n",
    "    return np.clip(w, lower, upper)\n",
    "\n",
    "# compute mapped weights\n",
    "val_weights = robust_signal_to_weight(val_ensemble)   # or pass val_cat/val_nn separately\n",
    "\n",
    "# compute official adjusted sharpe and components\n",
    "res = sharpe_penalty_official(val_weights, val_df['forward_returns'].to_numpy(), val_df['risk_free_rate'].to_numpy())\n",
    "\n",
    "print(\"Mapped weights stats:\", np.nanmin(val_weights), np.nanpercentile(val_weights,5), np.nanmedian(val_weights), np.nanpercentile(val_weights,95), np.nanmax(val_weights))\n",
    "print(\"Strategy raw Sharpe:\", res['strat_sharpe'])\n",
    "print(\"Adjusted Sharpe:\", res['adjusted_sharpe'])\n",
    "print(\"Vol penalty:\", res['vol_penalty'], \"Return penalty:\", res['return_penalty'], \"Return gap:\", res['return_gap'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23c741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # 6Ô∏è‚É£ Competition-Compliant Inference Function\n",
    "# # ================================================================\n",
    "# _cat_model = best_cbc\n",
    "# _nn_model = nn_model\n",
    "# _scaler = scaler\n",
    "# _feat_cols = features\n",
    "\n",
    "# \"\"\"\n",
    "#     Check if is really necessary exchange from pl to pd and back to pl?\n",
    "#     pl.DataFrame (we convert to pandas inside)\n",
    "# \"\"\"\n",
    "# def predict(pl_df):\n",
    "#     \"\"\"Competition inference function.\"\"\"\n",
    "#     pdf = pl_df.to_pandas().fillna(0.0)\n",
    "#     for f in _feat_cols:\n",
    "#         if f not in pdf.columns:\n",
    "#             pdf[f] = 0.0\n",
    "#     Xp = pdf[_feat_cols].values\n",
    "#     Xp_scaled = _scaler.transform(Xp)\n",
    "#     pred_cat = _cat_model.predict(pdf[_feat_cols])\n",
    "#     pred_nn = _nn_model.predict(Xp_scaled, verbose=0).ravel()\n",
    "#     preds = ensemble_cat_pct * pred_cat + ensemble_nn_pct * pred_nn\n",
    "#     lo, hi = np.percentile(preds, [5, 95])\n",
    "#     weights = np.clip((preds - lo) / (hi - lo + 1e-9) * 2.0, 0, 2)\n",
    "#     return pd.DataFrame({\"prediction\": weights.astype(\"float32\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4afa19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 6Ô∏è‚É£ Competition-Compliant Inference Function\n",
    "# ================================================================\n",
    "_cat_model = best_cbc\n",
    "_nn_model = nn_model\n",
    "_scaler = scaler\n",
    "_feat_cols = features\n",
    "_history_returns = list(train.loc[val_idx, 'forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "def predict(pl_df: pl.DataFrame) -> float:\n",
    "    \"\"\"Competition inference function - returns single float allocation.\"\"\"\n",
    "    global _history_returns\n",
    "    \n",
    "    # Convert Polars to Pandas and handle missing values\n",
    "    pdf = pl_df.to_pandas().fillna(0.0)\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for f in _feat_cols:\n",
    "        if f not in pdf.columns:\n",
    "            pdf[f] = 0.0\n",
    "    \n",
    "    # Get features in correct format\n",
    "    X_features = pdf[_feat_cols].values\n",
    "    X_scaled = _scaler.transform(X_features)\n",
    "    \n",
    "    # Make predictions from both models\n",
    "    pred_cat = _cat_model.predict(pdf[_feat_cols])[0]  # Get first prediction\n",
    "    pred_nn = _nn_model.predict(X_scaled, verbose=0).ravel()[0]  # Get first prediction\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    pred = ensemble_cat_pct * pred_cat + ensemble_nn_pct * pred_nn\n",
    "    \n",
    "    # Estimate rolling volatility for scaling\n",
    "    vol_est = np.std(_history_returns) if len(_history_returns) > 1 else 1e-3\n",
    "    \n",
    "    # Scale prediction to allocation with volatility adjustment\n",
    "    allocation = float(np.clip((best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "    \n",
    "    # Update history for rolling volatility estimation\n",
    "    if 'lagged_forward_returns' in pl_df.columns:\n",
    "        try:\n",
    "            _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "        except:\n",
    "            _history_returns.append(0.0)\n",
    "    else:\n",
    "        _history_returns.append(0.0)\n",
    "    \n",
    "    # Keep only last VOL_WINDOW entries\n",
    "    _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "    \n",
    "    return allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca07cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNEXT STEPS, IMPORTANT FOR IMPROVEMENT:\\n\\nStronger feature scaling\\n\\nPCA optional\\n\\nRolling retrain or time-based CV for robustness out of sample\\n\\nOptimization of the mix (CatBoost vs NN) to dynamically find the optimal weight based on your adjusted Sharpe\\nEventually to be extended to more models in the ensemble\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NEXT STEPS, IMPORTANT FOR IMPROVEMENT:\n",
    "\n",
    "Stronger feature scaling\n",
    "\n",
    "PCA optional\n",
    "\n",
    "Rolling retrain or time-based CV for robustness out of sample\n",
    "\n",
    "Optimization of the mix (CatBoost vs NN) to dynamically find the optimal weight based on your adjusted Sharpe\n",
    "Eventually to be extended to more models in the ensemble\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b37e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2cf25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # 7Ô∏è‚É£ Kaggle Evaluation Server / Local Submission\n",
    "# # ================================================================\n",
    "\n",
    "# if KAGGLE_ENV:\n",
    "#     # Kaggle competition environment\n",
    "#     server = kdeval.DefaultInferenceServer(predict)\n",
    "    \n",
    "#     if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#         server.serve()\n",
    "#     else:\n",
    "#         server.run_local_gateway((str(DATA_DIR),))\n",
    "        \n",
    "# else:\n",
    "#     # Local environment - generate submission file\n",
    "#     print(\"üîß Local mode - generating submission file...\")\n",
    "    \n",
    "#     # Generate predictions for test set\n",
    "#     test_pred_cat = best_cbc.predict(X_test)\n",
    "#     test_pred_nn = nn_model.predict(scaler.transform(X_test), verbose=0).ravel()\n",
    "#     preds = ensemble_cat_pct * test_pred_cat + ensemble_nn_pct * test_pred_nn\n",
    "    \n",
    "#     # Apply same scaling logic as validation\n",
    "#     test_exposures = np.clip(best_k * preds, 0, 2)\n",
    "    \n",
    "#     # Apply smoothing like in the working example\n",
    "#     alpha = 0.8\n",
    "#     smoothed_allocation = []\n",
    "#     prev = 0.0\n",
    "#     for x in test_exposures:\n",
    "#         s = alpha * x + (1 - alpha) * prev\n",
    "#         smoothed_allocation.append(s)\n",
    "#         prev = s\n",
    "#     smoothed_allocation = np.array(smoothed_allocation)\n",
    "    \n",
    "#     # Create submission\n",
    "#     submission = pd.DataFrame({\n",
    "#         'date_id': test['date_id'],\n",
    "#         'prediction': smoothed_allocation.astype('float32')\n",
    "#     })\n",
    "    \n",
    "#     submission.to_csv('submission_ensemble.csv', index=False)\n",
    "#     print(\"üìÅ Saved submission_ensemble.csv\")\n",
    "#     print(f\"üìä Prediction range: [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
