{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d207142",
   "metadata": {},
   "source": [
    "# Simple Baseline (LightGBM Version) â€” Hull Tactical Market Prediction\n",
    "\n",
    "This notebook provides a **safe, fast and competition-compliant\n",
    "baseline** for the Hull Tactical Market Prediction challenge on Kaggle.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "-   LightGBM regression model (fast and stable)\n",
    "-   Chronological split (no data leakage)\n",
    "-   Median imputation + missing indicators\n",
    "-   Volatility-based signal scaling\n",
    "-   Kaggle-compatible inference server wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d70b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d795b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8990, 98), (10, 99))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Configuration and Data Loading (kaggle_evaluation only)\n",
    "# import kaggle_evaluation.default_inference_server as kdeval\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "# Read CSV files from data_path\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "RANDOM_SEED = 42\n",
    "VOL_MULTIPLIER_LIMIT = 1.2\n",
    "VOL_WINDOW = 20\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Preparation\n",
    "excluded = {'date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns'}\n",
    "feature_cols = [c for c in train_raw.columns if c not in excluded]\n",
    "feature_cols = [c for c in feature_cols if c in test_raw.columns]\n",
    "\n",
    "\"\"\"\n",
    "The third line performs a crucial validation step by ensuring feature consistency between training and test datasets. \n",
    "It filters the feature list to include only columns that exist in both the training data and the test data \n",
    "(test_raw.columns). This step is essential because machine learning models require identical feature structures \n",
    "during training and prediction phases. If a feature exists in training data but not in test data, \n",
    "the model would fail during inference. \n",
    "This defensive programming approach prevents runtime errors and ensures that the model can successfully \n",
    "make predictions on the test set.\n",
    "\n",
    "This two-step filtering process - first removing inappropriate columns, \n",
    "then ensuring train-test consistency - represents a best practice in machine learning pipelines. \n",
    "It creates a robust feature set that avoids data leakage while maintaining compatibility across different data splits, \n",
    "which is particularly important in time-series financial prediction tasks where the test set represents \n",
    "future market conditions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in feature_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "            df[f'{c}_was_na'] = 1\n",
    "            continue\n",
    "        if df[c].dtype.kind in 'fiu':\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "This function implements a robust data preprocessing pipeline that handles missing values and data type inconsistencies \n",
    "while preserving information about missingness patterns - a crucial technique in machine learning feature engineering.\n",
    "\n",
    "The function begins by creating a copy of the input DataFrame to avoid modifying the original data, \n",
    "following defensive programming principles. It then iterates through each feature column to apply consistent \n",
    "preprocessing steps. The first conditional check handles the case where an expected feature column \n",
    "is completely missing from the DataFrame. Rather than failing, it gracefully creates the missing column filled \n",
    "with zeros and immediately creates a corresponding indicator variable set to 1, \n",
    "signaling that this entire feature was absent. \n",
    "This approach ensures model compatibility across datasets with different column structures.\n",
    "\n",
    "For columns that exist in the DataFrame, the function employs different strategies based on data type. \n",
    "The condition df[c].dtype.kind in 'fiu' checks if the column is already numeric (float, integer, or unsigned integer) \n",
    "using pandas' dtype kind codes. For numeric columns, it retrieves the pre-computed median from the median_map dictionary, \n",
    "creates a binary indicator tracking which values were originally missing, fills the missing values with the median, \n",
    "and stores the missingness indicator as a new feature column with the suffix _was_na.\n",
    "\n",
    "The else clause handles columns that aren't recognized as numeric types, which commonly occurs with object columns \n",
    "containing mixed data types or string representations of numbers. The function uses pd.to_numeric() with errors='coerce' \n",
    "to attempt conversion to numeric format, where invalid values become NaN rather than causing errors. \n",
    "After this conversion attempt, it applies the same median imputation and missingness indicator creation \n",
    "process used for originally numeric columns.\n",
    "\n",
    "This dual approach - median imputation combined with missingness indicators - is particularly valuable \n",
    "because it prevents information loss. The median provides a robust central tendency measure that's \n",
    "less sensitive to outliers than the mean, while the binary indicators allow the model to learn patterns \n",
    "related to missingness itself. In financial data, missing values often carry meaningful information \n",
    "(such as certain metrics not being available during market stress), \n",
    "making these indicator features potentially predictive. \n",
    "The function's comprehensive error handling and type conversion ensure it can process datasets \n",
    "with inconsistent formatting while maintaining feature consistency across training and test sets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf47989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 188\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train_raw, val_size=VALIDATION_SIZE)\n",
    "\n",
    "median_map = {c: float(train_df[c].median(skipna=True)) if train_df[c].dtype.kind in 'fiu' else 0.0 \n",
    "              for c in feature_cols}\n",
    "\n",
    "train_p = prepare_df(train_df, median_map, feature_cols)\n",
    "val_p   = prepare_df(val_df, median_map, feature_cols)\n",
    "test_p  = prepare_df(test_raw, median_map, feature_cols)\n",
    "\n",
    "final_features = [f for c in feature_cols for f in (c, f\"{c}_was_na\")]\n",
    "print(\"Number of features:\", len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5935853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D1',\n",
       " 'D1_was_na',\n",
       " 'D2',\n",
       " 'D2_was_na',\n",
       " 'D3',\n",
       " 'D3_was_na',\n",
       " 'D4',\n",
       " 'D4_was_na',\n",
       " 'D5',\n",
       " 'D5_was_na',\n",
       " 'D6',\n",
       " 'D6_was_na',\n",
       " 'D7',\n",
       " 'D7_was_na',\n",
       " 'D8',\n",
       " 'D8_was_na',\n",
       " 'D9',\n",
       " 'D9_was_na',\n",
       " 'E1',\n",
       " 'E1_was_na',\n",
       " 'E10',\n",
       " 'E10_was_na',\n",
       " 'E11',\n",
       " 'E11_was_na',\n",
       " 'E12',\n",
       " 'E12_was_na',\n",
       " 'E13',\n",
       " 'E13_was_na',\n",
       " 'E14',\n",
       " 'E14_was_na',\n",
       " 'E15',\n",
       " 'E15_was_na',\n",
       " 'E16',\n",
       " 'E16_was_na',\n",
       " 'E17',\n",
       " 'E17_was_na',\n",
       " 'E18',\n",
       " 'E18_was_na',\n",
       " 'E19',\n",
       " 'E19_was_na',\n",
       " 'E2',\n",
       " 'E2_was_na',\n",
       " 'E20',\n",
       " 'E20_was_na',\n",
       " 'E3',\n",
       " 'E3_was_na',\n",
       " 'E4',\n",
       " 'E4_was_na',\n",
       " 'E5',\n",
       " 'E5_was_na',\n",
       " 'E6',\n",
       " 'E6_was_na',\n",
       " 'E7',\n",
       " 'E7_was_na',\n",
       " 'E8',\n",
       " 'E8_was_na',\n",
       " 'E9',\n",
       " 'E9_was_na',\n",
       " 'I1',\n",
       " 'I1_was_na',\n",
       " 'I2',\n",
       " 'I2_was_na',\n",
       " 'I3',\n",
       " 'I3_was_na',\n",
       " 'I4',\n",
       " 'I4_was_na',\n",
       " 'I5',\n",
       " 'I5_was_na',\n",
       " 'I6',\n",
       " 'I6_was_na',\n",
       " 'I7',\n",
       " 'I7_was_na',\n",
       " 'I8',\n",
       " 'I8_was_na',\n",
       " 'I9',\n",
       " 'I9_was_na',\n",
       " 'M1',\n",
       " 'M1_was_na',\n",
       " 'M10',\n",
       " 'M10_was_na',\n",
       " 'M11',\n",
       " 'M11_was_na',\n",
       " 'M12',\n",
       " 'M12_was_na',\n",
       " 'M13',\n",
       " 'M13_was_na',\n",
       " 'M14',\n",
       " 'M14_was_na',\n",
       " 'M15',\n",
       " 'M15_was_na',\n",
       " 'M16',\n",
       " 'M16_was_na',\n",
       " 'M17',\n",
       " 'M17_was_na',\n",
       " 'M18',\n",
       " 'M18_was_na',\n",
       " 'M2',\n",
       " 'M2_was_na',\n",
       " 'M3',\n",
       " 'M3_was_na',\n",
       " 'M4',\n",
       " 'M4_was_na',\n",
       " 'M5',\n",
       " 'M5_was_na',\n",
       " 'M6',\n",
       " 'M6_was_na',\n",
       " 'M7',\n",
       " 'M7_was_na',\n",
       " 'M8',\n",
       " 'M8_was_na',\n",
       " 'M9',\n",
       " 'M9_was_na',\n",
       " 'P1',\n",
       " 'P1_was_na',\n",
       " 'P10',\n",
       " 'P10_was_na',\n",
       " 'P11',\n",
       " 'P11_was_na',\n",
       " 'P12',\n",
       " 'P12_was_na',\n",
       " 'P13',\n",
       " 'P13_was_na',\n",
       " 'P2',\n",
       " 'P2_was_na',\n",
       " 'P3',\n",
       " 'P3_was_na',\n",
       " 'P4',\n",
       " 'P4_was_na',\n",
       " 'P5',\n",
       " 'P5_was_na',\n",
       " 'P6',\n",
       " 'P6_was_na',\n",
       " 'P7',\n",
       " 'P7_was_na',\n",
       " 'P8',\n",
       " 'P8_was_na',\n",
       " 'P9',\n",
       " 'P9_was_na',\n",
       " 'S1',\n",
       " 'S1_was_na',\n",
       " 'S10',\n",
       " 'S10_was_na',\n",
       " 'S11',\n",
       " 'S11_was_na',\n",
       " 'S12',\n",
       " 'S12_was_na',\n",
       " 'S2',\n",
       " 'S2_was_na',\n",
       " 'S3',\n",
       " 'S3_was_na',\n",
       " 'S4',\n",
       " 'S4_was_na',\n",
       " 'S5',\n",
       " 'S5_was_na',\n",
       " 'S6',\n",
       " 'S6_was_na',\n",
       " 'S7',\n",
       " 'S7_was_na',\n",
       " 'S8',\n",
       " 'S8_was_na',\n",
       " 'S9',\n",
       " 'S9_was_na',\n",
       " 'V1',\n",
       " 'V1_was_na',\n",
       " 'V10',\n",
       " 'V10_was_na',\n",
       " 'V11',\n",
       " 'V11_was_na',\n",
       " 'V12',\n",
       " 'V12_was_na',\n",
       " 'V13',\n",
       " 'V13_was_na',\n",
       " 'V2',\n",
       " 'V2_was_na',\n",
       " 'V3',\n",
       " 'V3_was_na',\n",
       " 'V4',\n",
       " 'V4_was_na',\n",
       " 'V5',\n",
       " 'V5_was_na',\n",
       " 'V6',\n",
       " 'V6_was_na',\n",
       " 'V7',\n",
       " 'V7_was_na',\n",
       " 'V8',\n",
       " 'V8_was_na',\n",
       " 'V9',\n",
       " 'V9_was_na']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfa9923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 0.010955\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's rmse: 0.0102081\n"
     ]
    }
   ],
   "source": [
    "## LightGBM Training\n",
    "train_data = lgb.Dataset(train_p[final_features], label=train_p['forward_returns'])\n",
    "val_data   = lgb.Dataset(val_p[final_features], label=val_p['forward_returns'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 63,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    num_boost_round=2000,\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39b02fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen scaling factor k=5.000 with Sharpe=0.50\n"
     ]
    }
   ],
   "source": [
    "## Volatility Scaling Calibration\n",
    "def strategy_stats(returns, exposures):\n",
    "    strat = exposures * returns\n",
    "    mean = np.nanmean(strat)\n",
    "    std  = np.nanstd(strat)\n",
    "    sharpe = (mean / (std + 1e-9)) * np.sqrt(252)\n",
    "    vol = std * np.sqrt(252)\n",
    "    return {'sharpe': sharpe, 'vol': vol}\n",
    "\n",
    "val_pred = model.predict(val_p[final_features], num_iteration=model.best_iteration)\n",
    "market_vol = np.nanstd(train_p['forward_returns']) * np.sqrt(252)\n",
    "\n",
    "best_k, best_sharpe = 0.1, -1e9\n",
    "for k in np.linspace(0.01, 5.0, 100):\n",
    "    exposures = np.clip((k * val_pred), 0, 2)\n",
    "    stats = strategy_stats(val_p['forward_returns'], exposures)\n",
    "    if stats['vol'] <= VOL_MULTIPLIER_LIMIT * market_vol and stats['sharpe'] > best_sharpe:\n",
    "        best_k = k\n",
    "        best_sharpe = stats['sharpe']\n",
    "\n",
    "print(f\"Chosen scaling factor k={best_k:.3f} with Sharpe={best_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code implements a sophisticated portfolio optimization technique that calibrates \n",
    "the scaling of model predictions to maximize risk-adjusted returns while controlling \n",
    "portfolio volatility - a critical step in translating machine learning predictions into practical \n",
    "trading signals.\n",
    "\n",
    "The strategy_stats function serves as the core evaluation engine for portfolio performance. \n",
    "\n",
    "It calculates strategy returns by multiplying exposures (position sizes) with actual market returns, \n",
    "effectively simulating the profit and loss of the trading strategy. \n",
    "The function computes the mean and standard deviation of these strategy returns, \n",
    "then derives two key metrics: the Sharpe ratio and annualized volatility. \n",
    "The Sharpe ratio calculation includes a small epsilon value (1e-9) in the denominator \n",
    "to prevent division by zero, and multiplies by the square root of 252 (trading days per year) \n",
    "to annualize the ratio. Similarly, the volatility is annualized by multiplying \n",
    "the standard deviation by âˆš252, converting daily volatility to annual terms for easier interpretation.\n",
    "\n",
    "The calibration process begins by generating model predictions on the validation set \n",
    "using the best iteration from the trained LightGBM model. It then calculates the market's \n",
    "baseline volatility by taking the standard deviation of training returns and annualizing it. \n",
    "This baseline serves as a reference point for controlling the strategy's risk exposure. \n",
    "The algorithm initializes tracking variables for the best scaling factor (best_k) \n",
    "and corresponding Sharpe ratio, starting with conservative values.\n",
    "\n",
    "The heart of the optimization lies in the systematic search over scaling factors using \n",
    "np.linspace(0.01, 5.0, 100), which creates 100 evenly spaced values between 0.01 and 5.0. \n",
    "For each scaling factor k, the code multiplies the model predictions and clips the resulting \n",
    "exposures between 0 and 2, ensuring long-only positions with maximum 200% allocationation. \n",
    "This clipping prevents excessive leverage while allowing for concentrated positions \n",
    "when the model has high confidence.\n",
    "\n",
    "The selection criteria embodied in the conditional statement represents sophisticated \n",
    "risk management. The algorithm only considers scaling factors that satisfy two conditions: \n",
    "the strategy's volatility must not exceed VOL_MULTIPLIER_LIMIT times the market volatility \n",
    "(typically 1.2x, allowing 20% more risk than the market), and the Sharpe ratio must improve \n",
    "upon the current best. This dual constraint ensures that the strategy doesn't take excessive \n",
    "risk while maximizing risk-adjusted performance. The approach is particularly valuable \n",
    "in financial applications where controlling downside risk is as important as maximizing returns, \n",
    "as it prevents the model from being over-aggressive in its position sizing while still capturing \n",
    "the predictive signal effectively.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_lgb_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "## Test Predictions + Smoothing\n",
    "test_pred = model.predict(test_p[final_features], num_iteration=model.best_iteration)\n",
    "\n",
    "alpha = 0.8\n",
    "smoothed_allocation = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test_p['date_id'],\n",
    "    'weight': smoothed_allocation\n",
    "})\n",
    "submission_df.to_csv(\"submission_lgb_fixed.csv\", index=False)\n",
    "print(\"Saved submission_lgb_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48169788",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code implements the final prediction and submission generation phase, incorporating temporal smoothing to create more stable portfolio allocations \n",
    "- a crucial technique for reducing transaction costs and improving real-world trading performance.\n",
    "\n",
    "The process begins by generating raw predictions on the test dataset using the trained LightGBM model. \n",
    "\n",
    "The model.predict() call uses the best_iteration parameter to ensure predictions are made with the optimal number of boosting rounds \n",
    "determined during training, preventing overfitting. These raw predictions represent the model's assessment of expected returns \n",
    "for each date in the test set.\n",
    "\n",
    "The smoothing mechanism employs an exponential moving average (EMA) with an alpha parameter of 0.8. \n",
    "This technique addresses a common problem in quantitative trading: raw model predictions often exhibit high volatility that \n",
    "would result in excessive portfolio turnover if implemented directly. The EMA formula s = alpha * x + (1 - alpha) * prev \n",
    "creates a weighted average between the current scaled prediction and the previous smoothed allocation. With alpha = 0.8, \n",
    "the current prediction receives 80% weight while the previous allocation contributes 20%, \n",
    "striking a balance between responsiveness to new signals and stability.\n",
    "\n",
    "Before applying the smoothing, each prediction is scaled by the optimal factor best_k (determined during the volatility calibration phase) \n",
    "and clipped between 0 and 2 using np.clip(). This clipping enforces the constraint of long-only positions with maximum 200% allocation, \n",
    "preventing the model from suggesting impossible or overly risky position sizes. The clipping occurs within the loop's iteration variable, \n",
    "ensuring that each scaled prediction is properly bounded before entering the smoothing calculation.\n",
    "\n",
    "The smoothing loop maintains state through the prev variable, which starts at 0.0 and gets updated with each smoothed allocation. \n",
    "This creates a temporal dependency where each allocation decision considers not just the current model prediction, \n",
    "but also the recent history of allocations. This approach mimics how professional portfolio managers gradually adjust positions \n",
    "rather than making dramatic changes, reducing market impact and transaction costs.\n",
    "\n",
    "Finally, the code packages the results into a competition-ready submission format. \n",
    "The smoothed allocations are converted to a NumPy array for consistency, \n",
    "then combined with the corresponding date identifiers from the test set into a pandas DataFrame. \n",
    "The submission is saved as \"submission_lgb_fixed.csv\" with index=False to exclude row numbers, \n",
    "creating a clean two-column format that matches competition requirements. \n",
    "This systematic approach to prediction post-processing demonstrates sophisticated understanding of the practical challenges \n",
    "in translating machine learning signals into implementable trading strategies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb838fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kaggle Evaluation Metric:\n",
    "\n",
    "strategy_returns = risk_free_rate * (1 - position) + position * forward_returns\n",
    "\n",
    "if position = 0 â†’ invest in risk-free asset,\n",
    "\n",
    "if position = 1 â†’ invest like the market,\n",
    "\n",
    "if position = 2 â†’ you are leveraged Ã—2 on the market.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kaggle Inference Server Wrapper\n",
    "\n",
    "_model = model\n",
    "_best_k = best_k\n",
    "_history_returns = list(train_p['forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "def predict(pl_df: pl.DataFrame) -> float:\n",
    "    global _history_returns\n",
    "    pdf = pl_df.to_pandas()\n",
    "    pdf_p = prepare_df(pdf, median_map, feature_cols)\n",
    "    for f in final_features:\n",
    "        if f not in pdf_p.columns:\n",
    "            pdf_p[f] = 0.0\n",
    "    x = pdf_p[final_features].to_numpy()\n",
    "    pred = _model.predict(x, num_iteration=_model.best_iteration)[0]\n",
    "    vol_est = np.std(_history_returns) or 1e-3\n",
    "    allocation = float(np.clip((_best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "    if 'lagged_forward_returns' in pl_df.columns:\n",
    "        try:\n",
    "            _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "        except:\n",
    "            _history_returns.append(0.0)\n",
    "    _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "    return alloc\n",
    "\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway((str(DATA_DIR),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fdfba",
   "metadata": {},
   "source": [
    "### Notebook Summary\n",
    "\n",
    "| Feature        | Description                          |\n",
    "|----------------|--------------------------------------|\n",
    "| Model          | LightGBM (fast, robust)              |\n",
    "| Validation     | Time-based (last 2700 days)          |\n",
    "| Imputation     | Median + missing flags               |\n",
    "| Signal control | Volatility scaling (Sharpe-based)    |\n",
    "| Inference      | Kaggle-compatible `predict` function |\n",
    "| Runtime        | \\< 5 minutes on Kaggle GPU notebook  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcbe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115cdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
