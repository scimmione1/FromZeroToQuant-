{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d207142",
   "metadata": {},
   "source": [
    "# Simple Baseline (LightGBM Version) â€” Hull Tactical Market Prediction\n",
    "\n",
    "This notebook provides a **safe, fast and competition-compliant\n",
    "baseline** for the Hull Tactical Market Prediction challenge on Kaggle.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "-   LightGBM regression model (fast and stable)\n",
    "-   Chronological split (no data leakage)\n",
    "-   Median imputation + missing indicators\n",
    "-   Volatility-based signal scaling\n",
    "-   Kaggle-compatible inference server wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "39d70b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18d795b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8990, 98), (10, 99))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Configuration and Data Loading (kaggle_evaluation only)\n",
    "# import kaggle_evaluation.default_inference_server as kdeval\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "# Read CSV files from data_path\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "import random\n",
    "RANDOM_SEED = random.randint(1, 10000)\n",
    "\n",
    "VOL_MULTIPLIER_LIMIT = 1.2\n",
    "VOL_WINDOW = 20\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f378b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe third line performs a crucial validation step by ensuring feature consistency between training and test datasets. \\nIt filters the feature list to include only columns that exist in both the training data and the test data \\n(test_raw.columns). This step is essential because machine learning models require identical feature structures \\nduring training and prediction phases. If a feature exists in training data but not in test data, \\nthe model would fail during inference. \\nThis defensive programming approach prevents runtime errors and ensures that the model can successfully \\nmake predictions on the test set.\\n\\nThis two-step filtering process - first removing inappropriate columns, \\nthen ensuring train-test consistency - represents a best practice in machine learning pipelines. \\nIt creates a robust feature set that avoids data leakage while maintaining compatibility across different data splits, \\nwhich is particularly important in time-series financial prediction tasks where the test set represents \\nfuture market conditions.\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Feature Preparation\n",
    "excluded = {'date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns'}\n",
    "feature_cols = [c for c in train_raw.columns if c not in excluded]\n",
    "feature_cols = [c for c in feature_cols if c in test_raw.columns]\n",
    "\n",
    "\"\"\"\n",
    "The third line performs a crucial validation step by ensuring feature consistency between training and test datasets. \n",
    "It filters the feature list to include only columns that exist in both the training data and the test data \n",
    "(test_raw.columns). This step is essential because machine learning models require identical feature structures \n",
    "during training and prediction phases. If a feature exists in training data but not in test data, \n",
    "the model would fail during inference. \n",
    "This defensive programming approach prevents runtime errors and ensures that the model can successfully \n",
    "make predictions on the test set.\n",
    "\n",
    "This two-step filtering process - first removing inappropriate columns, \n",
    "then ensuring train-test consistency - represents a best practice in machine learning pipelines. \n",
    "It creates a robust feature set that avoids data leakage while maintaining compatibility across different data splits, \n",
    "which is particularly important in time-series financial prediction tasks where the test set represents \n",
    "future market conditions.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc3e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis function implements a robust data preprocessing pipeline that handles missing values and data type inconsistencies \\nwhile preserving information about missingness patterns - a crucial technique in machine learning feature engineering.\\n\\nThe function begins by creating a copy of the input DataFrame to avoid modifying the original data, \\nfollowing defensive programming principles. It then iterates through each feature column to apply consistent \\npreprocessing steps. The first conditional check handles the case where an expected feature column \\nis completely missing from the DataFrame. Rather than failing, it gracefully creates the missing column filled \\nwith zeros and immediately creates a corresponding indicator variable set to 1, \\nsignaling that this entire feature was absent. \\nThis approach ensures model compatibility across datasets with different column structures.\\n\\nFor columns that exist in the DataFrame, the function employs different strategy_returnsegies based on data type. \\nThe condition df[c].dtype.kind in 'fiu' checks if the column is already numeric (float, integer, or unsigned integer) \\nusing pandas' dtype kind codes. For numeric columns, it retrieves the pre-computed median from the median_map dictionary, \\ncreates a binary indicator tracking which values were originally missing, fills the missing values with the median, \\nand stores the missingness indicator as a new feature column with the suffix _was_na.\\n\\nThe else clause handles columns that aren't recognized as numeric types, which commonly occurs with object columns \\ncontaining mixed data types or string representations of numbers. The function uses pd.to_numeric() with errors='coerce' \\nto attempt conversion to numeric format, where invalid values become NaN rather than causing errors. \\nAfter this conversion attempt, it applies the same median imputation and missingness indicator creation \\nprocess used for originally numeric columns.\\n\\nThis dual approach - median imputation combined with missingness indicators - is particularly valuable \\nbecause it prevents information loss. The median provides a robust central tendency measure that's \\nless sensitive to outliers than the mean, while the binary indicators allow the model to learn patterns \\nrelated to missingness itself. In financial data, missing values often carry meaningful information \\n(such as certain metrics not being available during market stress), \\nmaking these indicator features potentially predictive. \\nThe function's comprehensive error handling and type conversion ensure it can process datasets \\nwith inconsistent formatting while maintaining feature consistency across training and test sets.\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in feature_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "            df[f'{c}_was_na'] = 1\n",
    "            continue\n",
    "        if df[c].dtype.kind in 'fiu':\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f1df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This function implements a robust data preprocessing pipeline that handles missing values and data type inconsistencies \n",
    "while preserving information about missingness patterns - a crucial technique in machine learning feature engineering.\n",
    "\n",
    "The function begins by creating a copy of the input DataFrame to avoid modifying the original data, \n",
    "following defensive programming principles. It then iterates through each feature column to apply consistent \n",
    "preprocessing steps. The first conditional check handles the case where an expected feature column \n",
    "is completely missing from the DataFrame. Rather than failing, it gracefully creates the missing column filled \n",
    "with zeros and immediately creates a corresponding indicator variable set to 1, \n",
    "signaling that this entire feature was absent. \n",
    "This approach ensures model compatibility across datasets with different column structures.\n",
    "\n",
    "For columns that exist in the DataFrame, the function employs different strategy_returnsegies based on data type. \n",
    "The condition df[c].dtype.kind in 'fiu' checks if the column is already numeric (float, integer, or unsigned integer) \n",
    "using pandas' dtype kind codes. For numeric columns, it retrieves the pre-computed median from the median_map dictionary, \n",
    "creates a binary indicator tracking which values were originally missing, fills the missing values with the median, \n",
    "and stores the missingness indicator as a new feature column with the suffix _was_na.\n",
    "\n",
    "The else clause handles columns that aren't recognized as numeric types, which commonly occurs with object columns \n",
    "containing mixed data types or string representations of numbers. The function uses pd.to_numeric() with errors='coerce' \n",
    "to attempt conversion to numeric format, where invalid values become NaN rather than causing errors. \n",
    "After this conversion attempt, it applies the same median imputation and missingness indicator creation \n",
    "process used for originally numeric columns.\n",
    "\n",
    "This dual approach - median imputation combined with missingness indicators - is particularly valuable \n",
    "because it prevents information loss. The median provides a robust central tendency measure that's \n",
    "less sensitive to outliers than the mean, while the binary indicators allow the model to learn patterns \n",
    "related to missingness itself. In financial data, missing values often carry meaningful information \n",
    "(such as certain metrics not being available during market stress), \n",
    "making these indicator features potentially predictive. \n",
    "The function's comprehensive error handling and type conversion ensure it can process datasets \n",
    "with inconsistent formatting while maintaining feature consistency across training and test sets.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4cf47989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 188\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train_raw, val_size=VALIDATION_SIZE)\n",
    "\n",
    "median_map = {c: float(train_df[c].median(skipna=True)) if train_df[c].dtype.kind in 'fiu' else 0.0 \n",
    "              for c in feature_cols}\n",
    "\n",
    "train_p = prepare_df(train_df, median_map, feature_cols)\n",
    "val_p   = prepare_df(val_df, median_map, feature_cols)\n",
    "test_p  = prepare_df(test_raw, median_map, feature_cols)\n",
    "\n",
    "final_features = [f for c in feature_cols for f in (c, f\"{c}_was_na\")]\n",
    "print(\"Number of features:\", len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5935853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0cfa9923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 0.0108706\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's rmse: 0.0101998\n"
     ]
    }
   ],
   "source": [
    "## LightGBM Training\n",
    "train_data = lgb.Dataset(train_p[final_features], label=train_p['forward_returns'])\n",
    "val_data   = lgb.Dataset(val_p[final_features], label=val_p['forward_returns'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 63,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    num_boost_round=2000,\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39b02fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen scaling factor k=5.000 with Sharpe=0.57\n"
     ]
    }
   ],
   "source": [
    "## Volatility Scaling Calibration\n",
    "def strategy_stats(returns, exposures):\n",
    "    strategy_returns = exposures * returns\n",
    "    mean = np.nanmean(strategy_returns)\n",
    "    std  = np.nanstd(strategy_returns)\n",
    "    sharpe = (mean / (std + 1e-9)) * np.sqrt(252)\n",
    "    vol = std * np.sqrt(252)\n",
    "    return {'sharpe': sharpe, 'vol': vol}\n",
    "\n",
    "val_pred = model.predict(val_p[final_features], num_iteration=model.best_iteration)\n",
    "market_vol = np.nanstd(train_p['forward_returns']) * np.sqrt(252)\n",
    "\n",
    "best_k, best_sharpe = 0.1, -1e9\n",
    "for k in np.linspace(0.01, 5.0, 100):\n",
    "    exposures = np.clip((k * val_pred), 0, 2)\n",
    "    stats = strategy_stats(val_p['forward_returns'], exposures)\n",
    "    if stats['vol'] <= VOL_MULTIPLIER_LIMIT * market_vol and stats['sharpe'] > best_sharpe:\n",
    "        best_k = k\n",
    "        best_sharpe = stats['sharpe']\n",
    "\n",
    "print(f\"Chosen scaling factor k={best_k:.3f} with Sharpe={best_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b6e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis code implements a sophisticated portfolio optimization technique that calibrates \\nthe scaling of model predictions to maximize risk-adjusted returns while controlling \\nportfolio volatility - a critical step in translating machine learning predictions into practical \\ntrading signals.\\n\\nThe strategy_stats function serves as the core evaluation engine for portfolio performance. \\n\\nIt calculates strategy returns by multiplying exposures (position sizes) with actual market returns, \\neffectively simulating the profit and loss of the trading strategy. \\nThe function computes the mean and standard deviation of these strategy returns, \\nthen derives two key metrics: the Sharpe ratio and annualized volatility. \\nThe Sharpe ratio calculation includes a small epsilon value (1e-9) in the denominator \\nto prevent division by zero, and multiplies by the square root of 252 (trading days per year) \\nto annualize the ratio. Similarly, the volatility is annualized by multiplying \\nthe standard deviation by âˆš252, converting daily volatility to annual terms for easier interpretation.\\n\\nThe calibration process begins by generating model predictions on the validation set \\nusing the best iteration from the trained LightGBM model. It then calculates the market's \\nbaseline volatility by taking the standard deviation of training returns and annualizing it. \\nThis baseline serves as a reference point for controlling the strategy's risk exposure. \\nThe algorithm initializes tracking variables for the best scaling factor (best_k) \\nand corresponding Sharpe ratio, starting with conservative values.\\n\\nThe heart of the optimization lies in the systematic search over scaling factors using \\nnp.linspace(0.01, 5.0, 100), which creates 100 evenly spaced values between 0.01 and 5.0. \\nFor each scaling factor k, the code multiplies the model predictions and clips the resulting \\nexposures between 0 and 2, ensuring long-only positions with maximum 200% allocationation. \\nThis clipping prevents excessive leverage while allowing for concentrated positions \\nwhen the model has high confidence.\\n\\nThe selection criteria embodied in the conditional statement represents sophisticated \\nrisk management. The algorithm only considers scaling factors that satisfy two conditions: \\nthe strategy's volatility must not exceed VOL_MULTIPLIER_LIMIT times the market volatility \\n(typically 1.2x, allowing 20% more risk than the market), and the Sharpe ratio must improve \\nupon the current best. This dual constraint ensures that the strategy doesn't take excessive \\nrisk while maximizing risk-adjusted performance. The approach is particularly valuable \\nin financial applications where controlling downside risk is as important as maximizing returns, \\nas it prevents the model from being over-aggressive in its position sizing while still capturing \\nthe predictive signal effectively.\\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code implements a sophisticated portfolio optimization technique that calibrates \n",
    "the scaling of model predictions to maximize risk-adjusted returns while controlling \n",
    "portfolio volatility - a critical step in translating machine learning predictions into practical \n",
    "trading signals.\n",
    "\n",
    "The strategy_stats function serves as the core evaluation engine for portfolio performance. \n",
    "\n",
    "It calculates strategy returns by multiplying exposures (position sizes) with actual market returns, \n",
    "effectively simulating the profit and loss of the trading strategy. \n",
    "The function computes the mean and standard deviation of these strategy returns, \n",
    "then derives two key metrics: the Sharpe ratio and annualized volatility. \n",
    "The Sharpe ratio calculation includes a small epsilon value (1e-9) in the denominator \n",
    "to prevent division by zero, and multiplies by the square root of 252 (trading days per year) \n",
    "to annualize the ratio. Similarly, the volatility is annualized by multiplying \n",
    "the standard deviation by âˆš252, converting daily volatility to annual terms for easier interpretation.\n",
    "\n",
    "The calibration process begins by generating model predictions on the validation set \n",
    "using the best iteration from the trained LightGBM model. It then calculates the market's \n",
    "baseline volatility by taking the standard deviation of training returns and annualizing it. \n",
    "This baseline serves as a reference point for controlling the strategy's risk exposure. \n",
    "The algorithm initializes tracking variables for the best scaling factor (best_k) \n",
    "and corresponding Sharpe ratio, starting with conservative values.\n",
    "\n",
    "The heart of the optimization lies in the systematic search over scaling factors using \n",
    "np.linspace(0.01, 5.0, 100), which creates 100 evenly spaced values between 0.01 and 5.0. \n",
    "For each scaling factor k, the code multiplies the model predictions and clips the resulting \n",
    "exposures between 0 and 2, ensuring long-only positions with maximum 200% allocationation. \n",
    "This clipping prevents excessive leverage while allowing for concentrated positions \n",
    "when the model has high confidence.\n",
    "\n",
    "The selection criteria embodied in the conditional statement represents sophisticated \n",
    "risk management. The algorithm only considers scaling factors that satisfy two conditions: \n",
    "the strategy's volatility must not exceed VOL_MULTIPLIER_LIMIT times the market volatility \n",
    "(typically 1.2x, allowing 20% more risk than the market), and the Sharpe ratio must improve \n",
    "upon the current best. This dual constraint ensures that the strategy doesn't take excessive \n",
    "risk while maximizing risk-adjusted performance. The approach is particularly valuable \n",
    "in financial applications where controlling downside risk is as important as maximizing returns, \n",
    "as it prevents the model from being over-aggressive in its position sizing while still capturing \n",
    "the predictive signal effectively.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_lgb_fixed.csv\n"
     ]
    }
   ],
   "source": [
    "## Test Predictions + Smoothing\n",
    "test_pred = model.predict(test_p[final_features], num_iteration=model.best_iteration)\n",
    "\n",
    "alpha = 0.8\n",
    "smoothed_allocation = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_allocation.append(s)\n",
    "    prev = s\n",
    "smoothed_allocation = np.array(smoothed_allocation)\n",
    "\n",
    "# replace in final submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test_p['date_id'],\n",
    "    'prediction': smoothed_allocation  \n",
    "})\n",
    "submission_df.to_csv(\"submission_lgb_fixed.csv\", index=False)\n",
    "print(\"Saved submission_lgb_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48169788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis code implements the final prediction and submission generation phase, incorporating temporal smoothing to create more stable portfolio allocations \\n- a crucial technique for reducing transaction costs and improving real-world trading performance.\\n\\nThe process begins by generating raw predictions on the test dataset using the trained LightGBM model. \\n\\nThe model.predict() call uses the best_iteration parameter to ensure predictions are made with the optimal number of boosting rounds \\ndetermined during training, preventing overfitting. These raw predictions represent the model\\'s assessment of expected returns \\nfor each date in the test set.\\n\\nThe smoothing mechanism employs an exponential moving average (EMA) with an alpha parameter of 0.8. \\nThis technique addresses a common problem in quantitative trading: raw model predictions often exhibit high volatility that \\nwould result in excessive portfolio turnover if implemented directly. The EMA formula s = alpha * x + (1 - alpha) * prev \\ncreates a weighted average between the current scaled prediction and the previous smoothed allocation. With alpha = 0.8, \\nthe current prediction receives 80% weight while the previous allocation contributes 20%, \\nstriking a balance between responsiveness to new signals and stability.\\n\\nBefore applying the smoothing, each prediction is scaled by the optimal factor best_k (determined during the volatility calibration phase) \\nand clipped between 0 and 2 using np.clip(). This clipping enforces the constraint of long-only positions with maximum 200% allocation, \\npreventing the model from suggesting impossible or overly risky position sizes. The clipping occurs within the loop\\'s iteration variable, \\nensuring that each scaled prediction is properly bounded before entering the smoothing calculation.\\n\\nThe smoothing loop maintains state through the prev variable, which starts at 0.0 and gets updated with each smoothed allocation. \\nThis creates a temporal dependency where each allocation decision considers not just the current model prediction, \\nbut also the recent history of allocations. This approach mimics how professional portfolio managers gradually adjust positions \\nrather than making dramatic changes, reducing market impact and transaction costs.\\n\\nFinally, the code packages the results into a competition-ready submission format. \\nThe smoothed allocations are converted to a NumPy array for consistency, \\nthen combined with the corresponding date identifiers from the test set into a pandas DataFrame. \\nThe submission is saved as \"submission_lgb_fixed.csv\" with index=False to exclude row numbers, \\ncreating a clean two-column format that matches competition requirements. \\nThis systematic approach to prediction post-processing demonstrates sophisticated understanding of the practical challenges \\nin translating machine learning signals into implementable trading strategies.\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code implements the final prediction and submission generation phase, incorporating temporal smoothing to create more stable portfolio allocations \n",
    "- a crucial technique for reducing transaction costs and improving real-world trading performance.\n",
    "\n",
    "The process begins by generating raw predictions on the test dataset using the trained LightGBM model. \n",
    "\n",
    "The model.predict() call uses the best_iteration parameter to ensure predictions are made with the optimal number of boosting rounds \n",
    "determined during training, preventing overfitting. These raw predictions represent the model's assessment of expected returns \n",
    "for each date in the test set.\n",
    "\n",
    "The smoothing mechanism employs an exponential moving average (EMA) with an alpha parameter of 0.8. \n",
    "This technique addresses a common problem in quantitative trading: raw model predictions often exhibit high volatility that \n",
    "would result in excessive portfolio turnover if implemented directly. The EMA formula s = alpha * x + (1 - alpha) * prev \n",
    "creates a weighted average between the current scaled prediction and the previous smoothed allocation. With alpha = 0.8, \n",
    "the current prediction receives 80% weight while the previous allocation contributes 20%, \n",
    "striking a balance between responsiveness to new signals and stability.\n",
    "\n",
    "Before applying the smoothing, each prediction is scaled by the optimal factor best_k (determined during the volatility calibration phase) \n",
    "and clipped between 0 and 2 using np.clip(). This clipping enforces the constraint of long-only positions with maximum 200% allocation, \n",
    "preventing the model from suggesting impossible or overly risky position sizes. The clipping occurs within the loop's iteration variable, \n",
    "ensuring that each scaled prediction is properly bounded before entering the smoothing calculation.\n",
    "\n",
    "The smoothing loop maintains state through the prev variable, which starts at 0.0 and gets updated with each smoothed allocation. \n",
    "This creates a temporal dependency where each allocation decision considers not just the current model prediction, \n",
    "but also the recent history of allocations. This approach mimics how professional portfolio managers gradually adjust positions \n",
    "rather than making dramatic changes, reducing market impact and transaction costs.\n",
    "\n",
    "Finally, the code packages the results into a competition-ready submission format. \n",
    "The smoothed allocations are converted to a NumPy array for consistency, \n",
    "then combined with the corresponding date identifiers from the test set into a pandas DataFrame. \n",
    "The submission is saved as \"submission_lgb_fixed.csv\" with index=False to exclude row numbers, \n",
    "creating a clean two-column format that matches competition requirements. \n",
    "This systematic approach to prediction post-processing demonstrates sophisticated understanding of the practical challenges \n",
    "in translating machine learning signals into implementable trading strategies.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb838fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKaggle Evaluation Metric:\\n\\nstrategy_returns = risk_free_rate * (1 - position) + position * forward_returns\\n\\nif position = 0 â†’ invest in risk-free asset,\\n\\nif position = 1 â†’ invest like the market,\\n\\nif position = 2 â†’ you are leveraged Ã—2 on the market.\\n\\n\\ndef score():\\n\\nstrategy_returns = rf * (1 - pos) + pos * fwd_returns\\n\\nIn the code, the calibration seeks the best Sharpe of the portfolio exposed to pos by calculating:\\n\\nstrat = exposures * returns\\n'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kaggle Evaluation Metric:\n",
    "\n",
    "strategy_returns = risk_free_rate * (1 - position) + position * forward_returns\n",
    "\n",
    "if position = 0 â†’ invest in risk-free asset,\n",
    "\n",
    "if position = 1 â†’ invest like the market,\n",
    "\n",
    "if position = 2 â†’ you are leveraged Ã—2 on the market.\n",
    "\n",
    "\n",
    "def score():\n",
    "\n",
    "strategy_returns = rf * (1 - pos) + pos * fwd_returns\n",
    "\n",
    "In the code, the calibration seeks the best Sharpe of the portfolio exposed to pos by calculating:\n",
    "\n",
    "strat = exposures * returns\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Kaggle Inference Server Wrapper\n",
    "\n",
    "# _model = model\n",
    "# _best_k = best_k\n",
    "# _history_returns = list(train_p['forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "# def predict(pl_df: pl.DataFrame) -> float:\n",
    "#     global _history_returns\n",
    "#     pdf = pl_df.to_pandas()\n",
    "#     pdf_p = prepare_df(pdf, median_map, feature_cols)\n",
    "#     for f in final_features:\n",
    "#         if f not in pdf_p.columns:\n",
    "#             pdf_p[f] = 0.0\n",
    "#     x = pdf_p[final_features].to_numpy()\n",
    "#     pred = _model.predict(x, num_iteration=_model.best_iteration)[0]\n",
    "#     vol_est = np.std(_history_returns) or 1e-3\n",
    "#     allocation = float(np.clip((_best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "#     if 'lagged_forward_returns' in pl_df.columns:\n",
    "#         try:\n",
    "#             _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "#         except:\n",
    "#             _history_returns.append(0.0)\n",
    "#     _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "#     return allocation\n",
    "\n",
    "# inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     inference_server.serve()\n",
    "# else:\n",
    "#     inference_server.run_local_gateway((str(DATA_DIR),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fdfba",
   "metadata": {},
   "source": [
    "### Notebook Summary\n",
    "\n",
    "| Feature        | Description                          |\n",
    "|----------------|--------------------------------------|\n",
    "| Model          | LightGBM (fast, robust)              |\n",
    "| Validation     | Time-based (last 2700 days)          |\n",
    "| Imputation     | Median + missing flags               |\n",
    "| Signal control | Volatility scaling (Sharpe-based)    |\n",
    "| Inference      | Kaggle-compatible `predict` function |\n",
    "| Runtime        | \\< 5 minutes on Kaggle GPU notebook  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcbe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115cdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
