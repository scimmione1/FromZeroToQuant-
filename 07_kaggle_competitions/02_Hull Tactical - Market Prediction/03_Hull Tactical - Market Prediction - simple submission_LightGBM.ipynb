{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d207142",
   "metadata": {},
   "source": [
    "# Simple Baseline (LightGBM Version) â€” Hull Tactical Market Prediction\n",
    "\n",
    "This notebook provides a **safe, fast and competition-compliant\n",
    "baseline** for the Hull Tactical Market Prediction challenge on Kaggle.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "-   LightGBM regression model (fast and stable)\n",
    "-   Chronological split (no data leakage)\n",
    "-   Median imputation + missing indicators\n",
    "-   Volatility-based signal scaling\n",
    "-   Kaggle-compatible inference server wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d70b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "# import os\n",
    "from pathlib import Path\n",
    "# import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d795b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8990, 98), (10, 99))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Configuration and Data Loading (kaggle_evaluation only)\n",
    "# import kaggle_evaluation.default_inference_server as kdeval\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading (local version only)\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "# Read CSV files from data_path\n",
    "TRAIN_PATH = DATA_DIR / 'train.csv'\n",
    "TEST_PATH  = DATA_DIR / 'test.csv'\n",
    "\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "RANDOM_SEED = 42\n",
    "VOL_MULTIPLIER_LIMIT = 1.2\n",
    "VOL_WINDOW = 20\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "test_raw  = pd.read_csv(TEST_PATH)\n",
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f378b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Preparation\n",
    "excluded = {'date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns'}\n",
    "feature_cols = [c for c in train_raw.columns if c not in excluded]\n",
    "feature_cols = [c for c in feature_cols if c in test_raw.columns]\n",
    "\n",
    "\"\"\"\n",
    "The third line performs a crucial validation step by ensuring feature consistency between training and test datasets. \n",
    "It filters the feature list to include only columns that exist in both the training data and the test data \n",
    "(test_raw.columns). This step is essential because machine learning models require identical feature structures \n",
    "during training and prediction phases. If a feature exists in training data but not in test data, \n",
    "the model would fail during inference. \n",
    "This defensive programming approach prevents runtime errors and ensures that the model can successfully \n",
    "make predictions on the test set.\n",
    "\n",
    "This two-step filtering process - first removing inappropriate columns, \n",
    "then ensuring train-test consistency - represents a best practice in machine learning pipelines. \n",
    "It creates a robust feature set that avoids data leakage while maintaining compatibility across different data splits, \n",
    "which is particularly important in time-series financial prediction tasks where the test set represents \n",
    "future market conditions.\n",
    "\"\"\"\n",
    "\n",
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in feature_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "            df[f'{c}_was_na'] = 1\n",
    "            continue\n",
    "        if df[c].dtype.kind in 'fiu':\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "            med = median_map.get(c, 0.0)\n",
    "            was_na = df[c].isna().astype(int)\n",
    "            df[c] = df[c].fillna(med)\n",
    "            df[f'{c}_was_na'] = was_na\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf47989",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "\n",
    "train_df, val_df = time_split_train_val(train_raw, val_size=VALIDATION_SIZE)\n",
    "\n",
    "median_map = {c: float(train_df[c].median(skipna=True)) if train_df[c].dtype.kind in 'fiu' else 0.0 \n",
    "              for c in feature_cols}\n",
    "\n",
    "train_p = prepare_df(train_df, median_map, feature_cols)\n",
    "val_p   = prepare_df(val_df, median_map, feature_cols)\n",
    "test_p  = prepare_df(test_raw, median_map, feature_cols)\n",
    "\n",
    "final_features = [f for c in feature_cols for f in (c, f\"{c}_was_na\")]\n",
    "print(\"Number of features:\", len(final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LightGBM Training\n",
    "\n",
    "train_data = lgb.Dataset(train_p[final_features], label=train_p['forward_returns'])\n",
    "val_data   = lgb.Dataset(val_p[final_features], label=val_p['forward_returns'])\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 63,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'seed': RANDOM_SEED,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[val_data],\n",
    "    num_boost_round=2000,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b02fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Volatility Scaling Calibration\n",
    "\n",
    "def strategy_stats(returns, exposures):\n",
    "    strat = exposures * returns\n",
    "    mean = np.nanmean(strat)\n",
    "    std  = np.nanstd(strat)\n",
    "    sharpe = (mean / (std + 1e-9)) * np.sqrt(252)\n",
    "    vol = std * np.sqrt(252)\n",
    "    return {'sharpe': sharpe, 'vol': vol}\n",
    "\n",
    "val_pred = model.predict(val_p[final_features], num_iteration=model.best_iteration)\n",
    "market_vol = np.nanstd(train_p['forward_returns']) * np.sqrt(252)\n",
    "\n",
    "best_k, best_sharpe = 0.1, -1e9\n",
    "for k in np.linspace(0.01, 5.0, 100):\n",
    "    exposures = np.clip((k * val_pred), 0, 2)\n",
    "    stats = strategy_stats(val_p['forward_returns'], exposures)\n",
    "    if stats['vol'] <= VOL_MULTIPLIER_LIMIT * market_vol and stats['sharpe'] > best_sharpe:\n",
    "        best_k = k\n",
    "        best_sharpe = stats['sharpe']\n",
    "\n",
    "print(f\"Chosen scaling factor k={best_k:.3f} with Sharpe={best_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262718",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Predictions + Smoothing\n",
    "\n",
    "test_pred = model.predict(test_p[final_features], num_iteration=model.best_iteration)\n",
    "\n",
    "alpha = 0.8\n",
    "smoothed_alloc = []\n",
    "prev = 0.0\n",
    "for x in np.clip(best_k * test_pred, 0, 2):\n",
    "    s = alpha * x + (1 - alpha) * prev\n",
    "    smoothed_alloc.append(s)\n",
    "    prev = s\n",
    "smoothed_alloc = np.array(smoothed_alloc)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'date_id': test_p['date_id'],\n",
    "    'weight': smoothed_alloc\n",
    "})\n",
    "submission_df.to_csv(\"submission_lgb_fixed.csv\", index=False)\n",
    "print(\"Saved submission_lgb_fixed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kaggle Inference Server Wrapper\n",
    "\n",
    "_model = model\n",
    "_best_k = best_k\n",
    "_history_returns = list(train_p['forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "def predict(pl_df: pl.DataFrame) -> float:\n",
    "    global _history_returns\n",
    "    pdf = pl_df.to_pandas()\n",
    "    pdf_p = prepare_df(pdf, median_map, feature_cols)\n",
    "    for f in final_features:\n",
    "        if f not in pdf_p.columns:\n",
    "            pdf_p[f] = 0.0\n",
    "    x = pdf_p[final_features].to_numpy()\n",
    "    pred = _model.predict(x, num_iteration=_model.best_iteration)[0]\n",
    "    vol_est = np.std(_history_returns) or 1e-3\n",
    "    alloc = float(np.clip((_best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "    if 'lagged_forward_returns' in pl_df.columns:\n",
    "        try:\n",
    "            _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "        except:\n",
    "            _history_returns.append(0.0)\n",
    "    _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "    return alloc\n",
    "\n",
    "server = kdeval.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    server.serve()\n",
    "else:\n",
    "    server.run_local_gateway((str(DATA_DIR),))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243fdfba",
   "metadata": {},
   "source": [
    "### Notebook Summary\n",
    "\n",
    "| Feature        | Description                          |\n",
    "|----------------|--------------------------------------|\n",
    "| Model          | LightGBM (fast, robust)              |\n",
    "| Validation     | Time-based (last 2700 days)          |\n",
    "| Imputation     | Median + missing flags               |\n",
    "| Signal control | Volatility scaling (Sharpe-based)    |\n",
    "| Inference      | Kaggle-compatible `predict` function |\n",
    "| Runtime        | \\< 5 minutes on Kaggle GPU notebook  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d9153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcbe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115cdd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
