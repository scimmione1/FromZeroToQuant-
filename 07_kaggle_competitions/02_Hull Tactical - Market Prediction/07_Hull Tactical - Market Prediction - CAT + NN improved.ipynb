{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7b72ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment - kaggle_evaluation not available\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ðŸ§  HULL TACTICAL MARKET PREDICTION â€” ENSEMBLE + SHARPEPENALTY\n",
    "# ================================================================\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use only first GPU if multiple\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import polars as pl\n",
    "from typing import Dict \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import TensorFlow after setting environment variables\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # Only show errors\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Try to import kaggle_evaluation, handle if not available\n",
    "try:\n",
    "    import kaggle_evaluation.default_inference_server as kdeval\n",
    "    KAGGLE_ENV = True\n",
    "    print(\"Running in Kaggle competition environment\")\n",
    "except ImportError:\n",
    "    KAGGLE_ENV = False\n",
    "    print(\"Running in local environment - kaggle_evaluation not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c00d6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from local environment\n",
      "Data loaded successfully\n",
      "Train shape: (8990, 98) | Test shape: (10, 99)\n",
      "Base features available: 94\n",
      "Target variable: market_forward_excess_returns\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Data Loading & Initial Feature Preparation\n",
    "# ================================================================\n",
    "\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "\n",
    "## Configuration and Data Loading\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "TARGET = \"market_forward_excess_returns\"\n",
    "drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\"]\n",
    "VOL_WINDOW = 20        # volatility window in days\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    \"\"\"Split data chronologically for time series validation.\"\"\"\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "# Load train/test data using the KAGGLE_ENV variable from cell 1\n",
    "if KAGGLE_ENV:\n",
    "    print(\"Loading data from Kaggle environment\")\n",
    "    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "    test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "else:\n",
    "    print(\"Loading data from local environment\")\n",
    "    # Try different possible local paths\n",
    "    local_paths = [\n",
    "        DATA_DIR / \"train.csv\",\n",
    "        Path(\"01_data/train.csv\"),\n",
    "        Path(\"train.csv\")\n",
    "    ]\n",
    "    \n",
    "    train_path = None\n",
    "    test_path = None\n",
    "    \n",
    "    for path in local_paths:\n",
    "        if path.exists():\n",
    "            train_path = path\n",
    "            test_path = path.parent / \"test.csv\"\n",
    "            break\n",
    "    \n",
    "    if train_path is None or not test_path.exists():\n",
    "        raise FileNotFoundError(\"âŒ Could not find train.csv and test.csv files in expected locations\")\n",
    "    \n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "train = train.sort_values(\"date_id\").reset_index(drop=True)\n",
    "test = test.sort_values(\"date_id\").reset_index(drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "train = train.fillna(0.0)\n",
    "test = test.fillna(0.0)\n",
    "\n",
    "# Base features (before advanced transformations)\n",
    "base_features = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "\n",
    "print(f\"Base features available: {len(base_features)}\")\n",
    "print(f\"Target variable: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39e94f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and prepare DataFrame by handling missing values intelligently.\n",
    "    \n",
    "    Strategy:\n",
    "    - Use median imputation for numeric columns with some missing values\n",
    "    - Use zero-fill for columns with very few missing values  \n",
    "    - Only process existing columns (no synthetic data creation)\n",
    "\n",
    "    Args:\n",
    "    df: Input DataFrame\n",
    "    median_map: Dictionary mapping column names to median values\n",
    "    feature_cols: List of feature column names to process\n",
    "\n",
    "    Returns:\n",
    "    Cleaned DataFrame\n",
    "\n",
    "    Median is much less sensitive to extreme values (outliers)\n",
    "    Mean can be heavily skewed by a few very large or very small values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Only work with columns that actually exist in the DataFrame\n",
    "    existing_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    if not existing_cols:\n",
    "        print(\"Warning: No feature columns found in DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    # Calculate missing percentages for existing columns\n",
    "    missing_pct = (df[existing_cols].isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # Categorize columns by missing percentage\n",
    "    cols_fill_median = missing_pct[(missing_pct > 5) & (missing_pct <= 50)].index.tolist()\n",
    "    cols_fill_zero = missing_pct[missing_pct <= 5].index.tolist()\n",
    "    \n",
    "    # Apply median imputation for moderately missing columns\n",
    "    if cols_fill_median:\n",
    "        for col in cols_fill_median:\n",
    "            median_val = median_map.get(col, df[col].median())\n",
    "            if pd.isna(median_val):  # Handle case where median is NaN\n",
    "                median_val = 0.0\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Apply zero-fill for low missing columns\n",
    "    if cols_fill_zero:\n",
    "        df[cols_fill_zero] = df[cols_fill_zero].fillna(0)\n",
    "    \n",
    "    # Ensure all feature columns are numeric\n",
    "    for col in existing_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    # Final cleanup - ensure no inf values\n",
    "    df[existing_cols] = df[existing_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6cf946a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: Train 6290 | Validation 2700 rows\n",
      "Preprocessing complete\n",
      "Number of base features: 94\n",
      "Base features available: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1']...\n",
      "Target variable 'market_forward_excess_returns' extracted separately\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train, val_size=VALIDATION_SIZE)\n",
    "print(f\"Data split: Train {train_df.shape[0]} | Validation {val_df.shape[0]} rows\")\n",
    "\n",
    "# Create median map from training portion only\n",
    "median_map = {}\n",
    "for c in base_features:\n",
    "    if c in train_df.columns:\n",
    "        if train_df[c].dtype.kind in 'fiu':  # numeric types\n",
    "            median_val = train_df[c].median(skipna=True)\n",
    "            median_map[c] = float(median_val) if not pd.isna(median_val) else 0.0\n",
    "        else:\n",
    "            median_map[c] = 0.0\n",
    "    else:\n",
    "        median_map[c] = 0.0\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "train_full = prepare_df(train_df, median_map, base_features)\n",
    "val_full   = prepare_df(val_df, median_map, base_features)\n",
    "test_full  = prepare_df(test, median_map, base_features)\n",
    "\n",
    "# Extract only the base features (remove drop_cols and target)\n",
    "final_features = [c for c in base_features if c in train_full.columns]\n",
    "train_p = train_full[final_features].copy()\n",
    "val_p   = val_full[final_features].copy()\n",
    "test_p  = test_full[final_features].copy()\n",
    "\n",
    "# Keep target and other columns separate for later use\n",
    "train_target = train_full[TARGET].copy()\n",
    "val_target   = val_full[TARGET].copy()\n",
    "\n",
    "# Validation check\n",
    "if not final_features:\n",
    "    raise ValueError(\"No features available after preprocessing!\")\n",
    "\n",
    "print(f\"Preprocessing complete\")\n",
    "print(f\"Number of base features: {len(final_features)}\")\n",
    "print(f\"Base features available: {final_features[:10]}...\" if len(final_features) > 10 else f\"Features: {final_features}\")\n",
    "\n",
    "print(f\"Target variable '{TARGET}' extracted separately\")\n",
    "\n",
    "# For feature engineering, we need to combine features with target\n",
    "train_for_engineering = train_p.copy()\n",
    "train_for_engineering[TARGET] = train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "44f708af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10',\n",
       "       'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2',\n",
       "       'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4',\n",
       "       'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14',\n",
       "       'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8',\n",
       "       'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6',\n",
       "       'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5',\n",
       "       'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3',\n",
       "       'V4', 'V5', 'V6', 'V7', 'V8', 'V9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "00ab9852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10',\n",
       "       'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2',\n",
       "       'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4',\n",
       "       'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14',\n",
       "       'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8',\n",
       "       'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6',\n",
       "       'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5',\n",
       "       'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3',\n",
       "       'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'market_forward_excess_returns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_for_engineering.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7dc1c038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'E1',\n",
       " 'E10',\n",
       " 'E11',\n",
       " 'E12',\n",
       " 'E13',\n",
       " 'E14',\n",
       " 'E15',\n",
       " 'E16',\n",
       " 'E17',\n",
       " 'E18',\n",
       " 'E19',\n",
       " 'E2',\n",
       " 'E20',\n",
       " 'E3',\n",
       " 'E4',\n",
       " 'E5',\n",
       " 'E6',\n",
       " 'E7',\n",
       " 'E8',\n",
       " 'E9',\n",
       " 'I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'M1',\n",
       " 'M10',\n",
       " 'M11',\n",
       " 'M12',\n",
       " 'M13',\n",
       " 'M14',\n",
       " 'M15',\n",
       " 'M16',\n",
       " 'M17',\n",
       " 'M18',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'P1',\n",
       " 'P10',\n",
       " 'P11',\n",
       " 'P12',\n",
       " 'P13',\n",
       " 'P2',\n",
       " 'P3',\n",
       " 'P4',\n",
       " 'P5',\n",
       " 'P6',\n",
       " 'P7',\n",
       " 'P8',\n",
       " 'P9',\n",
       " 'S1',\n",
       " 'S10',\n",
       " 'S11',\n",
       " 'S12',\n",
       " 'S2',\n",
       " 'S3',\n",
       " 'S4',\n",
       " 'S5',\n",
       " 'S6',\n",
       " 'S7',\n",
       " 'S8',\n",
       " 'S9',\n",
       " 'V1',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e0529784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of final features = len(final_features)\n",
    "final_selected_features = [c for c in final_features if c in train_p.columns]\n",
    "len(final_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fbd710c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ðŸ”§ Advanced Feature Factory (Enhanced) =====\n",
    "def create_advanced_features(df,\n",
    "                             top_features,\n",
    "                             macro_prefixes=('mom','m','v','p','s'),\n",
    "                             window_sizes=(5,10,20,60,120),\n",
    "                             max_features_to_keep=50,\n",
    "                             shift=1,  # Added shift parameter\n",
    "                             inplace=False):\n",
    "    \"\"\"\n",
    "    Create advanced features following a two-level approach:\n",
    "      1) Lightweight Core Features (applied to `top_features`)\n",
    "      2) Macro-Context Features (applied to columns starting with macro_prefixes)\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        top_features: List of most important features for Level 1 processing\n",
    "        macro_prefixes: Tuple of prefixes for Level 2 features\n",
    "        window_sizes: Rolling window sizes\n",
    "        max_features_to_keep: Maximum features to select\n",
    "        shift: Number of periods to shift for avoiding data leakage\n",
    "        inplace: Whether to modify DataFrame in place\n",
    "    \n",
    "    Returns:\n",
    "        df_out: DataFrame with new features (and original columns)\n",
    "        selected_features: list of top selected feature column names\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # Ensure datetime-like ordering by date_id if present\n",
    "    if 'date_id' in df.columns:\n",
    "        df = df.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "    # Helper: ensure numeric dtype for selected cols\n",
    "    def _to_numeric(cols):\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # ------------- Level 1: Core Features (top_features) -------------\n",
    "    def create_rolling_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling statistics with proper shift to avoid leakage\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll = shifted_col.rolling(window=w, min_periods=1)\n",
    "                \n",
    "                df[f\"{c}_mean_{w}\"] = roll.mean().astype('float32')\n",
    "                df[f\"{c}_std_{w}\"] = roll.std().astype('float32').fillna(0.0)\n",
    "                df[f\"{c}_median_{w}\"] = roll.median().astype('float32')\n",
    "\n",
    "    def create_zscore_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling z-scores with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_mean = shifted_col.rolling(window=w, min_periods=1).mean()\n",
    "                roll_std = shifted_col.rolling(window=w, min_periods=1).std().fillna(0.0)\n",
    "                \n",
    "                df[f\"{c}_z_{w}\"] = ((df[c] - roll_mean) / (roll_std + 1e-9)).astype('float32')\n",
    "\n",
    "    def create_spread_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create spread and percentage change features\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            # Use proper shift for difference calculations\n",
    "            df[f\"{c}_diff_1\"] = (df[c] - df[c].shift(shift_periods)).astype('float32')\n",
    "            df[f\"{c}_pctchg_1\"] = (df[c].pct_change(periods=shift_periods).fillna(0.0)).astype('float32')\n",
    "\n",
    "    # ------------- Level 2: Macro Features (selective) -------------\n",
    "    def create_cumulative_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create cumulative sums with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                df[f\"{c}_cumsum_{w}\"] = shifted_col.rolling(window=w, min_periods=1).sum().astype('float32')\n",
    "\n",
    "    def create_correlation_features(pairs=None, window=30, shift_periods=shift):\n",
    "        \"\"\"Create rolling correlations with proper shift\"\"\"\n",
    "        if pairs is None:\n",
    "            # Build pairs from top_features (limit to avoid explosion)\n",
    "            cand = []\n",
    "            for i in range(len(top_features)):\n",
    "                for j in range(i+1, len(top_features)):\n",
    "                    cand.append((top_features[i], top_features[j]))\n",
    "            pairs = cand[:10]  # Limit to 10 pairs\n",
    "        \n",
    "        for a, b in pairs:\n",
    "            if a not in df.columns or b not in df.columns:\n",
    "                continue\n",
    "            # Apply shift to both series\n",
    "            a_shifted = df[a].shift(shift_periods)\n",
    "            b_shifted = df[b].shift(shift_periods)\n",
    "            corr = a_shifted.rolling(window=window, min_periods=1).corr(b_shifted)\n",
    "            df[f\"corr_{a}_{b}_{window}\"] = corr.astype('float32').fillna(0.0)\n",
    "\n",
    "    def create_volatility_features(cols=None, windows=(20,60), shift_periods=shift):\n",
    "        \"\"\"Create volatility spread features with proper shift\"\"\"\n",
    "        if cols is None:\n",
    "            cols = [c for c in df.columns if c.startswith('v')]\n",
    "        \n",
    "        # Limit to prevent feature explosion\n",
    "        cols = cols[:8]\n",
    "        \n",
    "        for w in windows:\n",
    "            vols = {}\n",
    "            for c in cols:\n",
    "                if c in df.columns:\n",
    "                    shifted_col = df[c].shift(shift_periods)\n",
    "                    vols[c] = shifted_col.rolling(window=w, min_periods=1).std().astype('float32').fillna(0.0)\n",
    "            \n",
    "            # Create spread between consecutive volatilities\n",
    "            vol_keys = list(vols.keys())\n",
    "            for i in range(len(vol_keys) - 1):\n",
    "                a, b = vol_keys[i], vol_keys[i + 1]\n",
    "                df[f\"volspread_{a}_{b}_{w}\"] = (vols[a] - vols[b]).astype('float32')\n",
    "\n",
    "    def create_extremes_features(cols, windows=(20,60,120), shift_periods=shift):\n",
    "        \"\"\"Create high/low ratio features with proper shift\"\"\"\n",
    "        # Limit columns to prevent explosion\n",
    "        cols = [c for c in cols if c in df.columns][:10]\n",
    "        \n",
    "        for c in cols:\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_max = shifted_col.rolling(window=w, min_periods=1).max()\n",
    "                roll_min = shifted_col.rolling(window=w, min_periods=1).min()\n",
    "                \n",
    "                df[f\"{c}_high_ratio_{w}\"] = (df[c] / (roll_max + 1e-9)).astype('float32')\n",
    "                df[f\"{c}_low_ratio_{w}\"] = (df[c] / (roll_min + 1e-9)).astype('float32')\n",
    "\n",
    "    # Execute feature creation\n",
    "    print(\"ðŸ”§ Creating Level 1 features (Core)...\")\n",
    "    _to_numeric(top_features)\n",
    "    create_rolling_features(top_features)\n",
    "    create_zscore_features(top_features)\n",
    "    create_spread_features(top_features)\n",
    "\n",
    "    print(\"ðŸ”§ Creating Level 2 features (Macro)...\")\n",
    "    macro_cols = [c for c in df.columns if any(c.startswith(pref) for pref in macro_prefixes)]\n",
    "    _to_numeric(macro_cols)\n",
    "    \n",
    "    create_cumulative_features(macro_cols, windows=(5,10,20))\n",
    "    create_correlation_features(window=30)\n",
    "    create_volatility_features(windows=(20,60))\n",
    "    create_extremes_features([c for c in df.columns if c.startswith(('m','p'))], windows=(20,60,120))\n",
    "\n",
    "    # Clean data\n",
    "    print(\"ðŸ§¹ Cleaning and selecting features...\")\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0.0, inplace=True)\n",
    "\n",
    "    # Downcast to save memory\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    if len(float_cols) > 0:\n",
    "        df[float_cols] = df[float_cols].astype('float32')\n",
    "\n",
    "    # Feature selection\n",
    "    def select_top_features(df_in, target_col='market_forward_excess_returns', top_k=max_features_to_keep):\n",
    "        \"\"\"Select top features using tree-based importance\"\"\"\n",
    "        X_full = df_in.drop(columns=[target_col]) if target_col in df_in.columns else df_in.copy()\n",
    "        X_num = X_full.select_dtypes(include=[np.number]).copy()\n",
    "        \n",
    "        # Remove zero variance features\n",
    "        vt = VarianceThreshold(threshold=1e-6)\n",
    "        try:\n",
    "            vt.fit(X_num)\n",
    "            cols_kept = X_num.columns[vt.get_support()].tolist()\n",
    "        except Exception:\n",
    "            cols_kept = X_num.columns.tolist()\n",
    "\n",
    "        X_sel = X_num[cols_kept].copy()\n",
    "        y_sel = df_in[target_col].values if target_col in df_in.columns else None\n",
    "\n",
    "        # Use tree-based importance if target available\n",
    "        if y_sel is not None and len(y_sel) == len(X_sel) and len(np.unique(y_sel)) > 1:\n",
    "            try:\n",
    "                gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "                gb.fit(X_sel, y_sel)\n",
    "                imp = pd.Series(gb.feature_importances_, index=X_sel.columns).sort_values(ascending=False)\n",
    "                top_feats = imp.head(top_k).index.tolist()\n",
    "            except Exception:\n",
    "                # Fallback to variance-based selection\n",
    "                var_series = X_sel.var().sort_values(ascending=False)\n",
    "                top_feats = var_series.head(top_k).index.tolist()\n",
    "        else:\n",
    "            # Fallback to variance-based selection\n",
    "            var_series = X_sel.var().sort_values(ascending=False)\n",
    "            top_feats = var_series.head(top_k).index.tolist()\n",
    "\n",
    "        return top_feats\n",
    "\n",
    "    selected = select_top_features(df, target_col='market_forward_excess_returns', top_k=max_features_to_keep)\n",
    "\n",
    "    # Final DataFrame\n",
    "    keep_cols = ['date_id', 'forward_returns', 'risk_free_rate', 'market_forward_excess_returns']\n",
    "    keep_cols = [c for c in keep_cols if c in df.columns] + selected\n",
    "    df_out = df[keep_cols].copy()\n",
    "\n",
    "    print(f\"âœ… Feature engineering complete. Selected {len(selected)} features.\")\n",
    "    return df_out, selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "417e7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cancel when the modifications in the next cell are solved\n",
    "\n",
    "# # ===== Feature Engineering & Data Preparation =====\n",
    "\n",
    "# # Top features identified (Level 1 - Core)\n",
    "# top_features = ['m4', 'v13', 'm11', 's2', 'd4', 'd1', 'd2', 'e8', 'p6', 'm2', \n",
    "#                 'd8', 'm9', 'p8', 'p7', 's12', 'p13', 'v9', 'd5', 'p1', 's8']\n",
    "\n",
    "# print(\"ðŸ”§ Creating advanced features for training data...\")\n",
    "# # Create engineered features and select the top ones\n",
    "# train_enh, selected_features = create_advanced_features(\n",
    "#     train,\n",
    "#     top_features=top_features,\n",
    "#     window_sizes=(5, 10, 20, 60, 120),   # 1w, 2w, 1m, 3m, 6m\n",
    "#     max_features_to_keep=50,\n",
    "#     shift=1  # Explicitly set shift to avoid data leakage\n",
    "# )\n",
    "\n",
    "# print(\"ðŸ”§ Applying same transformation to test data...\")\n",
    "# # Apply the same transformation to test set (use same selected features)\n",
    "# test_enh, _ = create_advanced_features(\n",
    "#     test,\n",
    "#     top_features=top_features,\n",
    "#     window_sizes=(5, 10, 20, 60, 120),\n",
    "#     max_features_to_keep=50,\n",
    "#     shift=1\n",
    "# )\n",
    "\n",
    "# # Ensure test set has same features as training set\n",
    "# missing_features = [f for f in selected_features if f not in test_enh.columns]\n",
    "# if missing_features:\n",
    "#     print(f\"Adding missing features to test set: {len(missing_features)}\")\n",
    "#     for feature in missing_features:\n",
    "#         test_enh[feature] = 0.0  # Fill missing features with 0\n",
    "\n",
    "# # Final feature matrices\n",
    "# X = train_enh[selected_features].astype('float32')\n",
    "# y = train_enh[TARGET].astype('float32')\n",
    "# X_test = test_enh[selected_features].astype('float32')\n",
    "\n",
    "# print(f\"\\nData preparation complete!\")\n",
    "# print(f\"Final selected features: {len(selected_features)}\")\n",
    "# print(f\"Train shape: {X.shape}\")\n",
    "# print(f\"Test shape: {X_test.shape}\")\n",
    "# print(f\"Target shape: {y.shape}\")\n",
    "# print(f\"Selected features preview: {selected_features[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d2ae4657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features for training data only...\n",
      "ðŸ”§ Creating Level 1 features (Core)...\n",
      "ðŸ”§ Creating Level 2 features (Macro)...\n",
      "ðŸ§¹ Cleaning and selecting features...\n",
      "âœ… Feature engineering complete. Selected 50 features.\n",
      "\n",
      "Feature Engineering Results:\n",
      "Original base features available: 94\n",
      "Original features selected: 30\n",
      "New engineered features created: 20\n",
      "Total features for modeling: 50\n",
      "\n",
      "New engineered features added:\n",
      " 1. market_forward_excess_returns_low_ratio_120\n",
      " 2. market_forward_excess_returns_high_ratio_120\n",
      " 3. market_forward_excess_returns_low_ratio_20\n",
      " 4. market_forward_excess_returns_low_ratio_60\n",
      " 5. market_forward_excess_returns_high_ratio_20\n",
      " 6. market_forward_excess_returns_high_ratio_60\n",
      " 7. market_forward_excess_returns_cumsum_20\n",
      " 8. market_forward_excess_returns_cumsum_20_high_ratio_20\n",
      " 9. market_forward_excess_returns_cumsum_5_high_ratio_20\n",
      "10. market_forward_excess_returns_cumsum_5\n",
      "11. market_forward_excess_returns_cumsum_5_low_ratio_20\n",
      "12. market_forward_excess_returns_cumsum_10_high_ratio_20\n",
      "13. market_forward_excess_returns_cumsum_20_high_ratio_120\n",
      "14. market_forward_excess_returns_cumsum_5_high_ratio_60\n",
      "15. market_forward_excess_returns_cumsum_10\n",
      "16. market_forward_excess_returns_cumsum_20_low_ratio_120\n",
      "17. market_forward_excess_returns_cumsum_10_high_ratio_120\n",
      "18. market_forward_excess_returns_cumsum_5_low_ratio_120\n",
      "19. market_forward_excess_returns_cumsum_20_high_ratio_60\n",
      "20. market_forward_excess_returns_cumsum_10_low_ratio_20\n",
      "\n",
      "All 50 selected features:\n",
      " 1. market_forward_excess_returns_low_ratio_120 [ENGINEERED]\n",
      " 2. market_forward_excess_returns_high_ratio_120 [ENGINEERED]\n",
      " 3. market_forward_excess_returns_low_ratio_20 [ENGINEERED]\n",
      " 4. V7                        [ORIGINAL]\n",
      " 5. M18                       [ORIGINAL]\n",
      " 6. market_forward_excess_returns_low_ratio_60 [ENGINEERED]\n",
      " 7. V9                        [ORIGINAL]\n",
      " 8. P10                       [ORIGINAL]\n",
      " 9. E8                        [ORIGINAL]\n",
      "10. M9                        [ORIGINAL]\n",
      "11. M5                        [ORIGINAL]\n",
      "12. market_forward_excess_returns_high_ratio_20 [ENGINEERED]\n",
      "13. E19                       [ORIGINAL]\n",
      "14. P8                        [ORIGINAL]\n",
      "15. M17                       [ORIGINAL]\n",
      "16. E12                       [ORIGINAL]\n",
      "17. P11                       [ORIGINAL]\n",
      "18. market_forward_excess_returns_high_ratio_60 [ENGINEERED]\n",
      "19. P9                        [ORIGINAL]\n",
      "20. V8                        [ORIGINAL]\n",
      "21. E3                        [ORIGINAL]\n",
      "22. M12                       [ORIGINAL]\n",
      "23. market_forward_excess_returns_cumsum_20 [ENGINEERED]\n",
      "24. S1                        [ORIGINAL]\n",
      "25. M3                        [ORIGINAL]\n",
      "26. market_forward_excess_returns_cumsum_20_high_ratio_20 [ENGINEERED]\n",
      "27. M15                       [ORIGINAL]\n",
      "28. E2                        [ORIGINAL]\n",
      "29. market_forward_excess_returns_cumsum_5_high_ratio_20 [ENGINEERED]\n",
      "30. market_forward_excess_returns_cumsum_5 [ENGINEERED]\n",
      "31. M8                        [ORIGINAL]\n",
      "32. M10                       [ORIGINAL]\n",
      "33. E17                       [ORIGINAL]\n",
      "34. market_forward_excess_returns_cumsum_5_low_ratio_20 [ENGINEERED]\n",
      "35. E16                       [ORIGINAL]\n",
      "36. market_forward_excess_returns_cumsum_10_high_ratio_20 [ENGINEERED]\n",
      "37. V12                       [ORIGINAL]\n",
      "38. market_forward_excess_returns_cumsum_20_high_ratio_120 [ENGINEERED]\n",
      "39. market_forward_excess_returns_cumsum_5_high_ratio_60 [ENGINEERED]\n",
      "40. market_forward_excess_returns_cumsum_10 [ENGINEERED]\n",
      "41. market_forward_excess_returns_cumsum_20_low_ratio_120 [ENGINEERED]\n",
      "42. V6                        [ORIGINAL]\n",
      "43. V2                        [ORIGINAL]\n",
      "44. market_forward_excess_returns_cumsum_10_high_ratio_120 [ENGINEERED]\n",
      "45. S7                        [ORIGINAL]\n",
      "46. M7                        [ORIGINAL]\n",
      "47. market_forward_excess_returns_cumsum_5_low_ratio_120 [ENGINEERED]\n",
      "48. market_forward_excess_returns_cumsum_20_high_ratio_60 [ENGINEERED]\n",
      "49. E20                       [ORIGINAL]\n",
      "50. market_forward_excess_returns_cumsum_10_low_ratio_20 [ENGINEERED]\n",
      "\n",
      "Final Training Data Shapes:\n",
      "Training set shape: (6290, 50)\n",
      "Target shape: (6290,)\n",
      "Features selected: 50\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering & Data Preparation\n",
    "\n",
    "# Top features identified (Level 1 - Core)\n",
    "top_features = ['m4', 'v13', 'm11', 's2', 'd4', 'd1', 'd2', 'e8', 'p6', 'm2', \n",
    "                'd8', 'm9', 'p8', 'p7', 's12', 'p13', 'v9', 'd5', 'p1', 's8']\n",
    "\n",
    "print(\"Creating advanced features for training data only...\")\n",
    "\n",
    "# Apply feature engineering only to training data (features + target)\n",
    "train_enh, selected_features = create_advanced_features(\n",
    "    train_for_engineering,  # This contains only final_features + TARGET\n",
    "    top_features=top_features,\n",
    "    window_sizes=(5, 10, 20, 60, 120),   # 1w, 2w, 1m, 3m, 6m\n",
    "    max_features_to_keep=50,\n",
    "    shift=1  # Explicitly set shift to avoid data leakage\n",
    ")\n",
    "\n",
    "# Separate engineered features from original base features\n",
    "original_features_in_selection = [f for f in selected_features if f in final_features]\n",
    "new_engineered_features = [f for f in selected_features if f not in final_features]\n",
    "\n",
    "print(f\"\\nFeature Engineering Results:\")\n",
    "print(f\"Original base features available: {len(final_features)}\")\n",
    "print(f\"Original features selected: {len(original_features_in_selection)}\")\n",
    "print(f\"New engineered features created: {len(new_engineered_features)}\")\n",
    "print(f\"Total features for modeling: {len(selected_features)}\")\n",
    "\n",
    "print(f\"\\nNew engineered features added:\")\n",
    "for i, feat in enumerate(new_engineered_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nAll 50 selected features:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    feat_type = \"ORIGINAL\" if feat in final_features else \"ENGINEERED\"\n",
    "    print(f\"{i:2d}. {feat:<25} [{feat_type}]\")\n",
    "\n",
    "# Final feature matrices - only use training data that was engineered\n",
    "X = train_enh[selected_features].astype('float32')\n",
    "y = train_enh[TARGET].astype('float32')\n",
    "\n",
    "print(f\"\\nFinal Training Data Shapes:\")\n",
    "print(f\"Training set shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features selected: {len(selected_features)}\")\n",
    "\n",
    "# Store for later use in inference\n",
    "final_selected_features = selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ac494e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['market_forward_excess_returns_low_ratio_120',\n",
       " 'market_forward_excess_returns_high_ratio_120',\n",
       " 'market_forward_excess_returns_low_ratio_20',\n",
       " 'V7',\n",
       " 'M18',\n",
       " 'market_forward_excess_returns_low_ratio_60',\n",
       " 'V9',\n",
       " 'P10',\n",
       " 'E8',\n",
       " 'M9',\n",
       " 'M5',\n",
       " 'market_forward_excess_returns_high_ratio_20',\n",
       " 'E19',\n",
       " 'P8',\n",
       " 'M17',\n",
       " 'E12',\n",
       " 'P11',\n",
       " 'market_forward_excess_returns_high_ratio_60',\n",
       " 'P9',\n",
       " 'V8',\n",
       " 'E3',\n",
       " 'M12',\n",
       " 'market_forward_excess_returns_cumsum_20',\n",
       " 'S1',\n",
       " 'M3',\n",
       " 'market_forward_excess_returns_cumsum_20_high_ratio_20',\n",
       " 'M15',\n",
       " 'E2',\n",
       " 'market_forward_excess_returns_cumsum_5_high_ratio_20',\n",
       " 'market_forward_excess_returns_cumsum_5',\n",
       " 'M8',\n",
       " 'M10',\n",
       " 'E17',\n",
       " 'market_forward_excess_returns_cumsum_5_low_ratio_20',\n",
       " 'E16',\n",
       " 'market_forward_excess_returns_cumsum_10_high_ratio_20',\n",
       " 'V12',\n",
       " 'market_forward_excess_returns_cumsum_20_high_ratio_120',\n",
       " 'market_forward_excess_returns_cumsum_5_high_ratio_60',\n",
       " 'market_forward_excess_returns_cumsum_10',\n",
       " 'market_forward_excess_returns_cumsum_20_low_ratio_120',\n",
       " 'V6',\n",
       " 'V2',\n",
       " 'market_forward_excess_returns_cumsum_10_high_ratio_120',\n",
       " 'S7',\n",
       " 'M7',\n",
       " 'market_forward_excess_returns_cumsum_5_low_ratio_120',\n",
       " 'market_forward_excess_returns_cumsum_20_high_ratio_60',\n",
       " 'E20',\n",
       " 'market_forward_excess_returns_cumsum_10_low_ratio_20']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0bd6488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing test data with base features only...\n",
      "Test set base features shape: (10, 94)\n",
      "Base features available for test: 94\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data separately (without feature engineering)\n",
    "print(\"Preparing test data with base features only...\")\n",
    "\n",
    "# Test set only gets the original base features that we can compute\n",
    "test_base_features = [f for f in final_features if f in test_p.columns]\n",
    "X_test_base = test_p[test_base_features].astype('float32')\n",
    "\n",
    "print(f\"Test set base features shape: {X_test_base.shape}\")\n",
    "print(f\"Base features available for test: {len(test_base_features)}\")\n",
    "\n",
    "# Note: During inference, we would need to either:\n",
    "# 1. Only use original features that exist in both train and test, OR  \n",
    "# 2. Create a feature engineering pipeline that can be applied to single rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c1000a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared successfully\n",
      "Final selected features: 50\n",
      "Train shape: (6290, 50)\n"
     ]
    }
   ],
   "source": [
    "# Data matrices are already prepared in previous cell\n",
    "# X, y, and X_test are ready for model training\n",
    "\n",
    "print(f\"Data prepared successfully\")\n",
    "print(f\"Final selected features: {len(final_selected_features)}\")\n",
    "print(f\"Train shape: {X.shape}\")\n",
    "# print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b59af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f133d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6656f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Creating Level 1 features (Core)...\n",
      "ðŸ”§ Creating Level 2 features (Macro)...\n",
      "ðŸ§¹ Cleaning and selecting features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ===== CELL: Model Training with Selected Features =====\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Feature Engineering using your new function\u001b[39;00m\n\u001b[32m      4\u001b[39m top_features = [\u001b[33m'\u001b[39m\u001b[33mm4\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mv13\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mm11\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33ms2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33md4\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33md1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33md2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33me8\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mp6\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mm2\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33md8\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mm9\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mp8\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mp7\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33ms12\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mp13\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mv9\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33md5\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mp1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33ms8\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df_features, selected_features = \u001b[43mcreate_advanced_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_features_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Display summary\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSelected features (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(selected_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_features[:\u001b[32m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mcreate_advanced_features\u001b[39m\u001b[34m(df, top_features, macro_prefixes, window_sizes, max_features_to_keep, shift, inplace)\u001b[39m\n\u001b[32m    198\u001b[39m         top_feats = var_series.head(top_k).index.tolist()\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m top_feats\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m selected = \u001b[43mselect_top_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmarket_forward_excess_returns\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_features_to_keep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Final DataFrame\u001b[39;00m\n\u001b[32m    205\u001b[39m keep_cols = [\u001b[33m'\u001b[39m\u001b[33mdate_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mforward_returns\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrisk_free_rate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmarket_forward_excess_returns\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 188\u001b[39m, in \u001b[36mcreate_advanced_features.<locals>.select_top_features\u001b[39m\u001b[34m(df_in, target_col, top_k)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    187\u001b[39m     gb = GradientBoostingRegressor(n_estimators=\u001b[32m100\u001b[39m, max_depth=\u001b[32m3\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     imp = pd.Series(gb.feature_importances_, index=X_sel.columns).sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    190\u001b[39m     top_feats = imp.head(top_k).index.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:784\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    781\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    783\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:880\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    873\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    874\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    875\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    876\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    877\u001b[39m         )\n\u001b[32m    879\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:490\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    487\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    489\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    492\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    495\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1377\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1348\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1349\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1350\u001b[39m \n\u001b[32m   1351\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1374\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1375\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===== CELL: Model Training with Selected Features =====\n",
    "\n",
    "# 1. Feature Engineering using your new function\n",
    "top_features = ['m4','v13','m11','s2','d4','d1','d2','e8','p6','m2','d8','m9','p8','p7','s12','p13','v9','d5','p1','s8']\n",
    "\n",
    "df_features, selected_features = create_advanced_features(\n",
    "    train,\n",
    "    top_features,\n",
    "    window_sizes=(5,10,20,60,120),\n",
    "    max_features_to_keep=50\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(f\"Selected features ({len(selected_features)}): {selected_features[:10]} ...\")\n",
    "\n",
    "# Prepare X, y\n",
    "X = df_features[selected_features].astype('float32')\n",
    "y = df_features['market_forward_excess_returns'].astype('float32')\n",
    "\n",
    "\n",
    "# 2. Define CatBoost model (Ensemble part 1)\n",
    "cat_params = dict(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function='RMSE',\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(**cat_params)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cat_preds = np.zeros(len(X))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    train_pool = Pool(X_train, y_train)\n",
    "    val_pool = Pool(X_val, y_val)\n",
    "\n",
    "    cat_model.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "    preds = cat_model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.6f}\")\n",
    "\n",
    "    cat_preds[val_idx] = preds\n",
    "\n",
    "\n",
    "# 3. Define Neural Network model (Ensemble part 2)\n",
    "def build_nn(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "nn_preds = np.zeros(len(X))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    nn_model = build_nn(X.shape[1])\n",
    "    early_stop = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss', verbose=0)\n",
    "    nn_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=128,\n",
    "        verbose=0,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    preds = nn_model.predict(X_val).ravel()\n",
    "    rmse = mean_squared_error(y_val, preds, squared=False)\n",
    "    print(f\"[NN] Fold {fold} RMSE: {rmse:.6f}\")\n",
    "\n",
    "    nn_preds[val_idx] = preds\n",
    "\n",
    "\n",
    "# 4. Ensemble Combination (Weighted Average)\n",
    "ensemble_preds = 0.6 * cat_preds + 0.4 * nn_preds\n",
    "rmse_ensemble = mean_squared_error(y, ensemble_preds, squared=False)\n",
    "print(f\"\\nâœ… Ensemble RMSE: {rmse_ensemble:.6f}\")\n",
    "\n",
    "\n",
    "# 5. Diagnostics and Sanity Checks\n",
    "metrics = pd.DataFrame({\n",
    "    \"Model\": [\"CatBoost\", \"NeuralNet\", \"Ensemble\"],\n",
    "    \"RMSE\": [\n",
    "        mean_squared_error(y, cat_preds, squared=False),\n",
    "        mean_squared_error(y, nn_preds, squared=False),\n",
    "        rmse_ensemble\n",
    "    ]\n",
    "})\n",
    "display(metrics)\n",
    "\n",
    "print(\"\\nFeature importance snapshot (CatBoost):\")\n",
    "imp_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': cat_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "display(imp_df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature Importance Preview (Optional)\n",
    "\n",
    "# # ================================================================\n",
    "# # 2ï¸âƒ£ Feature Importance Preview (Optional Diagnostic)\n",
    "# # ================================================================\n",
    "# \"\"\"\n",
    "# Quick diagnostic cell to preview which engineered features\n",
    "# are most informative for predicting market_forward_excess_returns.\n",
    "\n",
    "# You can toggle the mode:\n",
    "#   - mode = \"fast\" â†’ uses Mutual Information (no model training)\n",
    "#   - mode = \"catboost\" â†’ trains a quick CatBoostRegressor for ranking\n",
    "# \"\"\"\n",
    "\n",
    "# from sklearn.feature_selection import mutual_info_regression\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Select mode\n",
    "# mode = \"fast\"  # \"fast\" or \"catboost\"\n",
    "\n",
    "# # Mutual Information Mode (fast)\n",
    "# if mode == \"fast\":\n",
    "#     print(\"âš¡ Running Mutual Information Importance (fast mode)...\")\n",
    "#     mi = mutual_info_regression(X, y, random_state=42)\n",
    "#     mi_df = pd.DataFrame({'feature': X.columns, 'importance': mi})\n",
    "#     mi_df = mi_df.sort_values(by='importance', ascending=False).head(20)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.barplot(data=mi_df, x='importance', y='feature', color='steelblue')\n",
    "#     plt.title(\"Top 20 Features by Mutual Information\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # CatBoost Mode (more precise)\n",
    "# elif mode == \"catboost\":\n",
    "#     from catboost import CatBoostRegressor\n",
    "\n",
    "#     print(\"ðŸ± Running CatBoost Feature Importance (model-based)...\")\n",
    "#     model = CatBoostRegressor(\n",
    "#         iterations=300,\n",
    "#         learning_rate=0.05,\n",
    "#         depth=6,\n",
    "#         random_seed=42,\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     model.fit(X, y)\n",
    "\n",
    "#     fi = model.get_feature_importance(prettified=True)\n",
    "#     fi = fi.sort_values(by='Importances', ascending=False).head(20)\n",
    "\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.barplot(data=fi, x='Importances', y='Feature Id', color='darkorange')\n",
    "#     plt.title(\"Top 20 Features by CatBoost Importance\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# else:\n",
    "#     print(\"Invalid mode. Choose 'fast' or 'catboost'.\")\n",
    "\n",
    "# print(\"âœ… Feature importance preview complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Training CatBoost model with TimeSeries CV...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 80 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n80 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 2395, in _fit\n    train_params = self._prepare_train_params(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 2275, in _prepare_train_params\n    train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs, graph,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 1513, in _build_train_pool\n    train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, graph=graph, weight=sample_weight, group_id=group_id,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 855, in __init__\n    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 1491, in _init\n    self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n  File \"_catboost.pyx\", line 4329, in _catboost._PoolBase._init_pool\n  File \"_catboost.pyx\", line 4352, in _catboost._PoolBase._init_pool\n  File \"_catboost.pyx\", line 2310, in _catboost._init_features_layout\n_catboost.CatBoostError: catboost/libs/data/features_layout.cpp:124: All feature names should be different, but 'forward_returns' used more than once.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     12\u001b[39m param_grid = {\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdepth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m4\u001b[39m, \u001b[32m6\u001b[39m],\n\u001b[32m     14\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.05\u001b[39m, \u001b[32m0.1\u001b[39m],\n\u001b[32m     15\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miterations\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m300\u001b[39m, \u001b[32m500\u001b[39m],\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33ml2_leaf_reg\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m]\n\u001b[32m     17\u001b[39m }\n\u001b[32m     19\u001b[39m grid = GridSearchCV(\n\u001b[32m     20\u001b[39m     estimator=cbc,\n\u001b[32m     21\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     26\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mgrid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m best_cbc = grid.best_estimator_\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Best Params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    964\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    965\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    966\u001b[39m     )\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    974\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1526\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:947\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) != n_candidates * n_splits:\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    942\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcv.split and cv.get_n_splits returned \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    943\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    944\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(n_splits, \u001b[38;5;28mlen\u001b[39m(out) // n_candidates)\n\u001b[32m    945\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[32m    950\u001b[39m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    530\u001b[39m     all_fits_failed_message = (\n\u001b[32m    531\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    532\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    533\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    534\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    535\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    539\u001b[39m     some_fits_failed_message = (\n\u001b[32m    540\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    541\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    546\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 80 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n80 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 5873, in fit\n    return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 2395, in _fit\n    train_params = self._prepare_train_params(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 2275, in _prepare_train_params\n    train_pool = _build_train_pool(X, y, cat_features, text_features, embedding_features, pairs, graph,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 1513, in _build_train_pool\n    train_pool = Pool(X, y, cat_features=cat_features, text_features=text_features, embedding_features=embedding_features, pairs=pairs, graph=graph, weight=sample_weight, group_id=group_id,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 855, in __init__\n    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n  File \"c:\\Users\\calli\\miniconda3\\envs\\ml\\Lib\\site-packages\\catboost\\core.py\", line 1491, in _init\n    self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, graph, weight,\n  File \"_catboost.pyx\", line 4329, in _catboost._PoolBase._init_pool\n  File \"_catboost.pyx\", line 4352, in _catboost._PoolBase._init_pool\n  File \"_catboost.pyx\", line 2310, in _catboost._init_features_layout\n_catboost.CatBoostError: catboost/libs/data/features_layout.cpp:124: All feature names should be different, but 'forward_returns' used more than once.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2ï¸âƒ£ CatBoost Base Model (GridSearch + TimeSeriesSplit)\n",
    "# ================================================================\n",
    "\n",
    "print(\"â³ Training CatBoost model with TimeSeries CV...\")\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# check here random_state = 42 for reproducibility!\n",
    "cbc = CatBoostRegressor(loss_function='RMSE', verbose=0, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'depth': [4, 6],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'iterations': [300, 500],\n",
    "    'l2_leaf_reg': [2, 5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=cbc,\n",
    "    param_grid=param_grid,\n",
    "    cv=tscv,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid.fit(X, y)\n",
    "best_cbc = grid.best_estimator_\n",
    "print(f\"âœ… Best Params: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed795b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Neural Network trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3ï¸âƒ£ Neural Network Model (Feedforward Regressor)\n",
    "# ================================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "def build_nn(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "nn_model = build_nn(X_scaled.shape[1])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# last 20% time-based validation\n",
    "date_cut = train[\"date_id\"].quantile(0.8)\n",
    "train_idx = train[\"date_id\"] <= date_cut\n",
    "val_idx = train[\"date_id\"] > date_cut\n",
    "\n",
    "X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "\n",
    "nn_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "             epochs=100, batch_size=256, verbose=0, callbacks=[es])\n",
    "print(\"âœ… Neural Network trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 4ï¸âƒ£ Ensemble Prediction (0.X Ã— CatBoost + 0.XX Ã— NN)\n",
    "# ================================================================\n",
    "ensemble_cat_pct = 0.8\n",
    "ensemble_nn_pct = 0.2\n",
    "\n",
    "val_cat = best_cbc.predict(X.loc[val_idx])\n",
    "val_nn = nn_model.predict(X_scaled[val_idx]).ravel()\n",
    "\n",
    "val_ensemble = ensemble_cat_pct * val_cat + ensemble_nn_pct * val_nn\n",
    "val_df = train.loc[val_idx].copy()\n",
    "val_df[\"pred\"] = val_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932d6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped weights stats: 0.0 0.00011407995932442048 0.6734031891719277 1.999445759525307 2.0\n",
      "Strategy raw Sharpe: 2.5628806391535512\n",
      "Adjusted Sharpe: 2.562880639150988\n",
      "Vol penalty: 1.0 Return penalty: 1.0 Return gap: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected evaluation: use mapped weights and official formula =====\n",
    "def compute_strategy_stats(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute strategy daily returns and Sharpe (annualized).\n",
    "    weights: array-like positions in [0,2]\n",
    "    forward_returns, risk_free_rate: arrays aligned\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    w = np.asarray(weights)\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "\n",
    "    # Strategy return per day: rf*(1 - w) + w * forward_returns\n",
    "    # Strategy excess over rf:\n",
    "    strat_ret = rf * (1.0 - w) + w * fr\n",
    "    strat_excess = strat_ret - rf   # == w * (fr - rf)\n",
    "    # annualized sharpe\n",
    "    mean = np.nanmean(strat_excess)\n",
    "    std = np.nanstd(strat_excess)\n",
    "    sharpe = (mean / (std + 1e-12)) * np.sqrt(252) if std > 0 else 0.0\n",
    "    # annualized vol of strategy returns\n",
    "    vol_ann = std * np.sqrt(252)\n",
    "    return {\n",
    "        'sharpe': sharpe,\n",
    "        'vol_ann': vol_ann,\n",
    "        'mean_daily_excess': mean,\n",
    "        'std_daily_excess': std,\n",
    "        'strat_ret_series': strat_ret,\n",
    "        'strat_excess_series': strat_excess\n",
    "    }\n",
    "\n",
    "def sharpe_penalty_official(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute adjusted Sharpe like the official metric:\n",
    "    - compute strategy sharpe\n",
    "    - compute market vol and strategy vol, form vol_penalty = 1 + max(0, strategy_vol/market_vol - 1.2)\n",
    "    - compute return_gap penalty like (max(0, (market_mean_excess - strat_mean_excess) * 100 * 252))**2 / 100 etc.\n",
    "    Returns adjusted_sharpe (float) and components.\n",
    "    \"\"\"\n",
    "    # strategy stats\n",
    "    stats = compute_strategy_stats(weights, forward_returns, risk_free_rate)\n",
    "    strat_excess = stats['strat_excess_series']\n",
    "    strat_sharpe = stats['sharpe']\n",
    "    strat_vol = stats['vol_ann']\n",
    "    # market stats\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "    market_excess = fr - rf\n",
    "    market_mean_excess = ( (1 + market_excess).prod() ) ** (1.0 / len(market_excess)) - 1 if len(market_excess)>0 else 0.0\n",
    "    # fallback simpler mean if product fails\n",
    "    # but safer to use mean:\n",
    "    market_mean_excess = np.nanmean(market_excess)\n",
    "    market_std = np.nanstd(fr)\n",
    "    market_vol = market_std * np.sqrt(252) if market_std>0 else 1e-9\n",
    "\n",
    "    # volatility penalty\n",
    "    excess_vol = max(0.0, (strat_vol / (market_vol + 1e-12)) - 1.2)\n",
    "    vol_penalty = 1.0 + excess_vol\n",
    "\n",
    "    # return gap penalty (use squared scaled gap similar to demo code)\n",
    "    strat_mean_excess = np.nanmean(strat_excess)\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100 * 252)  # percent annualized gap\n",
    "    return_penalty = 1.0 + (return_gap**2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = strat_sharpe / (vol_penalty * return_penalty + 1e-12)\n",
    "    return {\n",
    "        'adjusted_sharpe': adjusted_sharpe,\n",
    "        'strat_sharpe': strat_sharpe,\n",
    "        'vol_penalty': vol_penalty,\n",
    "        'return_penalty': return_penalty,\n",
    "        'strat_vol': strat_vol,\n",
    "        'market_vol': market_vol,\n",
    "        'return_gap': return_gap\n",
    "    }\n",
    "\n",
    "# ===== Use it on validation properly mapping raw preds to weights =====\n",
    "\n",
    "# val_ensemble is your raw ensemble prediction (unmapped)\n",
    "# First map to weights using your mapping function (or revised mapping)\n",
    "def robust_signal_to_weight(sig, lower=0.0, upper=2.0):\n",
    "    \"\"\"\n",
    "    Map raw signals to weights robustly using percentile clipping and stable scaling.\n",
    "    If distribution is degenerate, fallback to standard scaling.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig)\n",
    "    lo = np.nanpercentile(sig, 5)\n",
    "    hi = np.nanpercentile(sig, 95)\n",
    "    if np.isclose(hi, lo):\n",
    "        # fallback: z-score and sigmoid mapping\n",
    "        sig_z = (sig - np.nanmean(sig)) / (np.nanstd(sig) + 1e-12)\n",
    "        # map z to [0,2] via logistic\n",
    "        w = 2.0 / (1.0 + np.exp(-sig_z))\n",
    "    else:\n",
    "        w = (sig - lo) / (hi - lo + 1e-12) * (upper - lower) + lower\n",
    "    return np.clip(w, lower, upper)\n",
    "\n",
    "# compute mapped weights\n",
    "val_weights = robust_signal_to_weight(val_ensemble)   # or pass val_cat/val_nn separately\n",
    "\n",
    "# compute official adjusted sharpe and components\n",
    "res = sharpe_penalty_official(val_weights, val_df['forward_returns'].to_numpy(), val_df['risk_free_rate'].to_numpy())\n",
    "\n",
    "print(\"Mapped weights stats:\", np.nanmin(val_weights), np.nanpercentile(val_weights,5), np.nanmedian(val_weights), np.nanpercentile(val_weights,95), np.nanmax(val_weights))\n",
    "print(\"Strategy raw Sharpe:\", res['strat_sharpe'])\n",
    "print(\"Adjusted Sharpe:\", res['adjusted_sharpe'])\n",
    "print(\"Vol penalty:\", res['vol_penalty'], \"Return penalty:\", res['return_penalty'], \"Return gap:\", res['return_gap'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23c741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # 6ï¸âƒ£ Competition-Compliant Inference Function\n",
    "# # ================================================================\n",
    "# _cat_model = best_cbc\n",
    "# _nn_model = nn_model\n",
    "# _scaler = scaler\n",
    "# _feat_cols = features\n",
    "\n",
    "# \"\"\"\n",
    "#     Check if is really necessary exchange from pl to pd and back to pl?\n",
    "#     pl.DataFrame (we convert to pandas inside)\n",
    "# \"\"\"\n",
    "# def predict(pl_df):\n",
    "#     \"\"\"Competition inference function.\"\"\"\n",
    "#     pdf = pl_df.to_pandas().fillna(0.0)\n",
    "#     for f in _feat_cols:\n",
    "#         if f not in pdf.columns:\n",
    "#             pdf[f] = 0.0\n",
    "#     Xp = pdf[_feat_cols].values\n",
    "#     Xp_scaled = _scaler.transform(Xp)\n",
    "#     pred_cat = _cat_model.predict(pdf[_feat_cols])\n",
    "#     pred_nn = _nn_model.predict(Xp_scaled, verbose=0).ravel()\n",
    "#     preds = ensemble_cat_pct * pred_cat + ensemble_nn_pct * pred_nn\n",
    "#     lo, hi = np.percentile(preds, [5, 95])\n",
    "#     weights = np.clip((preds - lo) / (hi - lo + 1e-9) * 2.0, 0, 2)\n",
    "#     return pd.DataFrame({\"prediction\": weights.astype(\"float32\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 6ï¸âƒ£ Competition-Compliant Inference Function\n",
    "# ================================================================\n",
    "_cat_model = best_cbc\n",
    "_nn_model = nn_model\n",
    "_scaler = scaler\n",
    "_feat_cols = features\n",
    "_history_returns = list(train.loc[val_idx, 'forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "def predict(pl_df: pl.DataFrame) -> float:\n",
    "    \"\"\"Competition inference function - returns single float allocation.\"\"\"\n",
    "    global _history_returns\n",
    "    \n",
    "    # Convert Polars to Pandas and handle missing values\n",
    "    pdf = pl_df.to_pandas().fillna(0.0)\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for f in _feat_cols:\n",
    "        if f not in pdf.columns:\n",
    "            pdf[f] = 0.0\n",
    "    \n",
    "    # Get features in correct format\n",
    "    X_features = pdf[_feat_cols].values\n",
    "    X_scaled = _scaler.transform(X_features)\n",
    "    \n",
    "    # Make predictions from both models\n",
    "    pred_cat = _cat_model.predict(pdf[_feat_cols])[0]  # Get first prediction\n",
    "    pred_nn = _nn_model.predict(X_scaled, verbose=0).ravel()[0]  # Get first prediction\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    pred = ensemble_cat_pct * pred_cat + ensemble_nn_pct * pred_nn\n",
    "    \n",
    "    # Estimate rolling volatility for scaling\n",
    "    vol_est = np.std(_history_returns) if len(_history_returns) > 1 else 1e-3\n",
    "    \n",
    "    # Scale prediction to allocation with volatility adjustment\n",
    "    allocation = float(np.clip((best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "    \n",
    "    # Update history for rolling volatility estimation\n",
    "    if 'lagged_forward_returns' in pl_df.columns:\n",
    "        try:\n",
    "            _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "        except:\n",
    "            _history_returns.append(0.0)\n",
    "    else:\n",
    "        _history_returns.append(0.0)\n",
    "    \n",
    "    # Keep only last VOL_WINDOW entries\n",
    "    _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "    \n",
    "    return allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca07cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNEXT STEPS, IMPORTANT FOR IMPROVEMENT:\\n\\nStronger feature scaling\\n\\nPCA optional\\n\\nRolling retrain or time-based CV for robustness out of sample\\n\\nOptimization of the mix (CatBoost vs NN) to dynamically find the optimal weight based on your adjusted Sharpe\\nEventually to be extended to more models in the ensemble\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NEXT STEPS, IMPORTANT FOR IMPROVEMENT:\n",
    "\n",
    "Stronger feature scaling\n",
    "\n",
    "PCA optional\n",
    "\n",
    "Rolling retrain or time-based CV for robustness out of sample\n",
    "\n",
    "Optimization of the mix (CatBoost vs NN) to dynamically find the optimal weight based on your adjusted Sharpe\n",
    "Eventually to be extended to more models in the ensemble\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b37e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # 7ï¸âƒ£ Kaggle Evaluation Server / Local Submission\n",
    "# # ================================================================\n",
    "\n",
    "# if KAGGLE_ENV:\n",
    "#     # Kaggle competition environment\n",
    "#     server = kdeval.DefaultInferenceServer(predict)\n",
    "    \n",
    "#     if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#         server.serve()\n",
    "#     else:\n",
    "#         server.run_local_gateway((str(DATA_DIR),))\n",
    "        \n",
    "# else:\n",
    "#     # Local environment - generate submission file\n",
    "#     print(\"ðŸ”§ Local mode - generating submission file...\")\n",
    "    \n",
    "#     # Generate predictions for test set\n",
    "#     test_pred_cat = best_cbc.predict(X_test)\n",
    "#     test_pred_nn = nn_model.predict(scaler.transform(X_test), verbose=0).ravel()\n",
    "#     preds = ensemble_cat_pct * test_pred_cat + ensemble_nn_pct * test_pred_nn\n",
    "    \n",
    "#     # Apply same scaling logic as validation\n",
    "#     test_exposures = np.clip(best_k * preds, 0, 2)\n",
    "    \n",
    "#     # Apply smoothing like in the working example\n",
    "#     alpha = 0.8\n",
    "#     smoothed_allocation = []\n",
    "#     prev = 0.0\n",
    "#     for x in test_exposures:\n",
    "#         s = alpha * x + (1 - alpha) * prev\n",
    "#         smoothed_allocation.append(s)\n",
    "#         prev = s\n",
    "#     smoothed_allocation = np.array(smoothed_allocation)\n",
    "    \n",
    "#     # Create submission\n",
    "#     submission = pd.DataFrame({\n",
    "#         'date_id': test['date_id'],\n",
    "#         'prediction': smoothed_allocation.astype('float32')\n",
    "#     })\n",
    "    \n",
    "#     submission.to_csv('submission_ensemble.csv', index=False)\n",
    "#     print(\"ðŸ“ Saved submission_ensemble.csv\")\n",
    "#     print(f\"ðŸ“Š Prediction range: [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58021807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138d4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
