{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b72ebaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment - kaggle_evaluation not available\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "#  HULL TACTICAL MARKET PREDICTION — ENSEMBLE + SHARPEPENALTY\n",
    "# ================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from typing import Dict \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Try to import kaggle_evaluation, handle if not available\n",
    "try:\n",
    "    import kaggle_evaluation.default_inference_server as kdeval\n",
    "    KAGGLE_ENV = True\n",
    "    print(\"Running in Kaggle competition environment\")\n",
    "except ImportError:\n",
    "    KAGGLE_ENV = False\n",
    "    print(\"Running in local environment - kaggle_evaluation not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c00d6c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from local environment\n",
      "Data loaded successfully\n",
      "Train shape: (8990, 98) | Test shape: (10, 99)\n",
      "Base features available: 94\n",
      "Target variable: market_forward_excess_returns\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Data Loading & Initial Feature Preparation\n",
    "# ================================================================\n",
    "\n",
    "## Configuration and Data Loading\n",
    "# DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "DATA_DIR = Path(\"01_data\")\n",
    "\n",
    "TARGET = \"market_forward_excess_returns\"\n",
    "drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\"]\n",
    "VOL_WINDOW = 20        # volatility window in days\n",
    "VALIDATION_SIZE = 2700          # days, approx. 30% of data\n",
    "\n",
    "def time_split_train_val(df: pd.DataFrame, val_size: int = 2700):\n",
    "    \"\"\"Split data chronologically for time series validation.\"\"\"\n",
    "    df = df.sort_values('date_id').reset_index(drop=True)\n",
    "    train_df = df.iloc[:-val_size].copy()\n",
    "    val_df   = df.iloc[-val_size:].copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "# Load train/test data using the KAGGLE_ENV variable from cell 1\n",
    "if KAGGLE_ENV:\n",
    "    print(\"Loading data from Kaggle environment\")\n",
    "    DATA_DIR = Path('/kaggle/input/hull-tactical-market-prediction')\n",
    "    train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "    test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "else:\n",
    "    print(\"Loading data from local environment\")\n",
    "    # Try different possible local paths\n",
    "    local_paths = [\n",
    "        DATA_DIR / \"train.csv\",\n",
    "        Path(\"01_data/train.csv\"),\n",
    "        Path(\"train.csv\")\n",
    "    ]\n",
    "    \n",
    "    train_path = None\n",
    "    test_path = None\n",
    "    \n",
    "    for path in local_paths:\n",
    "        if path.exists():\n",
    "            train_path = path\n",
    "            test_path = path.parent / \"test.csv\"\n",
    "            break\n",
    "    \n",
    "    if train_path is None or not test_path.exists():\n",
    "        raise FileNotFoundError(\"Could not find train.csv and test.csv files in expected locations\")\n",
    "    \n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Data loaded successfully\")\n",
    "print(f\"Train shape: {train.shape} | Test shape: {test.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "train = train.sort_values(\"date_id\").reset_index(drop=True)\n",
    "test = test.sort_values(\"date_id\").reset_index(drop=True)\n",
    "\n",
    "# Handle missing values\n",
    "train = train.fillna(0.0)\n",
    "test = test.fillna(0.0)\n",
    "\n",
    "# Base features (before advanced transformations)\n",
    "base_features = [c for c in train.columns if c not in drop_cols + [TARGET]]\n",
    "\n",
    "print(f\"Base features available: {len(base_features)}\")\n",
    "print(f\"Target variable: {TARGET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "39e94f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, median_map: Dict[str, float], feature_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and prepare DataFrame by handling missing values intelligently.\n",
    "    \n",
    "    Strategy:\n",
    "    - Use median imputation for numeric columns with some missing values\n",
    "    - Use zero-fill for columns with very few missing values  \n",
    "    - Only process existing columns (no synthetic data creation)\n",
    "\n",
    "    Args:\n",
    "    df: Input DataFrame\n",
    "    median_map: Dictionary mapping column names to median values\n",
    "    feature_cols: List of feature column names to process\n",
    "\n",
    "    Returns:\n",
    "    Cleaned DataFrame\n",
    "\n",
    "    Median is much less sensitive to extreme values (outliers)\n",
    "    Mean can be heavily skewed by a few very large or very small values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Only work with columns that actually exist in the DataFrame\n",
    "    existing_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    if not existing_cols:\n",
    "        print(\"Warning: No feature columns found in DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    # Calculate missing percentages for existing columns\n",
    "    missing_pct = (df[existing_cols].isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # Categorize columns by missing percentage\n",
    "    cols_fill_median = missing_pct[(missing_pct > 5) & (missing_pct <= 50)].index.tolist()\n",
    "    cols_fill_zero = missing_pct[missing_pct <= 5].index.tolist()\n",
    "    \n",
    "    # Apply median imputation for moderately missing columns\n",
    "    if cols_fill_median:\n",
    "        for col in cols_fill_median:\n",
    "            median_val = median_map.get(col, df[col].median())\n",
    "            if pd.isna(median_val):  # Handle case where median is NaN\n",
    "                median_val = 0.0\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Apply zero-fill for low missing columns\n",
    "    if cols_fill_zero:\n",
    "        df[cols_fill_zero] = df[cols_fill_zero].fillna(0)\n",
    "    \n",
    "    # Ensure all feature columns are numeric\n",
    "    for col in existing_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    # Final cleanup - ensure no inf values\n",
    "    df[existing_cols] = df[existing_cols].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c643414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: Train 6290 | Validation 2700 rows\n",
      "Preprocessing complete\n",
      "Number of base features: 94\n",
      "Base features available: ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1']...\n",
      "Target variable 'market_forward_excess_returns' extracted separately\n",
      "Features and target prepared separately to avoid data leakage\n"
     ]
    }
   ],
   "source": [
    "## Train / Validation Split and Median Imputation\n",
    "train_df, val_df = time_split_train_val(train, val_size=VALIDATION_SIZE)\n",
    "print(f\"Data split: Train {train_df.shape[0]} | Validation {val_df.shape[0]} rows\")\n",
    "\n",
    "# Create median map from training portion only\n",
    "median_map = {}\n",
    "for c in base_features:\n",
    "    if c in train_df.columns:\n",
    "        if train_df[c].dtype.kind in 'fiu':  # numeric types\n",
    "            median_val = train_df[c].median(skipna=True)\n",
    "            median_map[c] = float(median_val) if not pd.isna(median_val) else 0.0\n",
    "        else:\n",
    "            median_map[c] = 0.0\n",
    "    else:\n",
    "        median_map[c] = 0.0\n",
    "\n",
    "# Apply preprocessing to all splits\n",
    "train_full = prepare_df(train_df, median_map, base_features)\n",
    "val_full   = prepare_df(val_df, median_map, base_features)\n",
    "test_full  = prepare_df(test, median_map, base_features)\n",
    "\n",
    "# Extract only the base features (remove drop_cols and target)\n",
    "final_features = [c for c in base_features if c in train_full.columns]\n",
    "train_p = train_full[final_features].copy()\n",
    "val_p   = val_full[final_features].copy()\n",
    "test_p  = test_full[final_features].copy()\n",
    "\n",
    "# Keep target and other columns separate for later use\n",
    "train_target = train_full[TARGET].copy()\n",
    "val_target   = val_full[TARGET].copy()\n",
    "\n",
    "# Validation check\n",
    "if not final_features:\n",
    "    raise ValueError(\"No features available after preprocessing!\")\n",
    "\n",
    "print(f\"Preprocessing complete\")\n",
    "print(f\"Number of base features: {len(final_features)}\")\n",
    "print(f\"Base features available: {final_features[:10]}...\" if len(final_features) > 10 else f\"Features: {final_features}\")\n",
    "\n",
    "print(f\"Target variable '{TARGET}' extracted separately\")\n",
    "\n",
    "# REMOVE THIS - causes data leakage:\n",
    "# train_for_engineering = train_p.copy()\n",
    "# train_for_engineering[TARGET] = train_target\n",
    "\n",
    "# CORRECT APPROACH - keep features and target separate\n",
    "print(\"Features and target prepared separately to avoid data leakage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7a04aa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'E1',\n",
       " 'E10',\n",
       " 'E11',\n",
       " 'E12',\n",
       " 'E13',\n",
       " 'E14',\n",
       " 'E15',\n",
       " 'E16',\n",
       " 'E17',\n",
       " 'E18',\n",
       " 'E19',\n",
       " 'E2',\n",
       " 'E20',\n",
       " 'E3',\n",
       " 'E4',\n",
       " 'E5',\n",
       " 'E6',\n",
       " 'E7',\n",
       " 'E8',\n",
       " 'E9',\n",
       " 'I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'M1',\n",
       " 'M10',\n",
       " 'M11',\n",
       " 'M12',\n",
       " 'M13',\n",
       " 'M14',\n",
       " 'M15',\n",
       " 'M16',\n",
       " 'M17',\n",
       " 'M18',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'P1',\n",
       " 'P10',\n",
       " 'P11',\n",
       " 'P12',\n",
       " 'P13',\n",
       " 'P2',\n",
       " 'P3',\n",
       " 'P4',\n",
       " 'P5',\n",
       " 'P6',\n",
       " 'P7',\n",
       " 'P8',\n",
       " 'P9',\n",
       " 'S1',\n",
       " 'S10',\n",
       " 'S11',\n",
       " 'S12',\n",
       " 'S2',\n",
       " 'S3',\n",
       " 'S4',\n",
       " 'S5',\n",
       " 'S6',\n",
       " 'S7',\n",
       " 'S8',\n",
       " 'S9',\n",
       " 'V1',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0c136238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1',\n",
       "       'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19',\n",
       "       'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3',\n",
       "       'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13',\n",
       "       'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7',\n",
       "       'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5',\n",
       "       'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4',\n",
       "       'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2',\n",
       "       'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'forward_returns',\n",
       "       'risk_free_rate', 'market_forward_excess_returns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_full.columns # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "44f708af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10',\n",
       "       'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2',\n",
       "       'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4',\n",
       "       'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14',\n",
       "       'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8',\n",
       "       'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6',\n",
       "       'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5',\n",
       "       'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3',\n",
       "       'V4', 'V5', 'V6', 'V7', 'V8', 'V9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_p.columns # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b8f669f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.003038\n",
       "1      -0.009114\n",
       "2      -0.010243\n",
       "3       0.004046\n",
       "4      -0.012301\n",
       "          ...   \n",
       "6285   -0.016339\n",
       "6286    0.004761\n",
       "6287   -0.016470\n",
       "6288   -0.007177\n",
       "6289   -0.008327\n",
       "Name: market_forward_excess_returns, Length: 6290, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target # correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7dc1c038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D1',\n",
       " 'D2',\n",
       " 'D3',\n",
       " 'D4',\n",
       " 'D5',\n",
       " 'D6',\n",
       " 'D7',\n",
       " 'D8',\n",
       " 'D9',\n",
       " 'E1',\n",
       " 'E10',\n",
       " 'E11',\n",
       " 'E12',\n",
       " 'E13',\n",
       " 'E14',\n",
       " 'E15',\n",
       " 'E16',\n",
       " 'E17',\n",
       " 'E18',\n",
       " 'E19',\n",
       " 'E2',\n",
       " 'E20',\n",
       " 'E3',\n",
       " 'E4',\n",
       " 'E5',\n",
       " 'E6',\n",
       " 'E7',\n",
       " 'E8',\n",
       " 'E9',\n",
       " 'I1',\n",
       " 'I2',\n",
       " 'I3',\n",
       " 'I4',\n",
       " 'I5',\n",
       " 'I6',\n",
       " 'I7',\n",
       " 'I8',\n",
       " 'I9',\n",
       " 'M1',\n",
       " 'M10',\n",
       " 'M11',\n",
       " 'M12',\n",
       " 'M13',\n",
       " 'M14',\n",
       " 'M15',\n",
       " 'M16',\n",
       " 'M17',\n",
       " 'M18',\n",
       " 'M2',\n",
       " 'M3',\n",
       " 'M4',\n",
       " 'M5',\n",
       " 'M6',\n",
       " 'M7',\n",
       " 'M8',\n",
       " 'M9',\n",
       " 'P1',\n",
       " 'P10',\n",
       " 'P11',\n",
       " 'P12',\n",
       " 'P13',\n",
       " 'P2',\n",
       " 'P3',\n",
       " 'P4',\n",
       " 'P5',\n",
       " 'P6',\n",
       " 'P7',\n",
       " 'P8',\n",
       " 'P9',\n",
       " 'S1',\n",
       " 'S10',\n",
       " 'S11',\n",
       " 'S12',\n",
       " 'S2',\n",
       " 'S3',\n",
       " 'S4',\n",
       " 'S5',\n",
       " 'S6',\n",
       " 'S7',\n",
       " 'S8',\n",
       " 'S9',\n",
       " 'V1',\n",
       " 'V10',\n",
       " 'V11',\n",
       " 'V12',\n",
       " 'V13',\n",
       " 'V2',\n",
       " 'V3',\n",
       " 'V4',\n",
       " 'V5',\n",
       " 'V6',\n",
       " 'V7',\n",
       " 'V8',\n",
       " 'V9']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features # correct, this are the columns of train_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e0529784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of final features = len(final_features)\n",
    "final_selected_features = [c for c in final_features if c in train_p.columns]\n",
    "len(final_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5dcb7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Advanced Feature Factory (Enhanced) =====\n",
    "def create_advanced_features(df,\n",
    "                             top_features,\n",
    "                             macro_prefixes=('M','V','P','S'),\n",
    "                             window_sizes=(5,10,20,60,120),\n",
    "                             shift=1,  # Added shift parameter\n",
    "                             inplace=False):\n",
    "    \"\"\"\n",
    "    Create advanced features following a two-level approach:\n",
    "      1) Lightweight Core Features (applied to `top_features`)\n",
    "      2) Macro-Context Features (applied to columns starting with macro_prefixes)\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        top_features: List of most important features for Level 1 processing\n",
    "        macro_prefixes: Tuple of prefixes for Level 2 features\n",
    "        window_sizes: Rolling window sizes\n",
    "        shift: Number of periods to shift for avoiding data leakage\n",
    "        inplace: Whether to modify DataFrame in place\n",
    "    \n",
    "    Returns:\n",
    "        df_out: DataFrame with new features (and original columns)\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "\n",
    "    # Ensure datetime-like ordering by date_id if present\n",
    "    if 'date_id' in df.columns:\n",
    "        df = df.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "    # Helper: ensure numeric dtype for selected cols\n",
    "    def _to_numeric(cols):\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors='coerce').fillna(0.0)\n",
    "\n",
    "    # ------------- Level 1: Core Features (top_features) -------------\n",
    "    # Function to calculate rolling statistics and distance to rolling mean\n",
    "    def create_rolling_and_distance_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling statistics and distance features efficiently using shared roll object\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll = shifted_col.rolling(window=w, min_periods=1)  # Create roll object once\n",
    "            \n",
    "                # Calculate all rolling statistics from the same roll object\n",
    "                roll_mean = roll.mean()\n",
    "                roll_std = roll.std().fillna(0.0)\n",
    "                roll_median = roll.median()\n",
    "                roll_max = roll.max()\n",
    "                roll_min = roll.min()\n",
    "            \n",
    "                # Store rolling features\n",
    "                df[f\"{c}_rolling_mean_{w}\"] = roll_mean.astype('float32')\n",
    "                df[f\"{c}_rolling_std_{w}\"] = roll_std.astype('float32')\n",
    "                df[f\"{c}_rolling_median_{w}\"] = roll_median.astype('float32')\n",
    "                df[f\"{c}_rolling_max_{w}\"] = roll_max.astype('float32')\n",
    "                df[f\"{c}_rolling_min_{w}\"] = roll_min.astype('float32')\n",
    "            \n",
    "                # Calculate distance to rolling mean using the same roll_mean\n",
    "                df[f\"{c}_dist_to_rolling_mean_{w}\"] = (df[c] - roll_mean).astype('float32')\n",
    "    \n",
    "    # Function to calculate variance features\n",
    "    def create_variance_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling variance features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_var = shifted_col.rolling(window=w, min_periods=1).var().fillna(0.0)\n",
    "                \n",
    "                df[f\"{c}_rolling_var_{w}\"] = roll_var.astype('float32')\n",
    "\n",
    "    # Function to calculate z-score features\n",
    "    def create_zscore_features(cols, windows=window_sizes, shift_periods=shift):\n",
    "        \"\"\"Create rolling z-scores with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                # Apply shift BEFORE rolling operations\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_mean = shifted_col.rolling(window=w, min_periods=1).mean()\n",
    "                roll_std = shifted_col.rolling(window=w, min_periods=1).std().fillna(0.0)\n",
    "                \n",
    "                df[f\"{c}_z_{w}\"] = ((df[c] - roll_mean) / (roll_std + 1e-9)).astype('float32')\n",
    "\n",
    "    # Function to calculate spread and percentage change features\n",
    "    def create_spread_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create spread and percentage change features\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            # Use proper shift for difference calculations\n",
    "            df[f\"{c}_diff_1\"] = (df[c] - df[c].shift(shift_periods)).astype('float32')\n",
    "            df[f\"{c}_pctchg_1\"] = (df[c].pct_change(periods=shift_periods).fillna(0.0)).astype('float32')\n",
    "\n",
    "    # Function to calculate numerical PACF values to be added as additional features\n",
    "    def create_pacf_features(cols, nlags=10, shift_periods=shift):\n",
    "        \"\"\"Create PACF features for selected columns\"\"\"\n",
    "        from statsmodels.tsa.stattools import pacf\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            # Change from 'ywunbiased' to 'yw' (Yule-Walker method)\n",
    "            pacf_values = pacf(shifted_col, nlags=nlags, method='yw')\n",
    "            for lag in range(1, nlags + 1):\n",
    "                df[f\"{c}_pacf_{lag}\"] = pacf_values[lag]\n",
    "\n",
    "    # Function to calculate numerical ACF values to be added as additional features\n",
    "    def create_acf_features(cols, nlags=10, shift_periods=shift):\n",
    "        \"\"\"Create ACF features for selected columns\"\"\"\n",
    "        from statsmodels.tsa.stattools import acf\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            acf_values = acf(shifted_col, nlags=nlags, fft=False)\n",
    "            for lag in range(1, nlags + 1):\n",
    "                df[f\"{c}_acf_{lag}\"] = acf_values[lag]\n",
    "\n",
    "    # Function to calculate pandas autocorr values to be added as additional features\n",
    "    def create_autocorr_features(cols, lags=10, shift_periods=shift):\n",
    "        \"\"\"Create autocorrelation features for selected columns\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for lag in range(1, lags + 1):\n",
    "                df[f\"{c}_autocorr_{lag}\"] = df[c].autocorr(lag=lag)\n",
    "\n",
    "    # Function to calculate skewness and kurtosis features\n",
    "    def create_skewness_kurtosis_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create skewness and kurtosis features for selected columns\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods)\n",
    "            df[f\"{c}_skewness\"] = shifted_col.rolling(window=30, min_periods=1).skew().astype('float32').fillna(0.0)\n",
    "            df[f\"{c}_kurtosis\"] = shifted_col.rolling(window=30, min_periods=1).kurt().astype('float32').fillna(0.0)\n",
    "\n",
    "    # Function to calculate momentum features\n",
    "    def create_momentum_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create momentum features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                df[f\"{c}_momentum_{w}\"] = (shifted_col - shifted_col.shift(w)).astype('float32')\n",
    "\n",
    "    # Function to calculate distance from value to momentum\n",
    "    def create_distance_to_momentum_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create distance to momentum features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                momentum = shifted_col - shifted_col.shift(w)\n",
    "                df[f\"{c}_dist_to_momentum_{w}\"] = (df[c] - momentum).astype('float32')\n",
    "\n",
    "    # Function to calculate difference series\n",
    "    def create_difference_features(cols, lags=(1,5,10), shift_periods=shift):\n",
    "        \"\"\"Create difference features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for lag in lags:\n",
    "                df[f\"{c}_diff_{lag}\"] = (df[c] - df[c].shift(lag + shift_periods)).astype('float32')\n",
    "\n",
    "    # Function to calculate normalized series\n",
    "    def create_normalized_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create normalized features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods)\n",
    "            df[f\"{c}_normalized\"] = (shifted_col - shifted_col.mean()) / shifted_col.std()\n",
    "\n",
    "    # Function to calculate rolling sums features\n",
    "    def create_rolling_sum_features(cols, windows=(5,10,20), shift_periods=shift):\n",
    "        \"\"\"Create rolling sums with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                df[f\"{c}_macro_rolling_sum_{w}\"] = shifted_col.rolling(window=w, min_periods=1).sum().astype('float32')            \n",
    "\n",
    "    # Function to calculate cumsum features\n",
    "    def create_cumsum_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create cumulative sum features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods)\n",
    "            df[f\"{c}_cumsum\"] = shifted_col.cumsum().astype('float32')\n",
    "\n",
    "    # Function to calculate Hurst exponent features\n",
    "    def create_hurst_features(cols, shift_periods=shift):\n",
    "        \"\"\"Create Hurst exponent features with proper shift\"\"\"\n",
    "        \"\"\"\n",
    "        The Hurst exponent uses lags to measure the long-term memory of the time series. \n",
    "        For each lag in the range, calculate the standard deviation of the differenced series. \n",
    "        Then calculate the slope of the log lags versus the standard deviations. \n",
    "        You can do this by returning the first value from NumPy’s polyfit function \n",
    "        which fits a first-degree polynomial function.\n",
    "        \n",
    "        The Hurst exponent ranges between 0 and 1.\n",
    "\n",
    "        If the Hurst exponent is below 0.5, the market is mean reverting. \n",
    "        Reversal strategies win in these markets.\n",
    "\n",
    "        If the Hurst exponent of 0.5 means the market is random. \n",
    "        In this case, a trading strategy that relies on the market direction will lose money.\n",
    "\n",
    "        If the Hurst exponent is above 0.5 the market is trending. \n",
    "        Markets with a high Hurst exponent are perfect for trend-following strategies.\n",
    "        \"\"\"\n",
    "        def hurst_exponent(ts):\n",
    "            lags = range(2, 20)\n",
    "            tau = [np.std(np.subtract(ts[lag:], ts[:-lag])) for lag in lags]\n",
    "            poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "            return poly[0] * 2.0\n",
    "        \n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            shifted_col = df[c].shift(shift_periods).fillna(0.0)\n",
    "            df[f\"{c}_hurst\"] = hurst_exponent(shifted_col)\n",
    "\n",
    "    # Function to calculate lagged observations from the past\n",
    "    def create_lagged_features(cols, lags=(1,5,10), shift_periods=shift):\n",
    "        \"\"\"Create lagged features with proper shift\"\"\"\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            for lag in lags:\n",
    "                df[f\"{c}_lag_{lag}\"] = df[c].shift(lag + shift_periods).astype('float32')\n",
    "\n",
    "    # ------------- Level 2: Macro Features (selective) -------------\n",
    "    # Function to calculate correlation features\n",
    "    def create_correlation_features(pairs=None, window=30, shift_periods=shift):\n",
    "        \"\"\"Create rolling correlations with proper shift\"\"\"\n",
    "        if pairs is None:\n",
    "            # Build pairs from top_features (limit to avoid explosion)\n",
    "            cand = []\n",
    "            for i in range(len(top_features)):\n",
    "                for j in range(i+1, len(top_features)):\n",
    "                    cand.append((top_features[i], top_features[j]))\n",
    "            pairs = cand[:10]  # Limit to 10 pairs\n",
    "        \n",
    "        for a, b in pairs:\n",
    "            if a not in df.columns or b not in df.columns:\n",
    "                continue\n",
    "            # Apply shift to both series\n",
    "            a_shifted = df[a].shift(shift_periods)\n",
    "            b_shifted = df[b].shift(shift_periods)\n",
    "            corr = a_shifted.rolling(window=window, min_periods=1).corr(b_shifted)\n",
    "            df[f\"macro_corr_{a}_{b}_{30}\"] = corr.astype('float32').fillna(0.0)\n",
    "\n",
    "    # Function to calculate volatility spread features\n",
    "    def create_volatility_features(cols=None, windows=(20,60), shift_periods=shift):\n",
    "        \"\"\"Create volatility spread features with proper shift\"\"\"\n",
    "        if cols is None:\n",
    "            cols = [c for c in df.columns if c.startswith('v')]\n",
    "        \n",
    "        # Limit to prevent feature explosion\n",
    "        cols = cols[:8]\n",
    "        \n",
    "        for w in windows:\n",
    "            vols = {}\n",
    "            for c in cols:\n",
    "                if c in df.columns:\n",
    "                    shifted_col = df[c].shift(shift_periods)\n",
    "                    vols[c] = shifted_col.rolling(window=w, min_periods=1).std().astype('float32').fillna(0.0)\n",
    "            \n",
    "            # Create spread between consecutive volatilities\n",
    "            vol_keys = list(vols.keys())\n",
    "            for i in range(len(vol_keys) - 1):\n",
    "                a, b = vol_keys[i], vol_keys[i + 1]\n",
    "                df[f\"macro_volspread_{a}_{b}_{w}\"] = (vols[a] - vols[b]).astype('float32')\n",
    "\n",
    "    # Function to calculate high/low ratio features\n",
    "    def create_extremes_features(cols, windows=(20,60,120), shift_periods=shift):\n",
    "        \"\"\"Create high/low ratio features with proper shift\"\"\"\n",
    "        # Limit columns to prevent explosion\n",
    "        cols = [c for c in cols if c in df.columns][:10]\n",
    "        \n",
    "        for c in cols:\n",
    "            for w in windows:\n",
    "                shifted_col = df[c].shift(shift_periods)\n",
    "                roll_max = shifted_col.rolling(window=w, min_periods=1).max()\n",
    "                roll_min = shifted_col.rolling(window=w, min_periods=1).min()\n",
    "\n",
    "                df[f\"{c}_macro_high_ratio_{w}\"] = (df[c] / (roll_max + 1e-9)).astype('float32')\n",
    "                df[f\"{c}_macro_low_ratio_{w}\"] = (df[c] / (roll_min + 1e-9)).astype('float32')\n",
    "\n",
    "    # Execute feature creation\n",
    "    print(\"Creating Level 1 features (Core)...\")\n",
    "    _to_numeric(top_features)\n",
    "    create_rolling_and_distance_features(top_features)\n",
    "    create_variance_features(top_features)\n",
    "    create_zscore_features(top_features)\n",
    "    create_spread_features(top_features)\n",
    "    create_pacf_features(top_features)\n",
    "    create_acf_features(top_features)\n",
    "    create_autocorr_features(top_features)\n",
    "    create_skewness_kurtosis_features(top_features)\n",
    "    create_momentum_features(top_features)\n",
    "    create_distance_to_momentum_features(top_features)\n",
    "    create_difference_features(top_features)\n",
    "    create_normalized_features(top_features)\n",
    "    create_rolling_sum_features(top_features)\n",
    "    create_cumsum_features(top_features)\n",
    "    create_hurst_features(top_features)\n",
    "    create_lagged_features(top_features)\n",
    "\n",
    "    print(\"Creating Level 2 features (Macro)...\")\n",
    "    macro_cols = [c for c in df.columns if any(c.startswith(pref) for pref in macro_prefixes)]\n",
    "    _to_numeric(macro_cols)\n",
    "    print('Macro columns for Level 2 features:', macro_cols)\n",
    "\n",
    "    create_correlation_features(window=30)\n",
    "    create_volatility_features(windows=(20,60))\n",
    "    create_extremes_features([c for c in df.columns if c.startswith(('m','p'))], windows=(20,60,120))\n",
    "\n",
    "    # Clean data\n",
    "    print(\"Cleaning and selecting features...\")\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0.0, inplace=True)\n",
    "\n",
    "    # Downcast to save memory\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    if len(float_cols) > 0:\n",
    "        df[float_cols] = df[float_cols].astype('float32')\n",
    "\n",
    "    print(f\"Feature engineering complete. Created {len(df.columns)} total columns.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d9d001b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working\n",
    "# # substitute with def enhanced_feature_selection()\n",
    "# # keep here for reference, in case needed later\n",
    "\n",
    "# # Feature Engineering & Data Preparation\n",
    "\n",
    "# # Fix the case sensitivity - use uppercase to match your DataFrame columns\n",
    "# top_features = ['M4', 'V13', 'M11', 'S2', 'D4', 'D1', 'D2', 'E8', 'P6', 'M2', \n",
    "#                 'D8', 'M9', 'P8', 'P7', 'S12', 'P13', 'V9', 'D5', 'P1', 'S8']\n",
    "\n",
    "# print(\"Creating advanced features for training data...\")\n",
    "\n",
    "# # CORRECT: Create DataFrame with date_id + features but WITHOUT target columns to prevent data leakage\n",
    "# # date_id is present because we need it for time-based operations in create_advanced_features()\n",
    "# columns_to_exclude = [\"market_forward_excess_returns\", \"forward_returns\", \"risk_free_rate\"]\n",
    "# columns_to_include = ['date_id'] + [col for col in final_features if col in train_full.columns]\n",
    "\n",
    "# train_for_engineering = train_full[columns_to_include].copy()\n",
    "\n",
    "# print(f\"Columns for feature engineering: {len(columns_to_include)}\")\n",
    "# print(f\"Excluded columns (prevent leakage): {columns_to_exclude}\")\n",
    "\n",
    "# train_enh = create_advanced_features(\n",
    "#     train_for_engineering,\n",
    "#     top_features=top_features,  # Now with correct case\n",
    "#     window_sizes=(5, 10, 20, 60, 120),\n",
    "#     shift=1\n",
    "# )\n",
    "\n",
    "# # Add target back AFTER feature engineering for supervised selection\n",
    "# train_enh[TARGET] = train_full[TARGET].values\n",
    "\n",
    "# # Now do supervised feature selection with target present\n",
    "# # Get only the engineered feature columns (exclude date_id and target)\n",
    "# feature_columns = [c for c in train_enh.columns if c not in ['date_id', TARGET]]\n",
    "# # print feature_columns\n",
    "# print(f\"Feature columns for selection: {feature_columns}\")\n",
    "\n",
    "# # Supervised feature selection\n",
    "# X_features = train_enh[feature_columns]\n",
    "# y_target = train_enh[TARGET]\n",
    "\n",
    "# # Remove zero variance features\n",
    "# vt = VarianceThreshold(threshold=1e-6)\n",
    "# X_filtered = X_features.loc[:, vt.fit(X_features).get_support()]\n",
    "\n",
    "# # Tree-based feature importance\n",
    "# gb = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "# gb.fit(X_filtered, y_target)\n",
    "\n",
    "# # Select top 50 features\n",
    "# importance_scores = pd.Series(gb.feature_importances_, index=X_filtered.columns)\n",
    "# selected_features = importance_scores.nlargest(50).index.tolist()\n",
    "\n",
    "# # Separate engineered features from original base features\n",
    "# original_features_in_selection = [f for f in selected_features if f in final_features]\n",
    "# new_engineered_features = [f for f in selected_features if f not in final_features]\n",
    "\n",
    "# print(f\"\\nFeature Engineering Results:\")\n",
    "# print(f\"Original base features available: {len(final_features)}\")\n",
    "# print(f\"Original features selected: {len(original_features_in_selection)}\")\n",
    "# print(f\"New engineered features created: {len(new_engineered_features)}\")\n",
    "# print(f\"Total features for modeling: {len(selected_features)}\")\n",
    "\n",
    "# print(f\"\\nNew engineered features added:\")\n",
    "# for i, feat in enumerate(new_engineered_features, 1):\n",
    "#     print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "# print(f\"\\nAll 50 selected features:\")\n",
    "# for i, feat in enumerate(selected_features, 1):\n",
    "#     feat_type = \"ORIGINAL\" if feat in final_features else \"ENGINEERED\"\n",
    "#     print(f\"{i:2d}. {feat:<25} [{feat_type}]\")\n",
    "\n",
    "# # Final feature matrices\n",
    "# X = train_enh[selected_features].astype('float32')\n",
    "# y = train_enh[TARGET].astype('float32')\n",
    "\n",
    "# print(f\"\\nFinal Training Data Shapes:\")\n",
    "# print(f\"Training set shape: {X.shape}\")\n",
    "# print(f\"Target shape: {y.shape}\")\n",
    "# print(f\"Features selected: {len(selected_features)}\")\n",
    "\n",
    "# # Store for later use in inference\n",
    "# final_selected_features = selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b277eef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127f5c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced features for training data...\n",
      "Columns for feature engineering (count): 95\n",
      "Included columns names: ['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1', 'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19', 'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9']\n",
      "Excluded columns (count): 3\n",
      "Excluded columns (prevent leakage) names: ['market_forward_excess_returns', 'forward_returns', 'risk_free_rate']\n",
      "Creating Level 1 features (Core)...\n",
      "Creating Level 2 features (Macro)...\n",
      "Macro columns for Level 2 features: ['M1', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'M4_rolling_mean_5', 'M4_rolling_std_5', 'M4_rolling_median_5', 'M4_rolling_max_5', 'M4_rolling_min_5', 'M4_dist_to_rolling_mean_5', 'M4_rolling_mean_10', 'M4_rolling_std_10', 'M4_rolling_median_10', 'M4_rolling_max_10', 'M4_rolling_min_10', 'M4_dist_to_rolling_mean_10', 'M4_rolling_mean_20', 'M4_rolling_std_20', 'M4_rolling_median_20', 'M4_rolling_max_20', 'M4_rolling_min_20', 'M4_dist_to_rolling_mean_20', 'M4_rolling_mean_60', 'M4_rolling_std_60', 'M4_rolling_median_60', 'M4_rolling_max_60', 'M4_rolling_min_60', 'M4_dist_to_rolling_mean_60', 'M4_rolling_mean_120', 'M4_rolling_std_120', 'M4_rolling_median_120', 'M4_rolling_max_120', 'M4_rolling_min_120', 'M4_dist_to_rolling_mean_120', 'V13_rolling_mean_5', 'V13_rolling_std_5', 'V13_rolling_median_5', 'V13_rolling_max_5', 'V13_rolling_min_5', 'V13_dist_to_rolling_mean_5', 'V13_rolling_mean_10', 'V13_rolling_std_10', 'V13_rolling_median_10', 'V13_rolling_max_10', 'V13_rolling_min_10', 'V13_dist_to_rolling_mean_10', 'V13_rolling_mean_20', 'V13_rolling_std_20', 'V13_rolling_median_20', 'V13_rolling_max_20', 'V13_rolling_min_20', 'V13_dist_to_rolling_mean_20', 'V13_rolling_mean_60', 'V13_rolling_std_60', 'V13_rolling_median_60', 'V13_rolling_max_60', 'V13_rolling_min_60', 'V13_dist_to_rolling_mean_60', 'V13_rolling_mean_120', 'V13_rolling_std_120', 'V13_rolling_median_120', 'V13_rolling_max_120', 'V13_rolling_min_120', 'V13_dist_to_rolling_mean_120', 'M11_rolling_mean_5', 'M11_rolling_std_5', 'M11_rolling_median_5', 'M11_rolling_max_5', 'M11_rolling_min_5', 'M11_dist_to_rolling_mean_5', 'M11_rolling_mean_10', 'M11_rolling_std_10', 'M11_rolling_median_10', 'M11_rolling_max_10', 'M11_rolling_min_10', 'M11_dist_to_rolling_mean_10', 'M11_rolling_mean_20', 'M11_rolling_std_20', 'M11_rolling_median_20', 'M11_rolling_max_20', 'M11_rolling_min_20', 'M11_dist_to_rolling_mean_20', 'M11_rolling_mean_60', 'M11_rolling_std_60', 'M11_rolling_median_60', 'M11_rolling_max_60', 'M11_rolling_min_60', 'M11_dist_to_rolling_mean_60', 'M11_rolling_mean_120', 'M11_rolling_std_120', 'M11_rolling_median_120', 'M11_rolling_max_120', 'M11_rolling_min_120', 'M11_dist_to_rolling_mean_120', 'S2_rolling_mean_5', 'S2_rolling_std_5', 'S2_rolling_median_5', 'S2_rolling_max_5', 'S2_rolling_min_5', 'S2_dist_to_rolling_mean_5', 'S2_rolling_mean_10', 'S2_rolling_std_10', 'S2_rolling_median_10', 'S2_rolling_max_10', 'S2_rolling_min_10', 'S2_dist_to_rolling_mean_10', 'S2_rolling_mean_20', 'S2_rolling_std_20', 'S2_rolling_median_20', 'S2_rolling_max_20', 'S2_rolling_min_20', 'S2_dist_to_rolling_mean_20', 'S2_rolling_mean_60', 'S2_rolling_std_60', 'S2_rolling_median_60', 'S2_rolling_max_60', 'S2_rolling_min_60', 'S2_dist_to_rolling_mean_60', 'S2_rolling_mean_120', 'S2_rolling_std_120', 'S2_rolling_median_120', 'S2_rolling_max_120', 'S2_rolling_min_120', 'S2_dist_to_rolling_mean_120', 'P6_rolling_mean_5', 'P6_rolling_std_5', 'P6_rolling_median_5', 'P6_rolling_max_5', 'P6_rolling_min_5', 'P6_dist_to_rolling_mean_5', 'P6_rolling_mean_10', 'P6_rolling_std_10', 'P6_rolling_median_10', 'P6_rolling_max_10', 'P6_rolling_min_10', 'P6_dist_to_rolling_mean_10', 'P6_rolling_mean_20', 'P6_rolling_std_20', 'P6_rolling_median_20', 'P6_rolling_max_20', 'P6_rolling_min_20', 'P6_dist_to_rolling_mean_20', 'P6_rolling_mean_60', 'P6_rolling_std_60', 'P6_rolling_median_60', 'P6_rolling_max_60', 'P6_rolling_min_60', 'P6_dist_to_rolling_mean_60', 'P6_rolling_mean_120', 'P6_rolling_std_120', 'P6_rolling_median_120', 'P6_rolling_max_120', 'P6_rolling_min_120', 'P6_dist_to_rolling_mean_120', 'M2_rolling_mean_5', 'M2_rolling_std_5', 'M2_rolling_median_5', 'M2_rolling_max_5', 'M2_rolling_min_5', 'M2_dist_to_rolling_mean_5', 'M2_rolling_mean_10', 'M2_rolling_std_10', 'M2_rolling_median_10', 'M2_rolling_max_10', 'M2_rolling_min_10', 'M2_dist_to_rolling_mean_10', 'M2_rolling_mean_20', 'M2_rolling_std_20', 'M2_rolling_median_20', 'M2_rolling_max_20', 'M2_rolling_min_20', 'M2_dist_to_rolling_mean_20', 'M2_rolling_mean_60', 'M2_rolling_std_60', 'M2_rolling_median_60', 'M2_rolling_max_60', 'M2_rolling_min_60', 'M2_dist_to_rolling_mean_60', 'M2_rolling_mean_120', 'M2_rolling_std_120', 'M2_rolling_median_120', 'M2_rolling_max_120', 'M2_rolling_min_120', 'M2_dist_to_rolling_mean_120', 'M9_rolling_mean_5', 'M9_rolling_std_5', 'M9_rolling_median_5', 'M9_rolling_max_5', 'M9_rolling_min_5', 'M9_dist_to_rolling_mean_5', 'M9_rolling_mean_10', 'M9_rolling_std_10', 'M9_rolling_median_10', 'M9_rolling_max_10', 'M9_rolling_min_10', 'M9_dist_to_rolling_mean_10', 'M9_rolling_mean_20', 'M9_rolling_std_20', 'M9_rolling_median_20', 'M9_rolling_max_20', 'M9_rolling_min_20', 'M9_dist_to_rolling_mean_20', 'M9_rolling_mean_60', 'M9_rolling_std_60', 'M9_rolling_median_60', 'M9_rolling_max_60', 'M9_rolling_min_60', 'M9_dist_to_rolling_mean_60', 'M9_rolling_mean_120', 'M9_rolling_std_120', 'M9_rolling_median_120', 'M9_rolling_max_120', 'M9_rolling_min_120', 'M9_dist_to_rolling_mean_120', 'P8_rolling_mean_5', 'P8_rolling_std_5', 'P8_rolling_median_5', 'P8_rolling_max_5', 'P8_rolling_min_5', 'P8_dist_to_rolling_mean_5', 'P8_rolling_mean_10', 'P8_rolling_std_10', 'P8_rolling_median_10', 'P8_rolling_max_10', 'P8_rolling_min_10', 'P8_dist_to_rolling_mean_10', 'P8_rolling_mean_20', 'P8_rolling_std_20', 'P8_rolling_median_20', 'P8_rolling_max_20', 'P8_rolling_min_20', 'P8_dist_to_rolling_mean_20', 'P8_rolling_mean_60', 'P8_rolling_std_60', 'P8_rolling_median_60', 'P8_rolling_max_60', 'P8_rolling_min_60', 'P8_dist_to_rolling_mean_60', 'P8_rolling_mean_120', 'P8_rolling_std_120', 'P8_rolling_median_120', 'P8_rolling_max_120', 'P8_rolling_min_120', 'P8_dist_to_rolling_mean_120', 'P7_rolling_mean_5', 'P7_rolling_std_5', 'P7_rolling_median_5', 'P7_rolling_max_5', 'P7_rolling_min_5', 'P7_dist_to_rolling_mean_5', 'P7_rolling_mean_10', 'P7_rolling_std_10', 'P7_rolling_median_10', 'P7_rolling_max_10', 'P7_rolling_min_10', 'P7_dist_to_rolling_mean_10', 'P7_rolling_mean_20', 'P7_rolling_std_20', 'P7_rolling_median_20', 'P7_rolling_max_20', 'P7_rolling_min_20', 'P7_dist_to_rolling_mean_20', 'P7_rolling_mean_60', 'P7_rolling_std_60', 'P7_rolling_median_60', 'P7_rolling_max_60', 'P7_rolling_min_60', 'P7_dist_to_rolling_mean_60', 'P7_rolling_mean_120', 'P7_rolling_std_120', 'P7_rolling_median_120', 'P7_rolling_max_120', 'P7_rolling_min_120', 'P7_dist_to_rolling_mean_120', 'S12_rolling_mean_5', 'S12_rolling_std_5', 'S12_rolling_median_5', 'S12_rolling_max_5', 'S12_rolling_min_5', 'S12_dist_to_rolling_mean_5', 'S12_rolling_mean_10', 'S12_rolling_std_10', 'S12_rolling_median_10', 'S12_rolling_max_10', 'S12_rolling_min_10', 'S12_dist_to_rolling_mean_10', 'S12_rolling_mean_20', 'S12_rolling_std_20', 'S12_rolling_median_20', 'S12_rolling_max_20', 'S12_rolling_min_20', 'S12_dist_to_rolling_mean_20', 'S12_rolling_mean_60', 'S12_rolling_std_60', 'S12_rolling_median_60', 'S12_rolling_max_60', 'S12_rolling_min_60', 'S12_dist_to_rolling_mean_60', 'S12_rolling_mean_120', 'S12_rolling_std_120', 'S12_rolling_median_120', 'S12_rolling_max_120', 'S12_rolling_min_120', 'S12_dist_to_rolling_mean_120', 'P13_rolling_mean_5', 'P13_rolling_std_5', 'P13_rolling_median_5', 'P13_rolling_max_5', 'P13_rolling_min_5', 'P13_dist_to_rolling_mean_5', 'P13_rolling_mean_10', 'P13_rolling_std_10', 'P13_rolling_median_10', 'P13_rolling_max_10', 'P13_rolling_min_10', 'P13_dist_to_rolling_mean_10', 'P13_rolling_mean_20', 'P13_rolling_std_20', 'P13_rolling_median_20', 'P13_rolling_max_20', 'P13_rolling_min_20', 'P13_dist_to_rolling_mean_20', 'P13_rolling_mean_60', 'P13_rolling_std_60', 'P13_rolling_median_60', 'P13_rolling_max_60', 'P13_rolling_min_60', 'P13_dist_to_rolling_mean_60', 'P13_rolling_mean_120', 'P13_rolling_std_120', 'P13_rolling_median_120', 'P13_rolling_max_120', 'P13_rolling_min_120', 'P13_dist_to_rolling_mean_120', 'V9_rolling_mean_5', 'V9_rolling_std_5', 'V9_rolling_median_5', 'V9_rolling_max_5', 'V9_rolling_min_5', 'V9_dist_to_rolling_mean_5', 'V9_rolling_mean_10', 'V9_rolling_std_10', 'V9_rolling_median_10', 'V9_rolling_max_10', 'V9_rolling_min_10', 'V9_dist_to_rolling_mean_10', 'V9_rolling_mean_20', 'V9_rolling_std_20', 'V9_rolling_median_20', 'V9_rolling_max_20', 'V9_rolling_min_20', 'V9_dist_to_rolling_mean_20', 'V9_rolling_mean_60', 'V9_rolling_std_60', 'V9_rolling_median_60', 'V9_rolling_max_60', 'V9_rolling_min_60', 'V9_dist_to_rolling_mean_60', 'V9_rolling_mean_120', 'V9_rolling_std_120', 'V9_rolling_median_120', 'V9_rolling_max_120', 'V9_rolling_min_120', 'V9_dist_to_rolling_mean_120', 'P1_rolling_mean_5', 'P1_rolling_std_5', 'P1_rolling_median_5', 'P1_rolling_max_5', 'P1_rolling_min_5', 'P1_dist_to_rolling_mean_5', 'P1_rolling_mean_10', 'P1_rolling_std_10', 'P1_rolling_median_10', 'P1_rolling_max_10', 'P1_rolling_min_10', 'P1_dist_to_rolling_mean_10', 'P1_rolling_mean_20', 'P1_rolling_std_20', 'P1_rolling_median_20', 'P1_rolling_max_20', 'P1_rolling_min_20', 'P1_dist_to_rolling_mean_20', 'P1_rolling_mean_60', 'P1_rolling_std_60', 'P1_rolling_median_60', 'P1_rolling_max_60', 'P1_rolling_min_60', 'P1_dist_to_rolling_mean_60', 'P1_rolling_mean_120', 'P1_rolling_std_120', 'P1_rolling_median_120', 'P1_rolling_max_120', 'P1_rolling_min_120', 'P1_dist_to_rolling_mean_120', 'S8_rolling_mean_5', 'S8_rolling_std_5', 'S8_rolling_median_5', 'S8_rolling_max_5', 'S8_rolling_min_5', 'S8_dist_to_rolling_mean_5', 'S8_rolling_mean_10', 'S8_rolling_std_10', 'S8_rolling_median_10', 'S8_rolling_max_10', 'S8_rolling_min_10', 'S8_dist_to_rolling_mean_10', 'S8_rolling_mean_20', 'S8_rolling_std_20', 'S8_rolling_median_20', 'S8_rolling_max_20', 'S8_rolling_min_20', 'S8_dist_to_rolling_mean_20', 'S8_rolling_mean_60', 'S8_rolling_std_60', 'S8_rolling_median_60', 'S8_rolling_max_60', 'S8_rolling_min_60', 'S8_dist_to_rolling_mean_60', 'S8_rolling_mean_120', 'S8_rolling_std_120', 'S8_rolling_median_120', 'S8_rolling_max_120', 'S8_rolling_min_120', 'S8_dist_to_rolling_mean_120', 'M4_rolling_var_5', 'M4_rolling_var_10', 'M4_rolling_var_20', 'M4_rolling_var_60', 'M4_rolling_var_120', 'V13_rolling_var_5', 'V13_rolling_var_10', 'V13_rolling_var_20', 'V13_rolling_var_60', 'V13_rolling_var_120', 'M11_rolling_var_5', 'M11_rolling_var_10', 'M11_rolling_var_20', 'M11_rolling_var_60', 'M11_rolling_var_120', 'S2_rolling_var_5', 'S2_rolling_var_10', 'S2_rolling_var_20', 'S2_rolling_var_60', 'S2_rolling_var_120', 'P6_rolling_var_5', 'P6_rolling_var_10', 'P6_rolling_var_20', 'P6_rolling_var_60', 'P6_rolling_var_120', 'M2_rolling_var_5', 'M2_rolling_var_10', 'M2_rolling_var_20', 'M2_rolling_var_60', 'M2_rolling_var_120', 'M9_rolling_var_5', 'M9_rolling_var_10', 'M9_rolling_var_20', 'M9_rolling_var_60', 'M9_rolling_var_120', 'P8_rolling_var_5', 'P8_rolling_var_10', 'P8_rolling_var_20', 'P8_rolling_var_60', 'P8_rolling_var_120', 'P7_rolling_var_5', 'P7_rolling_var_10', 'P7_rolling_var_20', 'P7_rolling_var_60', 'P7_rolling_var_120', 'S12_rolling_var_5', 'S12_rolling_var_10', 'S12_rolling_var_20', 'S12_rolling_var_60', 'S12_rolling_var_120', 'P13_rolling_var_5', 'P13_rolling_var_10', 'P13_rolling_var_20', 'P13_rolling_var_60', 'P13_rolling_var_120', 'V9_rolling_var_5', 'V9_rolling_var_10', 'V9_rolling_var_20', 'V9_rolling_var_60', 'V9_rolling_var_120', 'P1_rolling_var_5', 'P1_rolling_var_10', 'P1_rolling_var_20', 'P1_rolling_var_60', 'P1_rolling_var_120', 'S8_rolling_var_5', 'S8_rolling_var_10', 'S8_rolling_var_20', 'S8_rolling_var_60', 'S8_rolling_var_120', 'M4_z_5', 'M4_z_10', 'M4_z_20', 'M4_z_60', 'M4_z_120', 'V13_z_5', 'V13_z_10', 'V13_z_20', 'V13_z_60', 'V13_z_120', 'M11_z_5', 'M11_z_10', 'M11_z_20', 'M11_z_60', 'M11_z_120', 'S2_z_5', 'S2_z_10', 'S2_z_20', 'S2_z_60', 'S2_z_120', 'P6_z_5', 'P6_z_10', 'P6_z_20', 'P6_z_60', 'P6_z_120', 'M2_z_5', 'M2_z_10', 'M2_z_20', 'M2_z_60', 'M2_z_120', 'M9_z_5', 'M9_z_10', 'M9_z_20', 'M9_z_60', 'M9_z_120', 'P8_z_5', 'P8_z_10', 'P8_z_20', 'P8_z_60', 'P8_z_120', 'P7_z_5', 'P7_z_10', 'P7_z_20', 'P7_z_60', 'P7_z_120', 'S12_z_5', 'S12_z_10', 'S12_z_20', 'S12_z_60', 'S12_z_120', 'P13_z_5', 'P13_z_10', 'P13_z_20', 'P13_z_60', 'P13_z_120', 'V9_z_5', 'V9_z_10', 'V9_z_20', 'V9_z_60', 'V9_z_120', 'P1_z_5', 'P1_z_10', 'P1_z_20', 'P1_z_60', 'P1_z_120', 'S8_z_5', 'S8_z_10', 'S8_z_20', 'S8_z_60', 'S8_z_120', 'M4_diff_1', 'M4_pctchg_1', 'V13_diff_1', 'V13_pctchg_1', 'M11_diff_1', 'M11_pctchg_1', 'S2_diff_1', 'S2_pctchg_1', 'P6_diff_1', 'P6_pctchg_1', 'M2_diff_1', 'M2_pctchg_1', 'M9_diff_1', 'M9_pctchg_1', 'P8_diff_1', 'P8_pctchg_1', 'P7_diff_1', 'P7_pctchg_1', 'S12_diff_1', 'S12_pctchg_1', 'P13_diff_1', 'P13_pctchg_1', 'V9_diff_1', 'V9_pctchg_1', 'P1_diff_1', 'P1_pctchg_1', 'S8_diff_1', 'S8_pctchg_1', 'M4_pacf_1', 'M4_pacf_2', 'M4_pacf_3', 'M4_pacf_4', 'M4_pacf_5', 'M4_pacf_6', 'M4_pacf_7', 'M4_pacf_8', 'M4_pacf_9', 'M4_pacf_10', 'V13_pacf_1', 'V13_pacf_2', 'V13_pacf_3', 'V13_pacf_4', 'V13_pacf_5', 'V13_pacf_6', 'V13_pacf_7', 'V13_pacf_8', 'V13_pacf_9', 'V13_pacf_10', 'M11_pacf_1', 'M11_pacf_2', 'M11_pacf_3', 'M11_pacf_4', 'M11_pacf_5', 'M11_pacf_6', 'M11_pacf_7', 'M11_pacf_8', 'M11_pacf_9', 'M11_pacf_10', 'S2_pacf_1', 'S2_pacf_2', 'S2_pacf_3', 'S2_pacf_4', 'S2_pacf_5', 'S2_pacf_6', 'S2_pacf_7', 'S2_pacf_8', 'S2_pacf_9', 'S2_pacf_10', 'P6_pacf_1', 'P6_pacf_2', 'P6_pacf_3', 'P6_pacf_4', 'P6_pacf_5', 'P6_pacf_6', 'P6_pacf_7', 'P6_pacf_8', 'P6_pacf_9', 'P6_pacf_10', 'M2_pacf_1', 'M2_pacf_2', 'M2_pacf_3', 'M2_pacf_4', 'M2_pacf_5', 'M2_pacf_6', 'M2_pacf_7', 'M2_pacf_8', 'M2_pacf_9', 'M2_pacf_10', 'M9_pacf_1', 'M9_pacf_2', 'M9_pacf_3', 'M9_pacf_4', 'M9_pacf_5', 'M9_pacf_6', 'M9_pacf_7', 'M9_pacf_8', 'M9_pacf_9', 'M9_pacf_10', 'P8_pacf_1', 'P8_pacf_2', 'P8_pacf_3', 'P8_pacf_4', 'P8_pacf_5', 'P8_pacf_6', 'P8_pacf_7', 'P8_pacf_8', 'P8_pacf_9', 'P8_pacf_10', 'P7_pacf_1', 'P7_pacf_2', 'P7_pacf_3', 'P7_pacf_4', 'P7_pacf_5', 'P7_pacf_6', 'P7_pacf_7', 'P7_pacf_8', 'P7_pacf_9', 'P7_pacf_10', 'S12_pacf_1', 'S12_pacf_2', 'S12_pacf_3', 'S12_pacf_4', 'S12_pacf_5', 'S12_pacf_6', 'S12_pacf_7', 'S12_pacf_8', 'S12_pacf_9', 'S12_pacf_10', 'P13_pacf_1', 'P13_pacf_2', 'P13_pacf_3', 'P13_pacf_4', 'P13_pacf_5', 'P13_pacf_6', 'P13_pacf_7', 'P13_pacf_8', 'P13_pacf_9', 'P13_pacf_10', 'V9_pacf_1', 'V9_pacf_2', 'V9_pacf_3', 'V9_pacf_4', 'V9_pacf_5', 'V9_pacf_6', 'V9_pacf_7', 'V9_pacf_8', 'V9_pacf_9', 'V9_pacf_10', 'P1_pacf_1', 'P1_pacf_2', 'P1_pacf_3', 'P1_pacf_4', 'P1_pacf_5', 'P1_pacf_6', 'P1_pacf_7', 'P1_pacf_8', 'P1_pacf_9', 'P1_pacf_10', 'S8_pacf_1', 'S8_pacf_2', 'S8_pacf_3', 'S8_pacf_4', 'S8_pacf_5', 'S8_pacf_6', 'S8_pacf_7', 'S8_pacf_8', 'S8_pacf_9', 'S8_pacf_10', 'M4_acf_1', 'M4_acf_2', 'M4_acf_3', 'M4_acf_4', 'M4_acf_5', 'M4_acf_6', 'M4_acf_7', 'M4_acf_8', 'M4_acf_9', 'M4_acf_10', 'V13_acf_1', 'V13_acf_2', 'V13_acf_3', 'V13_acf_4', 'V13_acf_5', 'V13_acf_6', 'V13_acf_7', 'V13_acf_8', 'V13_acf_9', 'V13_acf_10', 'M11_acf_1', 'M11_acf_2', 'M11_acf_3', 'M11_acf_4', 'M11_acf_5', 'M11_acf_6', 'M11_acf_7', 'M11_acf_8', 'M11_acf_9', 'M11_acf_10', 'S2_acf_1', 'S2_acf_2', 'S2_acf_3', 'S2_acf_4', 'S2_acf_5', 'S2_acf_6', 'S2_acf_7', 'S2_acf_8', 'S2_acf_9', 'S2_acf_10', 'P6_acf_1', 'P6_acf_2', 'P6_acf_3', 'P6_acf_4', 'P6_acf_5', 'P6_acf_6', 'P6_acf_7', 'P6_acf_8', 'P6_acf_9', 'P6_acf_10', 'M2_acf_1', 'M2_acf_2', 'M2_acf_3', 'M2_acf_4', 'M2_acf_5', 'M2_acf_6', 'M2_acf_7', 'M2_acf_8', 'M2_acf_9', 'M2_acf_10', 'M9_acf_1', 'M9_acf_2', 'M9_acf_3', 'M9_acf_4', 'M9_acf_5', 'M9_acf_6', 'M9_acf_7', 'M9_acf_8', 'M9_acf_9', 'M9_acf_10', 'P8_acf_1', 'P8_acf_2', 'P8_acf_3', 'P8_acf_4', 'P8_acf_5', 'P8_acf_6', 'P8_acf_7', 'P8_acf_8', 'P8_acf_9', 'P8_acf_10', 'P7_acf_1', 'P7_acf_2', 'P7_acf_3', 'P7_acf_4', 'P7_acf_5', 'P7_acf_6', 'P7_acf_7', 'P7_acf_8', 'P7_acf_9', 'P7_acf_10', 'S12_acf_1', 'S12_acf_2', 'S12_acf_3', 'S12_acf_4', 'S12_acf_5', 'S12_acf_6', 'S12_acf_7', 'S12_acf_8', 'S12_acf_9', 'S12_acf_10', 'P13_acf_1', 'P13_acf_2', 'P13_acf_3', 'P13_acf_4', 'P13_acf_5', 'P13_acf_6', 'P13_acf_7', 'P13_acf_8', 'P13_acf_9', 'P13_acf_10', 'V9_acf_1', 'V9_acf_2', 'V9_acf_3', 'V9_acf_4', 'V9_acf_5', 'V9_acf_6', 'V9_acf_7', 'V9_acf_8', 'V9_acf_9', 'V9_acf_10', 'P1_acf_1', 'P1_acf_2', 'P1_acf_3', 'P1_acf_4', 'P1_acf_5', 'P1_acf_6', 'P1_acf_7', 'P1_acf_8', 'P1_acf_9', 'P1_acf_10', 'S8_acf_1', 'S8_acf_2', 'S8_acf_3', 'S8_acf_4', 'S8_acf_5', 'S8_acf_6', 'S8_acf_7', 'S8_acf_8', 'S8_acf_9', 'S8_acf_10', 'M4_autocorr_1', 'M4_autocorr_2', 'M4_autocorr_3', 'M4_autocorr_4', 'M4_autocorr_5', 'M4_autocorr_6', 'M4_autocorr_7', 'M4_autocorr_8', 'M4_autocorr_9', 'M4_autocorr_10', 'V13_autocorr_1', 'V13_autocorr_2', 'V13_autocorr_3', 'V13_autocorr_4', 'V13_autocorr_5', 'V13_autocorr_6', 'V13_autocorr_7', 'V13_autocorr_8', 'V13_autocorr_9', 'V13_autocorr_10', 'M11_autocorr_1', 'M11_autocorr_2', 'M11_autocorr_3', 'M11_autocorr_4', 'M11_autocorr_5', 'M11_autocorr_6', 'M11_autocorr_7', 'M11_autocorr_8', 'M11_autocorr_9', 'M11_autocorr_10', 'S2_autocorr_1', 'S2_autocorr_2', 'S2_autocorr_3', 'S2_autocorr_4', 'S2_autocorr_5', 'S2_autocorr_6', 'S2_autocorr_7', 'S2_autocorr_8', 'S2_autocorr_9', 'S2_autocorr_10', 'P6_autocorr_1', 'P6_autocorr_2', 'P6_autocorr_3', 'P6_autocorr_4', 'P6_autocorr_5', 'P6_autocorr_6', 'P6_autocorr_7', 'P6_autocorr_8', 'P6_autocorr_9', 'P6_autocorr_10', 'M2_autocorr_1', 'M2_autocorr_2', 'M2_autocorr_3', 'M2_autocorr_4', 'M2_autocorr_5', 'M2_autocorr_6', 'M2_autocorr_7', 'M2_autocorr_8', 'M2_autocorr_9', 'M2_autocorr_10', 'M9_autocorr_1', 'M9_autocorr_2', 'M9_autocorr_3', 'M9_autocorr_4', 'M9_autocorr_5', 'M9_autocorr_6', 'M9_autocorr_7', 'M9_autocorr_8', 'M9_autocorr_9', 'M9_autocorr_10', 'P8_autocorr_1', 'P8_autocorr_2', 'P8_autocorr_3', 'P8_autocorr_4', 'P8_autocorr_5', 'P8_autocorr_6', 'P8_autocorr_7', 'P8_autocorr_8', 'P8_autocorr_9', 'P8_autocorr_10', 'P7_autocorr_1', 'P7_autocorr_2', 'P7_autocorr_3', 'P7_autocorr_4', 'P7_autocorr_5', 'P7_autocorr_6', 'P7_autocorr_7', 'P7_autocorr_8', 'P7_autocorr_9', 'P7_autocorr_10', 'S12_autocorr_1', 'S12_autocorr_2', 'S12_autocorr_3', 'S12_autocorr_4', 'S12_autocorr_5', 'S12_autocorr_6', 'S12_autocorr_7', 'S12_autocorr_8', 'S12_autocorr_9', 'S12_autocorr_10', 'P13_autocorr_1', 'P13_autocorr_2', 'P13_autocorr_3', 'P13_autocorr_4', 'P13_autocorr_5', 'P13_autocorr_6', 'P13_autocorr_7', 'P13_autocorr_8', 'P13_autocorr_9', 'P13_autocorr_10', 'V9_autocorr_1', 'V9_autocorr_2', 'V9_autocorr_3', 'V9_autocorr_4', 'V9_autocorr_5', 'V9_autocorr_6', 'V9_autocorr_7', 'V9_autocorr_8', 'V9_autocorr_9', 'V9_autocorr_10', 'P1_autocorr_1', 'P1_autocorr_2', 'P1_autocorr_3', 'P1_autocorr_4', 'P1_autocorr_5', 'P1_autocorr_6', 'P1_autocorr_7', 'P1_autocorr_8', 'P1_autocorr_9', 'P1_autocorr_10', 'S8_autocorr_1', 'S8_autocorr_2', 'S8_autocorr_3', 'S8_autocorr_4', 'S8_autocorr_5', 'S8_autocorr_6', 'S8_autocorr_7', 'S8_autocorr_8', 'S8_autocorr_9', 'S8_autocorr_10', 'M4_skewness', 'M4_kurtosis', 'V13_skewness', 'V13_kurtosis', 'M11_skewness', 'M11_kurtosis', 'S2_skewness', 'S2_kurtosis', 'P6_skewness', 'P6_kurtosis', 'M2_skewness', 'M2_kurtosis', 'M9_skewness', 'M9_kurtosis', 'P8_skewness', 'P8_kurtosis', 'P7_skewness', 'P7_kurtosis', 'S12_skewness', 'S12_kurtosis', 'P13_skewness', 'P13_kurtosis', 'V9_skewness', 'V9_kurtosis', 'P1_skewness', 'P1_kurtosis', 'S8_skewness', 'S8_kurtosis', 'M4_momentum_5', 'M4_momentum_10', 'M4_momentum_20', 'V13_momentum_5', 'V13_momentum_10', 'V13_momentum_20', 'M11_momentum_5', 'M11_momentum_10', 'M11_momentum_20', 'S2_momentum_5', 'S2_momentum_10', 'S2_momentum_20', 'P6_momentum_5', 'P6_momentum_10', 'P6_momentum_20', 'M2_momentum_5', 'M2_momentum_10', 'M2_momentum_20', 'M9_momentum_5', 'M9_momentum_10', 'M9_momentum_20', 'P8_momentum_5', 'P8_momentum_10', 'P8_momentum_20', 'P7_momentum_5', 'P7_momentum_10', 'P7_momentum_20', 'S12_momentum_5', 'S12_momentum_10', 'S12_momentum_20', 'P13_momentum_5', 'P13_momentum_10', 'P13_momentum_20', 'V9_momentum_5', 'V9_momentum_10', 'V9_momentum_20', 'P1_momentum_5', 'P1_momentum_10', 'P1_momentum_20', 'S8_momentum_5', 'S8_momentum_10', 'S8_momentum_20', 'M4_dist_to_momentum_5', 'M4_dist_to_momentum_10', 'M4_dist_to_momentum_20', 'V13_dist_to_momentum_5', 'V13_dist_to_momentum_10', 'V13_dist_to_momentum_20', 'M11_dist_to_momentum_5', 'M11_dist_to_momentum_10', 'M11_dist_to_momentum_20', 'S2_dist_to_momentum_5', 'S2_dist_to_momentum_10', 'S2_dist_to_momentum_20', 'P6_dist_to_momentum_5', 'P6_dist_to_momentum_10', 'P6_dist_to_momentum_20', 'M2_dist_to_momentum_5', 'M2_dist_to_momentum_10', 'M2_dist_to_momentum_20', 'M9_dist_to_momentum_5', 'M9_dist_to_momentum_10', 'M9_dist_to_momentum_20', 'P8_dist_to_momentum_5', 'P8_dist_to_momentum_10', 'P8_dist_to_momentum_20', 'P7_dist_to_momentum_5', 'P7_dist_to_momentum_10', 'P7_dist_to_momentum_20', 'S12_dist_to_momentum_5', 'S12_dist_to_momentum_10', 'S12_dist_to_momentum_20', 'P13_dist_to_momentum_5', 'P13_dist_to_momentum_10', 'P13_dist_to_momentum_20', 'V9_dist_to_momentum_5', 'V9_dist_to_momentum_10', 'V9_dist_to_momentum_20', 'P1_dist_to_momentum_5', 'P1_dist_to_momentum_10', 'P1_dist_to_momentum_20', 'S8_dist_to_momentum_5', 'S8_dist_to_momentum_10', 'S8_dist_to_momentum_20', 'M4_diff_5', 'M4_diff_10', 'V13_diff_5', 'V13_diff_10', 'M11_diff_5', 'M11_diff_10', 'S2_diff_5', 'S2_diff_10', 'P6_diff_5', 'P6_diff_10', 'M2_diff_5', 'M2_diff_10', 'M9_diff_5', 'M9_diff_10', 'P8_diff_5', 'P8_diff_10', 'P7_diff_5', 'P7_diff_10', 'S12_diff_5', 'S12_diff_10', 'P13_diff_5', 'P13_diff_10', 'V9_diff_5', 'V9_diff_10', 'P1_diff_5', 'P1_diff_10', 'S8_diff_5', 'S8_diff_10', 'M4_normalized', 'V13_normalized', 'M11_normalized', 'S2_normalized', 'P6_normalized', 'M2_normalized', 'M9_normalized', 'P8_normalized', 'P7_normalized', 'S12_normalized', 'P13_normalized', 'V9_normalized', 'P1_normalized', 'S8_normalized', 'M4_macro_rolling_sum_5', 'M4_macro_rolling_sum_10', 'M4_macro_rolling_sum_20', 'V13_macro_rolling_sum_5', 'V13_macro_rolling_sum_10', 'V13_macro_rolling_sum_20', 'M11_macro_rolling_sum_5', 'M11_macro_rolling_sum_10', 'M11_macro_rolling_sum_20', 'S2_macro_rolling_sum_5', 'S2_macro_rolling_sum_10', 'S2_macro_rolling_sum_20', 'P6_macro_rolling_sum_5', 'P6_macro_rolling_sum_10', 'P6_macro_rolling_sum_20', 'M2_macro_rolling_sum_5', 'M2_macro_rolling_sum_10', 'M2_macro_rolling_sum_20', 'M9_macro_rolling_sum_5', 'M9_macro_rolling_sum_10', 'M9_macro_rolling_sum_20', 'P8_macro_rolling_sum_5', 'P8_macro_rolling_sum_10', 'P8_macro_rolling_sum_20', 'P7_macro_rolling_sum_5', 'P7_macro_rolling_sum_10', 'P7_macro_rolling_sum_20', 'S12_macro_rolling_sum_5', 'S12_macro_rolling_sum_10', 'S12_macro_rolling_sum_20', 'P13_macro_rolling_sum_5', 'P13_macro_rolling_sum_10', 'P13_macro_rolling_sum_20', 'V9_macro_rolling_sum_5', 'V9_macro_rolling_sum_10', 'V9_macro_rolling_sum_20', 'P1_macro_rolling_sum_5', 'P1_macro_rolling_sum_10', 'P1_macro_rolling_sum_20', 'S8_macro_rolling_sum_5', 'S8_macro_rolling_sum_10', 'S8_macro_rolling_sum_20', 'M4_cumsum', 'V13_cumsum', 'M11_cumsum', 'S2_cumsum', 'P6_cumsum', 'M2_cumsum', 'M9_cumsum', 'P8_cumsum', 'P7_cumsum', 'S12_cumsum', 'P13_cumsum', 'V9_cumsum', 'P1_cumsum', 'S8_cumsum', 'M4_hurst', 'V13_hurst', 'M11_hurst', 'S2_hurst', 'P6_hurst', 'M2_hurst', 'M9_hurst', 'P8_hurst', 'P7_hurst', 'S12_hurst', 'P13_hurst', 'V9_hurst', 'P1_hurst', 'S8_hurst', 'M4_lag_1', 'M4_lag_5', 'M4_lag_10', 'V13_lag_1', 'V13_lag_5', 'V13_lag_10', 'M11_lag_1', 'M11_lag_5', 'M11_lag_10', 'S2_lag_1', 'S2_lag_5', 'S2_lag_10', 'P6_lag_1', 'P6_lag_5', 'P6_lag_10', 'M2_lag_1', 'M2_lag_5', 'M2_lag_10', 'M9_lag_1', 'M9_lag_5', 'M9_lag_10', 'P8_lag_1', 'P8_lag_5', 'P8_lag_10', 'P7_lag_1', 'P7_lag_5', 'P7_lag_10', 'S12_lag_1', 'S12_lag_5', 'S12_lag_10', 'P13_lag_1', 'P13_lag_5', 'P13_lag_10', 'V9_lag_1', 'V9_lag_5', 'V9_lag_10', 'P1_lag_1', 'P1_lag_5', 'P1_lag_10', 'S8_lag_1', 'S8_lag_5', 'S8_lag_10']\n",
      "Cleaning and selecting features...\n",
      "Feature engineering complete. Created 1985 total columns.\n",
      "Feature columns for selection: 1984 total features available\n",
      "\n",
      "============================================================\n",
      "ENHANCED ENSEMBLE FEATURE SELECTION\n",
      "============================================================\n",
      "Using dynamic seed: 45260\n",
      "Features after variance filtering: 1334\n",
      "Method 1a: Gradient Boosting Feature Importance...\n",
      "Method 1b: Bagging Regressor Feature Importance...\n",
      "Ensemble Voting: Combining all methods...\n",
      "\n",
      "Ensemble Feature Selection Results:\n",
      "   Total unique features considered: 235\n",
      "   Selected by 4 methods: 0\n",
      "   Selected by 3 methods: 0\n",
      "   Selected by 2 methods: 65\n",
      "   Selected by 1 method:  170\n",
      "   Final ensemble selection: 150 features\n",
      "\n",
      "Feature Engineering Results:\n",
      "Original base features available: 94\n",
      "Original features selected: 18\n",
      "New engineered features created: 132\n",
      "Total features for modeling: 150\n",
      "\n",
      "New engineered features added:\n",
      " 1. V13_dist_to_rolling_mean_5\n",
      " 2. V13_diff_1\n",
      " 3. S8_normalized\n",
      " 4. V9_diff_1\n",
      " 5. M4_dist_to_rolling_mean_60\n",
      " 6. V13_momentum_5\n",
      " 7. M4_diff_5\n",
      " 8. V13_diff_5\n",
      " 9. V13_dist_to_momentum_20\n",
      "10. M11_diff_1\n",
      "11. M4_dist_to_rolling_mean_120\n",
      "12. M11_diff_5\n",
      "13. M4_lag_1\n",
      "14. M11_dist_to_rolling_mean_10\n",
      "15. V9_dist_to_rolling_mean_60\n",
      "16. M4_diff_10\n",
      "17. P7_rolling_median_5\n",
      "18. P13_pctchg_1\n",
      "19. V13_rolling_min_5\n",
      "20. macro_corr_M4_D8_30_macro_low_ratio_20\n",
      "21. M4_momentum_20\n",
      "22. V13_momentum_10\n",
      "23. P8_dist_to_rolling_mean_10\n",
      "24. M4_dist_to_rolling_mean_20\n",
      "25. P6_cumsum\n",
      "26. E8_momentum_20\n",
      "27. P8_pctchg_1\n",
      "28. V13_pctchg_1\n",
      "29. M4_momentum_10\n",
      "30. M11_z_120\n",
      "31. P7_normalized\n",
      "32. P6_dist_to_momentum_20\n",
      "33. V9_momentum_20\n",
      "34. E8_cumsum\n",
      "35. M11_dist_to_rolling_mean_5\n",
      "36. M4_rolling_median_20\n",
      "37. M4_rolling_mean_120\n",
      "38. M9_diff_5\n",
      "39. P13_momentum_5\n",
      "40. V9_momentum_10\n",
      "41. P7_lag_5\n",
      "42. M9_z_20\n",
      "43. P6_diff_5\n",
      "44. S2_pctchg_1\n",
      "45. P13_dist_to_momentum_20\n",
      "46. macro_corr_M4_P6_30_macro_high_ratio_60\n",
      "47. P6_dist_to_momentum_5\n",
      "48. M9_pctchg_1\n",
      "49. macro_corr_M4_S2_30_macro_high_ratio_20\n",
      "50. E8_dist_to_momentum_10\n",
      "51. P13_dist_to_momentum_10\n",
      "52. P7_diff_1\n",
      "53. P13_momentum_20\n",
      "54. M9_momentum_10\n",
      "55. P7_kurtosis\n",
      "56. M11_momentum_10\n",
      "57. P7_rolling_median_120\n",
      "58. P13_dist_to_momentum_5\n",
      "59. D4_cumsum\n",
      "60. V9_dist_to_rolling_mean_120\n",
      "61. M9_cumsum\n",
      "62. V13_lag_1\n",
      "63. P8_dist_to_rolling_mean_20\n",
      "64. S8_rolling_min_10\n",
      "65. M11_z_5\n",
      "66. macro_corr_M4_D8_30_macro_low_ratio_120\n",
      "67. P8_diff_5\n",
      "68. S2_z_5\n",
      "69. D4_rolling_mean_120\n",
      "70. V9_dist_to_rolling_mean_5\n",
      "71. S8_dist_to_rolling_mean_120\n",
      "72. M9_diff_1\n",
      "73. P6_pctchg_1\n",
      "74. D4_rolling_std_120\n",
      "75. S2_momentum_10\n",
      "76. D4_rolling_var_120\n",
      "77. M11_dist_to_momentum_5\n",
      "78. V9_diff_10\n",
      "79. M4_rolling_mean_60\n",
      "80. M4_z_60\n",
      "81. P6_dist_to_rolling_mean_60\n",
      "82. M11_pctchg_1\n",
      "83. M4_pctchg_1\n",
      "84. P7_lag_1\n",
      "85. V9_rolling_var_20\n",
      "86. S2_rolling_min_5\n",
      "87. M9_dist_to_rolling_mean_120\n",
      "88. M4_lag_5\n",
      "89. P13_diff_1\n",
      "90. M4_dist_to_momentum_5\n",
      "91. P6_rolling_mean_10\n",
      "92. M11_lag_5\n",
      "93. V13_dist_to_momentum_5\n",
      "94. S8_skewness\n",
      "95. V13_lag_5\n",
      "96. M4_lag_10\n",
      "97. P13_z_10\n",
      "98. macro_corr_M4_D4_30_macro_low_ratio_60\n",
      "99. M4_dist_to_rolling_mean_10\n",
      "100. S2_dist_to_momentum_5\n",
      "101. P8_dist_to_rolling_mean_120\n",
      "102. S2_momentum_20\n",
      "103. V13_dist_to_rolling_mean_10\n",
      "104. macro_corr_M4_P6_30_macro_high_ratio_20\n",
      "105. P1_diff_10\n",
      "106. M4_diff_1\n",
      "107. D5_cumsum\n",
      "108. E8_rolling_std_5\n",
      "109. S12_lag_10\n",
      "110. P8_z_120\n",
      "111. M4_macro_rolling_sum_10\n",
      "112. M11_skewness\n",
      "113. P7_dist_to_momentum_20\n",
      "114. M11_z_20\n",
      "115. S2_z_10\n",
      "116. E8_macro_rolling_sum_5\n",
      "117. D4_dist_to_rolling_mean_120\n",
      "118. V9_lag_1\n",
      "119. P7_dist_to_momentum_5\n",
      "120. M9_z_10\n",
      "121. V13_skewness\n",
      "122. M11_momentum_5\n",
      "123. V13_rolling_std_60\n",
      "124. M11_cumsum\n",
      "125. M11_diff_10\n",
      "126. M11_dist_to_rolling_mean_20\n",
      "127. M11_rolling_median_120\n",
      "128. P7_pctchg_1\n",
      "129. P6_diff_1\n",
      "130. M9_kurtosis\n",
      "131. P8_dist_to_rolling_mean_60\n",
      "132. V13_dist_to_momentum_10\n",
      "\n",
      "All 150 selected features:\n",
      " 1. V13_dist_to_rolling_mean_5 [ENGINEERED]\n",
      " 2. V13_diff_1                [ENGINEERED]\n",
      " 3. S8_normalized             [ENGINEERED]\n",
      " 4. V9_diff_1                 [ENGINEERED]\n",
      " 5. M4_dist_to_rolling_mean_60 [ENGINEERED]\n",
      " 6. V13_momentum_5            [ENGINEERED]\n",
      " 7. M4_diff_5                 [ENGINEERED]\n",
      " 8. S2                        [ORIGINAL]\n",
      " 9. I2                        [ORIGINAL]\n",
      "10. V13_diff_5                [ENGINEERED]\n",
      "11. V13_dist_to_momentum_20   [ENGINEERED]\n",
      "12. M11_diff_1                [ENGINEERED]\n",
      "13. M4_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "14. M11_diff_5                [ENGINEERED]\n",
      "15. M4_lag_1                  [ENGINEERED]\n",
      "16. M11_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "17. V9_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "18. M4_diff_10                [ENGINEERED]\n",
      "19. M4                        [ORIGINAL]\n",
      "20. P7_rolling_median_5       [ENGINEERED]\n",
      "21. P13_pctchg_1              [ENGINEERED]\n",
      "22. V13_rolling_min_5         [ENGINEERED]\n",
      "23. macro_corr_M4_D8_30_macro_low_ratio_20 [ENGINEERED]\n",
      "24. M4_momentum_20            [ENGINEERED]\n",
      "25. V13_momentum_10           [ENGINEERED]\n",
      "26. E12                       [ORIGINAL]\n",
      "27. E19                       [ORIGINAL]\n",
      "28. P8_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "29. P11                       [ORIGINAL]\n",
      "30. M4_dist_to_rolling_mean_20 [ENGINEERED]\n",
      "31. P6_cumsum                 [ENGINEERED]\n",
      "32. E8_momentum_20            [ENGINEERED]\n",
      "33. P8_pctchg_1               [ENGINEERED]\n",
      "34. V13_pctchg_1              [ENGINEERED]\n",
      "35. M4_momentum_10            [ENGINEERED]\n",
      "36. M11_z_120                 [ENGINEERED]\n",
      "37. P7_normalized             [ENGINEERED]\n",
      "38. P6_dist_to_momentum_20    [ENGINEERED]\n",
      "39. V9_momentum_20            [ENGINEERED]\n",
      "40. E8_cumsum                 [ENGINEERED]\n",
      "41. M11_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "42. M4_rolling_median_20      [ENGINEERED]\n",
      "43. M4_rolling_mean_120       [ENGINEERED]\n",
      "44. M9_diff_5                 [ENGINEERED]\n",
      "45. P13_momentum_5            [ENGINEERED]\n",
      "46. V9_momentum_10            [ENGINEERED]\n",
      "47. P7_lag_5                  [ENGINEERED]\n",
      "48. M9_z_20                   [ENGINEERED]\n",
      "49. P6_diff_5                 [ENGINEERED]\n",
      "50. S2_pctchg_1               [ENGINEERED]\n",
      "51. P13_dist_to_momentum_20   [ENGINEERED]\n",
      "52. macro_corr_M4_P6_30_macro_high_ratio_60 [ENGINEERED]\n",
      "53. P6_dist_to_momentum_5     [ENGINEERED]\n",
      "54. M9_pctchg_1               [ENGINEERED]\n",
      "55. macro_corr_M4_S2_30_macro_high_ratio_20 [ENGINEERED]\n",
      "56. E8_dist_to_momentum_10    [ENGINEERED]\n",
      "57. P13_dist_to_momentum_10   [ENGINEERED]\n",
      "58. S5                        [ORIGINAL]\n",
      "59. P7_diff_1                 [ENGINEERED]\n",
      "60. P13_momentum_20           [ENGINEERED]\n",
      "61. M9_momentum_10            [ENGINEERED]\n",
      "62. P7_kurtosis               [ENGINEERED]\n",
      "63. M11_momentum_10           [ENGINEERED]\n",
      "64. P7_rolling_median_120     [ENGINEERED]\n",
      "65. P13_dist_to_momentum_5    [ENGINEERED]\n",
      "66. D4_cumsum                 [ENGINEERED]\n",
      "67. V9_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "68. P10                       [ORIGINAL]\n",
      "69. M9_cumsum                 [ENGINEERED]\n",
      "70. V13_lag_1                 [ENGINEERED]\n",
      "71. P8_dist_to_rolling_mean_20 [ENGINEERED]\n",
      "72. S8_rolling_min_10         [ENGINEERED]\n",
      "73. M11_z_5                   [ENGINEERED]\n",
      "74. E11                       [ORIGINAL]\n",
      "75. macro_corr_M4_D8_30_macro_low_ratio_120 [ENGINEERED]\n",
      "76. P8_diff_5                 [ENGINEERED]\n",
      "77. S2_z_5                    [ENGINEERED]\n",
      "78. D4_rolling_mean_120       [ENGINEERED]\n",
      "79. V9_dist_to_rolling_mean_5 [ENGINEERED]\n",
      "80. S8_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "81. M9_diff_1                 [ENGINEERED]\n",
      "82. P6_pctchg_1               [ENGINEERED]\n",
      "83. S11                       [ORIGINAL]\n",
      "84. D4_rolling_std_120        [ENGINEERED]\n",
      "85. S2_momentum_10            [ENGINEERED]\n",
      "86. D4_rolling_var_120        [ENGINEERED]\n",
      "87. E16                       [ORIGINAL]\n",
      "88. M11_dist_to_momentum_5    [ENGINEERED]\n",
      "89. V9_diff_10                [ENGINEERED]\n",
      "90. M4_rolling_mean_60        [ENGINEERED]\n",
      "91. M4_z_60                   [ENGINEERED]\n",
      "92. P6_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "93. M11_pctchg_1              [ENGINEERED]\n",
      "94. M4_pctchg_1               [ENGINEERED]\n",
      "95. S8                        [ORIGINAL]\n",
      "96. P7_lag_1                  [ENGINEERED]\n",
      "97. V9_rolling_var_20         [ENGINEERED]\n",
      "98. S2_rolling_min_5          [ENGINEERED]\n",
      "99. M9_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "100. M4_lag_5                  [ENGINEERED]\n",
      "101. P13_diff_1                [ENGINEERED]\n",
      "102. M4_dist_to_momentum_5     [ENGINEERED]\n",
      "103. P6_rolling_mean_10        [ENGINEERED]\n",
      "104. M11_lag_5                 [ENGINEERED]\n",
      "105. V13_dist_to_momentum_5    [ENGINEERED]\n",
      "106. S8_skewness               [ENGINEERED]\n",
      "107. V13_lag_5                 [ENGINEERED]\n",
      "108. M11                       [ORIGINAL]\n",
      "109. M4_lag_10                 [ENGINEERED]\n",
      "110. P13_z_10                  [ENGINEERED]\n",
      "111. P12                       [ORIGINAL]\n",
      "112. macro_corr_M4_D4_30_macro_low_ratio_60 [ENGINEERED]\n",
      "113. S4                        [ORIGINAL]\n",
      "114. M4_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "115. S2_dist_to_momentum_5     [ENGINEERED]\n",
      "116. P8_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "117. S2_momentum_20            [ENGINEERED]\n",
      "118. V13_dist_to_rolling_mean_10 [ENGINEERED]\n",
      "119. macro_corr_M4_P6_30_macro_high_ratio_20 [ENGINEERED]\n",
      "120. P1_diff_10                [ENGINEERED]\n",
      "121. E17                       [ORIGINAL]\n",
      "122. M4_diff_1                 [ENGINEERED]\n",
      "123. D5_cumsum                 [ENGINEERED]\n",
      "124. E8_rolling_std_5          [ENGINEERED]\n",
      "125. S12_lag_10                [ENGINEERED]\n",
      "126. P8_z_120                  [ENGINEERED]\n",
      "127. M4_macro_rolling_sum_10   [ENGINEERED]\n",
      "128. M11_skewness              [ENGINEERED]\n",
      "129. P7_dist_to_momentum_20    [ENGINEERED]\n",
      "130. M11_z_20                  [ENGINEERED]\n",
      "131. S2_z_10                   [ENGINEERED]\n",
      "132. E8_macro_rolling_sum_5    [ENGINEERED]\n",
      "133. P2                        [ORIGINAL]\n",
      "134. D4_dist_to_rolling_mean_120 [ENGINEERED]\n",
      "135. V9_lag_1                  [ENGINEERED]\n",
      "136. P3                        [ORIGINAL]\n",
      "137. P7_dist_to_momentum_5     [ENGINEERED]\n",
      "138. M9_z_10                   [ENGINEERED]\n",
      "139. V13_skewness              [ENGINEERED]\n",
      "140. M11_momentum_5            [ENGINEERED]\n",
      "141. V13_rolling_std_60        [ENGINEERED]\n",
      "142. M11_cumsum                [ENGINEERED]\n",
      "143. M11_diff_10               [ENGINEERED]\n",
      "144. M11_dist_to_rolling_mean_20 [ENGINEERED]\n",
      "145. M11_rolling_median_120    [ENGINEERED]\n",
      "146. P7_pctchg_1               [ENGINEERED]\n",
      "147. P6_diff_1                 [ENGINEERED]\n",
      "148. M9_kurtosis               [ENGINEERED]\n",
      "149. P8_dist_to_rolling_mean_60 [ENGINEERED]\n",
      "150. V13_dist_to_momentum_10   [ENGINEERED]\n",
      "\n",
      "Top 10 Ensemble Features by Score:\n",
      "    1. V13_dist_to_rolling_mean_5 | Votes: 2 | Score: 0.774 | [ENGINEERED]\n",
      "    2. V13_diff_1                | Votes: 2 | Score: 0.765 | [ENGINEERED]\n",
      "    3. S8_normalized             | Votes: 2 | Score: 0.663 | [ENGINEERED]\n",
      "    4. V9_diff_1                 | Votes: 2 | Score: 0.636 | [ENGINEERED]\n",
      "    5. M4_dist_to_rolling_mean_60 | Votes: 2 | Score: 0.621 | [ENGINEERED]\n",
      "    6. V13_momentum_5            | Votes: 2 | Score: 0.488 | [ENGINEERED]\n",
      "    7. M4_diff_5                 | Votes: 2 | Score: 0.476 | [ENGINEERED]\n",
      "    8. S2                        | Votes: 2 | Score: 0.470 | [ORIGINAL]\n",
      "    9. I2                        | Votes: 2 | Score: 0.457 | [ORIGINAL]\n",
      "   10. V13_diff_5                | Votes: 2 | Score: 0.451 | [ENGINEERED]\n",
      "\n",
      "Final Training Data Shapes:\n",
      "Training set shape: (6290, 150)\n",
      "Target shape: (6290,)\n",
      "Features selected: 150\n",
      "\n",
      "Enhanced feature selection complete!\n",
      "Ready for model training with dynamically selected features\n"
     ]
    }
   ],
   "source": [
    "# ===== Enhanced Ensemble Feature Selection (Replaces the old selection method) =====\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "def enhanced_feature_selection(X_features, y_target, final_features, n_features=150, verbose=True):\n",
    "    \"\"\"\n",
    "    Enhanced ensemble feature selection combining multiple methods.\n",
    "    \n",
    "    Uses dynamic random states based on current time to ensure\n",
    "    different feature selections on each run for robustness testing.\n",
    "    \n",
    "    Args:\n",
    "        X_features: Feature DataFrame (from train_enh after feature engineering)\n",
    "        y_target: Target Series (from train_enh[TARGET])\n",
    "        final_features: List of original base features for categorization\n",
    "        n_features: Number of top features to select\n",
    "        verbose: Print progress information\n",
    "    \n",
    "    Returns:\n",
    "        list: Selected feature names using ensemble voting\n",
    "        dict: Detailed results from each method\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate dynamic random state based on current time\n",
    "    dynamic_seed = int(time.time() * 1000) % 100000\n",
    "    if verbose:\n",
    "        print(f\"Using dynamic seed: {dynamic_seed}\")\n",
    "    \n",
    "    # Remove zero variance features first\n",
    "    vt = VarianceThreshold(threshold=1e-6)\n",
    "    X_filtered = X_features.loc[:, vt.fit(X_features).get_support()]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Features after variance filtering: {X_filtered.shape[1]}\")\n",
    "    \n",
    "    feature_scores = {}\n",
    "    selected_features_by_method = {}\n",
    "    \n",
    "    # Method 1a: Gradient Boosting Importance (Dynamic Random State)\n",
    "    if verbose:\n",
    "        print(\"Method 1a: Gradient Boosting Feature Importance...\")\n",
    "    \n",
    "    gb = GradientBoostingRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=3, \n",
    "        random_state=dynamic_seed,  # Dynamic instead of 42\n",
    "        subsample=0.8,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "    gb.fit(X_filtered, y_target)\n",
    "    gb_scores = pd.Series(gb.feature_importances_, index=X_filtered.columns)\n",
    "    gb_top = gb_scores.nlargest(n_features).index.tolist()\n",
    "    \n",
    "    feature_scores['gradient_boosting'] = gb_scores\n",
    "    selected_features_by_method['gradient_boosting'] = gb_top\n",
    "\n",
    "    # Method 1b: Bagging Regressor Importance\n",
    "    if verbose:\n",
    "        print(\"Method 1b: Bagging Regressor Feature Importance...\")\n",
    "\n",
    "    br = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=100,\n",
    "        max_samples=0.8,\n",
    "        random_state=dynamic_seed,\n",
    "        n_jobs=-1,\n",
    "        bootstrap=True\n",
    "    )\n",
    "    br.fit(X_filtered, y_target)\n",
    "\n",
    "    # Aggregate feature importances from fitted base estimators\n",
    "    _importances = np.zeros(X_filtered.shape[1], dtype=float)\n",
    "    count = 0\n",
    "    for est in br.estimators_:\n",
    "        est_imp = getattr(est, \"feature_importances_\", None)\n",
    "        if est_imp is not None:\n",
    "            _importances += est_imp\n",
    "            count += 1\n",
    "\n",
    "    if count > 0:\n",
    "        _importances /= count\n",
    "    else:\n",
    "        _importances = np.zeros(X_filtered.shape[1], dtype=float)\n",
    "\n",
    "    br_scores = pd.Series(_importances, index=X_filtered.columns)\n",
    "    br_top = br_scores.nlargest(n_features).index.tolist()\n",
    "\n",
    "    feature_scores['bagging_regressor'] = br_scores\n",
    "    selected_features_by_method['bagging_regressor'] = br_top\n",
    "\n",
    "    # # Method 2: Random Forest Importance (Dynamic Random State)\n",
    "    # if verbose:\n",
    "    #     print(\"Method 2: Random Forest Feature Importance...\")\n",
    "    \n",
    "    # rf = RandomForestRegressor(\n",
    "    #     n_estimators=100, \n",
    "    #     max_depth=5, \n",
    "    #     random_state=dynamic_seed + 1,  # Dynamic + offset\n",
    "    #     n_jobs=-1,\n",
    "    #     bootstrap=True\n",
    "    # )\n",
    "    # rf.fit(X_filtered, y_target)\n",
    "    # rf_scores = pd.Series(rf.feature_importances_, index=X_filtered.columns)\n",
    "    # rf_top = rf_scores.nlargest(n_features).index.tolist()\n",
    "    \n",
    "    # feature_scores['random_forest'] = rf_scores\n",
    "    # selected_features_by_method['random_forest'] = rf_top\n",
    "    \n",
    "    # # Method 3: F-test Statistical Significance  \n",
    "    # if verbose:\n",
    "    #     print(\"Method 3: F-test Statistical Selection...\")\n",
    "    \n",
    "    # f_selector = SelectKBest(score_func=f_regression, k=min(n_features, X_filtered.shape[1]))\n",
    "    # f_selector.fit(X_filtered, y_target)\n",
    "    # f_scores = pd.Series(f_selector.scores_, index=X_filtered.columns)\n",
    "    # f_top = f_scores.nlargest(n_features).index.tolist()\n",
    "    \n",
    "    # feature_scores['f_test'] = f_scores\n",
    "    # selected_features_by_method['f_test'] = f_top\n",
    "    \n",
    "    # # Method 4: Mutual Information (Dynamic Random State)\n",
    "    # if verbose:\n",
    "    #     print(\"Method 4: Mutual Information Selection...\")\n",
    "    \n",
    "    # mi_scores = mutual_info_regression(\n",
    "    #     X_filtered, y_target, \n",
    "    #     random_state=dynamic_seed + 2  # Dynamic + offset\n",
    "    # )\n",
    "    # mi_scores_series = pd.Series(mi_scores, index=X_filtered.columns)\n",
    "    # mi_top = mi_scores_series.nlargest(n_features).index.tolist()\n",
    "    \n",
    "    # feature_scores['mutual_info'] = mi_scores_series\n",
    "    # selected_features_by_method['mutual_info'] = mi_top\n",
    "    \n",
    "    # Ensemble Voting: Features selected by multiple methods\n",
    "    if verbose:\n",
    "        print(\"Ensemble Voting: Combining all methods...\")\n",
    "    \n",
    "    # Count votes for each feature\n",
    "    feature_votes = {}\n",
    "    all_features = set()\n",
    "    \n",
    "    for method, features in selected_features_by_method.items():\n",
    "        all_features.update(features)\n",
    "        for feature in features:\n",
    "            feature_votes[feature] = feature_votes.get(feature, 0) + 1\n",
    "    \n",
    "    # Sort by votes, then by average score across methods\n",
    "    def get_average_score(feature):\n",
    "        scores = []\n",
    "        for method, score_series in feature_scores.items():\n",
    "            if feature in score_series.index:\n",
    "                # Normalize scores to [0,1] for fair averaging\n",
    "                normalized = (score_series[feature] - score_series.min()) / (score_series.max() - score_series.min() + 1e-10)\n",
    "                scores.append(normalized)\n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    # Create ensemble ranking\n",
    "    ensemble_ranking = []\n",
    "    for feature in all_features:\n",
    "        votes = feature_votes.get(feature, 0)\n",
    "        avg_score = get_average_score(feature)\n",
    "        ensemble_ranking.append({\n",
    "            'feature': feature,\n",
    "            'votes': votes,\n",
    "            'avg_score': avg_score,\n",
    "            'ensemble_score': votes + avg_score  # Hybrid scoring\n",
    "        })\n",
    "    \n",
    "    # Sort by ensemble score (votes + normalized average)\n",
    "    ensemble_ranking.sort(key=lambda x: x['ensemble_score'], reverse=True)\n",
    "    \n",
    "    # Select top features\n",
    "    ensemble_features = [item['feature'] for item in ensemble_ranking[:n_features]]\n",
    "    \n",
    "    if verbose:\n",
    "        # Separate engineered features from original base features for reporting\n",
    "        original_features_in_selection = [f for f in ensemble_features if f in final_features]\n",
    "        new_engineered_features = [f for f in ensemble_features if f not in final_features]\n",
    "        \n",
    "        print(f\"\\nEnsemble Feature Selection Results:\")\n",
    "        print(f\"   Total unique features considered: {len(all_features)}\")\n",
    "        print(f\"   Selected by 4 methods: {sum(1 for f in all_features if feature_votes.get(f, 0) == 4)}\")\n",
    "        print(f\"   Selected by 3 methods: {sum(1 for f in all_features if feature_votes.get(f, 0) == 3)}\")\n",
    "        print(f\"   Selected by 2 methods: {sum(1 for f in all_features if feature_votes.get(f, 0) == 2)}\")\n",
    "        # =============================== TO FIX ===============================\n",
    "        print(f\"   Selected by 1b method:  {sum(1 for f in all_features if feature_votes.get(f, 0) == 1)}\")\n",
    "        print(f\"   Selected by 1a method:  {sum(1 for f in all_features if feature_votes.get(f, 0) == 1)}\")\n",
    "        # =============================== TO FIX ===============================\n",
    "        print(f\"   Final ensemble selection: {len(ensemble_features)} features\")\n",
    "        \n",
    "        print(f\"\\nFeature Engineering Results:\")\n",
    "        print(f\"Original base features available: {len(final_features)}\")\n",
    "        print(f\"Original features selected: {len(original_features_in_selection)}\")\n",
    "        print(f\"New engineered features created: {len(new_engineered_features)}\")\n",
    "        print(f\"Total features for modeling: {len(ensemble_features)}\")\n",
    "\n",
    "        print(f\"\\nNew engineered features added:\")\n",
    "        for i, feat in enumerate(new_engineered_features, 1):\n",
    "            print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "        print(f\"\\nAll {len(ensemble_features)} selected features:\")\n",
    "        for i, feat in enumerate(ensemble_features, 1):\n",
    "            feat_type = \"ORIGINAL\" if feat in final_features else \"ENGINEERED\"\n",
    "            print(f\"{i:2d}. {feat:<25} [{feat_type}]\")\n",
    "        \n",
    "        # Show top 10 features with vote details\n",
    "        print(f\"\\nTop 10 Ensemble Features by Score:\")\n",
    "        for i, item in enumerate(ensemble_ranking[:10], 1):\n",
    "            feat_type = \"ORIGINAL\" if item['feature'] in final_features else \"ENGINEERED\"\n",
    "            print(f\"   {i:2d}. {item['feature']:<25} | Votes: {item['votes']} | Score: {item['avg_score']:.3f} | [{feat_type}]\")\n",
    "    \n",
    "    results = {\n",
    "        'ensemble_features': ensemble_features,\n",
    "        'method_features': selected_features_by_method,\n",
    "        'feature_scores': feature_scores,\n",
    "        'ensemble_ranking': ensemble_ranking,\n",
    "        'dynamic_seed': dynamic_seed,\n",
    "        'original_features_selected': [f for f in ensemble_features if f in final_features],\n",
    "        'engineered_features_selected': [f for f in ensemble_features if f not in final_features]\n",
    "    }\n",
    "    \n",
    "    return ensemble_features, results\n",
    "\n",
    "# ===== REPLACE THE OLD FEATURE SELECTION SECTION =====\n",
    "\n",
    "# Feature Engineering & Data Preparation\n",
    "top_features = ['M4', 'V13', 'M11', 'S2', 'D4', 'D1', 'D2', 'E8', 'P6', 'M2', \n",
    "                'D8', 'M9', 'P8', 'P7', 'S12', 'P13', 'V9', 'D5', 'P1', 'S8']\n",
    "\n",
    "print(\"Creating advanced features for training data...\")\n",
    "\n",
    "# CORRECT: Create DataFrame with date_id + features but WITHOUT target columns to prevent data leakage\n",
    "columns_to_exclude = [\"market_forward_excess_returns\", \"forward_returns\", \"risk_free_rate\"]\n",
    "columns_to_include = ['date_id'] + [col for col in final_features if col in train_full.columns]\n",
    "\n",
    "train_for_engineering = train_full[columns_to_include].copy()\n",
    "\n",
    "print(f\"Columns for feature engineering (count): {len(columns_to_include)}\")\n",
    "# print name of columns included\n",
    "print(f\"Included columns names: {columns_to_include}\")\n",
    "# print length of excluded columns\n",
    "print(f\"Excluded columns (count): {len(columns_to_exclude)}\")\n",
    "print(f\"Excluded columns (prevent leakage) names: {columns_to_exclude}\")\n",
    "\n",
    "train_enh = create_advanced_features(\n",
    "    train_for_engineering,\n",
    "    top_features=top_features,\n",
    "    window_sizes=(5, 10, 20, 60, 120),\n",
    "    shift=1\n",
    ")\n",
    "\n",
    "# Add target back AFTER feature engineering for supervised selection\n",
    "train_enh[TARGET] = train_full[TARGET].values\n",
    "\n",
    "# Now do ENHANCED supervised feature selection with target present\n",
    "feature_columns = [c for c in train_enh.columns if c not in ['date_id', TARGET]]\n",
    "print(f\"Feature columns for selection: {len(feature_columns)} total features available\")\n",
    "\n",
    "# Supervised feature selection using ENHANCED method\n",
    "X_features = train_enh[feature_columns]\n",
    "y_target = train_enh[TARGET]\n",
    "\n",
    "# Apply Enhanced Feature Selection (replaces the old single-method approach)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENHANCED ENSEMBLE FEATURE SELECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "selected_features, selection_results = enhanced_feature_selection(\n",
    "    X_features, y_target, final_features,\n",
    "    n_features=150,  \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Final feature matrices\n",
    "X = train_enh[selected_features].astype('float32')\n",
    "y = train_enh[TARGET].astype('float32')\n",
    "\n",
    "print(f\"\\nFinal Training Data Shapes:\")\n",
    "print(f\"Training set shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features selected: {len(selected_features)}\")\n",
    "\n",
    "# Store for later use in inference\n",
    "final_selected_features = selected_features\n",
    "\n",
    "print(\"\\nEnhanced feature selection complete!\")\n",
    "print(\"Ready for model training with dynamically selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5102fc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_id', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1',\n",
       "       'E10', 'E11', 'E12', 'E13', 'E14', 'E15', 'E16', 'E17', 'E18', 'E19',\n",
       "       'E2', 'E20', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'I1', 'I2', 'I3',\n",
       "       'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'M1', 'M10', 'M11', 'M12', 'M13',\n",
       "       'M14', 'M15', 'M16', 'M17', 'M18', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7',\n",
       "       'M8', 'M9', 'P1', 'P10', 'P11', 'P12', 'P13', 'P2', 'P3', 'P4', 'P5',\n",
       "       'P6', 'P7', 'P8', 'P9', 'S1', 'S10', 'S11', 'S12', 'S2', 'S3', 'S4',\n",
       "       'S5', 'S6', 'S7', 'S8', 'S9', 'V1', 'V10', 'V11', 'V12', 'V13', 'V2',\n",
       "       'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_for_engineering.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518e802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncheck this:\\n\\n    # Create ensemble ranking\\n    ensemble_ranking = []\\n    for feature in all_features:\\n        votes = feature_votes.get(feature, 0)\\n        avg_score = get_average_score(feature)\\n        ensemble_ranking.append({\\n            'feature': feature,\\n            'votes': votes,\\n            'avg_score': avg_score,\\n            'ensemble_score': votes + avg_score  # Hybrid scoring\\n        })\\n\\nverify if really required\\n\\n============================\\n\\nFeature engineering complete. Created 1985 total columns.\\nFeature columns for selection: 1984 total features available (date_id removed, TO CHECK)\\n\\nFeatures after variance filtering: 1334\\n\\nCheck why 464\\n   Total unique features considered: 464\\n   Selected by 4 methods: 0\\n   Selected by 3 methods: 30\\n   Selected by 2 methods: 76\\n   Selected by 1 method:  358\\n   Final ensemble selection: 150 features\\n\\nThis can be removed:\\n   Selected by 4 methods: 0\\n   Selected by 3 methods: 30\\nno added value\\n\\nok:\\nFinal Training Data Shapes:\\nTraining set shape: (6290, 150)\\nTarget shape: (6290,)\\nFeatures selected: 150\\n\""
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "check this:\n",
    "   Selected by 2 methods: 65\n",
    "   Selected by 1 method:  170\n",
    "   Final ensemble selection: 150 features\n",
    "same numbers already twice seen,\n",
    "\n",
    "check this:\n",
    "\n",
    "    # Create ensemble ranking\n",
    "    ensemble_ranking = []\n",
    "    for feature in all_features:\n",
    "        votes = feature_votes.get(feature, 0)\n",
    "        avg_score = get_average_score(feature)\n",
    "        ensemble_ranking.append({\n",
    "            'feature': feature,\n",
    "            'votes': votes,\n",
    "            'avg_score': avg_score,\n",
    "            'ensemble_score': votes + avg_score  # Hybrid scoring\n",
    "        })\n",
    "\n",
    "verify if really required\n",
    "\n",
    "============================\n",
    "\n",
    "Feature engineering complete. Created 1985 total columns.\n",
    "Feature columns for selection: 1984 total features available (date_id removed, TO CHECK)\n",
    "\n",
    "Features after variance filtering: 1334\n",
    "\n",
    "Check why 464\n",
    "   Total unique features considered: 464\n",
    "   Selected by 4 methods: 0\n",
    "   Selected by 3 methods: 30\n",
    "   Selected by 2 methods: 76\n",
    "   Selected by 1 method:  358\n",
    "   Final ensemble selection: 150 features\n",
    "\n",
    "This can be removed:\n",
    "   Selected by 4 methods: 0\n",
    "   Selected by 3 methods: 30\n",
    "no added value\n",
    "\n",
    "ok:\n",
    "Final Training Data Shapes:\n",
    "Training set shape: (6290, 150)\n",
    "Target shape: (6290,)\n",
    "Features selected: 150\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0f4dfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can add also MC shuffle of.... or synthetic data generation from existing data to increase training set size\n",
    "\n",
    "!!!!!!!!!!!!!!!!\n",
    "synthetic data generation should be used carefully to avoid data leakage, \n",
    "overfitting and unrealistic patterns.\n",
    "!!!!!!!!!!!!!!!!\n",
    "\n",
    "dai calcoli rolling, sum, ...... altri da controllare, andrebbero tolte le colonne con\n",
    "valori binari (0/1) o poche variazioni, perche' rolling mean, std, ecc... \n",
    "non hanno senso su quelle colonne e portano a risultati errati\n",
    "..controllare le colonne..e rimuovere quelle non idonee prima di fare feature engineering\n",
    "\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ac494e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V13_dist_to_rolling_mean_5',\n",
       " 'V13_diff_1',\n",
       " 'S8_normalized',\n",
       " 'V9_diff_1',\n",
       " 'M4_dist_to_rolling_mean_60',\n",
       " 'V13_momentum_5',\n",
       " 'M4_diff_5',\n",
       " 'S2',\n",
       " 'I2',\n",
       " 'V13_diff_5',\n",
       " 'V13_dist_to_momentum_20',\n",
       " 'M11_diff_1',\n",
       " 'M4_dist_to_rolling_mean_120',\n",
       " 'M11_diff_5',\n",
       " 'M4_lag_1',\n",
       " 'M11_dist_to_rolling_mean_10',\n",
       " 'V9_dist_to_rolling_mean_60',\n",
       " 'M4_diff_10',\n",
       " 'M4',\n",
       " 'P7_rolling_median_5',\n",
       " 'P13_pctchg_1',\n",
       " 'V13_rolling_min_5',\n",
       " 'macro_corr_M4_D8_30_macro_low_ratio_20',\n",
       " 'M4_momentum_20',\n",
       " 'V13_momentum_10',\n",
       " 'E12',\n",
       " 'E19',\n",
       " 'P8_dist_to_rolling_mean_10',\n",
       " 'P11',\n",
       " 'M4_dist_to_rolling_mean_20',\n",
       " 'P6_cumsum',\n",
       " 'E8_momentum_20',\n",
       " 'P8_pctchg_1',\n",
       " 'V13_pctchg_1',\n",
       " 'M4_momentum_10',\n",
       " 'M11_z_120',\n",
       " 'P7_normalized',\n",
       " 'P6_dist_to_momentum_20',\n",
       " 'V9_momentum_20',\n",
       " 'E8_cumsum',\n",
       " 'M11_dist_to_rolling_mean_5',\n",
       " 'M4_rolling_median_20',\n",
       " 'M4_rolling_mean_120',\n",
       " 'M9_diff_5',\n",
       " 'P13_momentum_5',\n",
       " 'V9_momentum_10',\n",
       " 'P7_lag_5',\n",
       " 'M9_z_20',\n",
       " 'P6_diff_5',\n",
       " 'S2_pctchg_1',\n",
       " 'P13_dist_to_momentum_20',\n",
       " 'macro_corr_M4_P6_30_macro_high_ratio_60',\n",
       " 'P6_dist_to_momentum_5',\n",
       " 'M9_pctchg_1',\n",
       " 'macro_corr_M4_S2_30_macro_high_ratio_20',\n",
       " 'E8_dist_to_momentum_10',\n",
       " 'P13_dist_to_momentum_10',\n",
       " 'S5',\n",
       " 'P7_diff_1',\n",
       " 'P13_momentum_20',\n",
       " 'M9_momentum_10',\n",
       " 'P7_kurtosis',\n",
       " 'M11_momentum_10',\n",
       " 'P7_rolling_median_120',\n",
       " 'P13_dist_to_momentum_5',\n",
       " 'D4_cumsum',\n",
       " 'V9_dist_to_rolling_mean_120',\n",
       " 'P10',\n",
       " 'M9_cumsum',\n",
       " 'V13_lag_1',\n",
       " 'P8_dist_to_rolling_mean_20',\n",
       " 'S8_rolling_min_10',\n",
       " 'M11_z_5',\n",
       " 'E11',\n",
       " 'macro_corr_M4_D8_30_macro_low_ratio_120',\n",
       " 'P8_diff_5',\n",
       " 'S2_z_5',\n",
       " 'D4_rolling_mean_120',\n",
       " 'V9_dist_to_rolling_mean_5',\n",
       " 'S8_dist_to_rolling_mean_120',\n",
       " 'M9_diff_1',\n",
       " 'P6_pctchg_1',\n",
       " 'S11',\n",
       " 'D4_rolling_std_120',\n",
       " 'S2_momentum_10',\n",
       " 'D4_rolling_var_120',\n",
       " 'E16',\n",
       " 'M11_dist_to_momentum_5',\n",
       " 'V9_diff_10',\n",
       " 'M4_rolling_mean_60',\n",
       " 'M4_z_60',\n",
       " 'P6_dist_to_rolling_mean_60',\n",
       " 'M11_pctchg_1',\n",
       " 'M4_pctchg_1',\n",
       " 'S8',\n",
       " 'P7_lag_1',\n",
       " 'V9_rolling_var_20',\n",
       " 'S2_rolling_min_5',\n",
       " 'M9_dist_to_rolling_mean_120',\n",
       " 'M4_lag_5',\n",
       " 'P13_diff_1',\n",
       " 'M4_dist_to_momentum_5',\n",
       " 'P6_rolling_mean_10',\n",
       " 'M11_lag_5',\n",
       " 'V13_dist_to_momentum_5',\n",
       " 'S8_skewness',\n",
       " 'V13_lag_5',\n",
       " 'M11',\n",
       " 'M4_lag_10',\n",
       " 'P13_z_10',\n",
       " 'P12',\n",
       " 'macro_corr_M4_D4_30_macro_low_ratio_60',\n",
       " 'S4',\n",
       " 'M4_dist_to_rolling_mean_10',\n",
       " 'S2_dist_to_momentum_5',\n",
       " 'P8_dist_to_rolling_mean_120',\n",
       " 'S2_momentum_20',\n",
       " 'V13_dist_to_rolling_mean_10',\n",
       " 'macro_corr_M4_P6_30_macro_high_ratio_20',\n",
       " 'P1_diff_10',\n",
       " 'E17',\n",
       " 'M4_diff_1',\n",
       " 'D5_cumsum',\n",
       " 'E8_rolling_std_5',\n",
       " 'S12_lag_10',\n",
       " 'P8_z_120',\n",
       " 'M4_macro_rolling_sum_10',\n",
       " 'M11_skewness',\n",
       " 'P7_dist_to_momentum_20',\n",
       " 'M11_z_20',\n",
       " 'S2_z_10',\n",
       " 'E8_macro_rolling_sum_5',\n",
       " 'P2',\n",
       " 'D4_dist_to_rolling_mean_120',\n",
       " 'V9_lag_1',\n",
       " 'P3',\n",
       " 'P7_dist_to_momentum_5',\n",
       " 'M9_z_10',\n",
       " 'V13_skewness',\n",
       " 'M11_momentum_5',\n",
       " 'V13_rolling_std_60',\n",
       " 'M11_cumsum',\n",
       " 'M11_diff_10',\n",
       " 'M11_dist_to_rolling_mean_20',\n",
       " 'M11_rolling_median_120',\n",
       " 'P7_pctchg_1',\n",
       " 'P6_diff_1',\n",
       " 'M9_kurtosis',\n",
       " 'P8_dist_to_rolling_mean_60',\n",
       " 'V13_dist_to_momentum_10']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8b7f9866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GridSearchCV requires a lot of time to run'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"GridSearchCV requires a lot of time to run\"\"\"\n",
    "\n",
    "# # Now your models train on the BEST features, not fixed ones\n",
    "\n",
    "# # Generate dynamic random state for models\n",
    "# model_seed = int(time.time() * 1000) % 100000\n",
    "# print(f\"Using dynamic random seed for models: {model_seed}\")\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# param_grid = {\n",
    "#     'depth': [4, 6],\n",
    "#     'learning_rate': [0.05, 0.1],\n",
    "#     'iterations': [300, 500],\n",
    "#     'l2_leaf_reg': [2, 5]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(\n",
    "#     estimator=CatBoostRegressor(random_seed=model_seed, verbose=False),\n",
    "#     param_grid=param_grid,\n",
    "#     cv=tscv,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# grid.fit(X, y)\n",
    "# ml_model = grid.best_estimator_\n",
    "# print(f\"Best Params: {grid.best_params_}\")\n",
    "\n",
    "# # # Make predictions (uncomment if needed)\n",
    "# # cat_predictions = ml_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1c495a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V13_dist_to_rolling_mean_5',\n",
       " 'V13_diff_1',\n",
       " 'S8_normalized',\n",
       " 'V9_diff_1',\n",
       " 'M4_dist_to_rolling_mean_60',\n",
       " 'V13_momentum_5',\n",
       " 'M4_diff_5',\n",
       " 'S2',\n",
       " 'I2',\n",
       " 'V13_diff_5',\n",
       " 'V13_dist_to_momentum_20',\n",
       " 'M11_diff_1',\n",
       " 'M4_dist_to_rolling_mean_120',\n",
       " 'M11_diff_5',\n",
       " 'M4_lag_1',\n",
       " 'M11_dist_to_rolling_mean_10',\n",
       " 'V9_dist_to_rolling_mean_60',\n",
       " 'M4_diff_10',\n",
       " 'M4',\n",
       " 'P7_rolling_median_5',\n",
       " 'P13_pctchg_1',\n",
       " 'V13_rolling_min_5',\n",
       " 'macro_corr_M4_D8_30_macro_low_ratio_20',\n",
       " 'M4_momentum_20',\n",
       " 'V13_momentum_10',\n",
       " 'E12',\n",
       " 'E19',\n",
       " 'P8_dist_to_rolling_mean_10',\n",
       " 'P11',\n",
       " 'M4_dist_to_rolling_mean_20',\n",
       " 'P6_cumsum',\n",
       " 'E8_momentum_20',\n",
       " 'P8_pctchg_1',\n",
       " 'V13_pctchg_1',\n",
       " 'M4_momentum_10',\n",
       " 'M11_z_120',\n",
       " 'P7_normalized',\n",
       " 'P6_dist_to_momentum_20',\n",
       " 'V9_momentum_20',\n",
       " 'E8_cumsum',\n",
       " 'M11_dist_to_rolling_mean_5',\n",
       " 'M4_rolling_median_20',\n",
       " 'M4_rolling_mean_120',\n",
       " 'M9_diff_5',\n",
       " 'P13_momentum_5',\n",
       " 'V9_momentum_10',\n",
       " 'P7_lag_5',\n",
       " 'M9_z_20',\n",
       " 'P6_diff_5',\n",
       " 'S2_pctchg_1',\n",
       " 'P13_dist_to_momentum_20',\n",
       " 'macro_corr_M4_P6_30_macro_high_ratio_60',\n",
       " 'P6_dist_to_momentum_5',\n",
       " 'M9_pctchg_1',\n",
       " 'macro_corr_M4_S2_30_macro_high_ratio_20',\n",
       " 'E8_dist_to_momentum_10',\n",
       " 'P13_dist_to_momentum_10',\n",
       " 'S5',\n",
       " 'P7_diff_1',\n",
       " 'P13_momentum_20',\n",
       " 'M9_momentum_10',\n",
       " 'P7_kurtosis',\n",
       " 'M11_momentum_10',\n",
       " 'P7_rolling_median_120',\n",
       " 'P13_dist_to_momentum_5',\n",
       " 'D4_cumsum',\n",
       " 'V9_dist_to_rolling_mean_120',\n",
       " 'P10',\n",
       " 'M9_cumsum',\n",
       " 'V13_lag_1',\n",
       " 'P8_dist_to_rolling_mean_20',\n",
       " 'S8_rolling_min_10',\n",
       " 'M11_z_5',\n",
       " 'E11',\n",
       " 'macro_corr_M4_D8_30_macro_low_ratio_120',\n",
       " 'P8_diff_5',\n",
       " 'S2_z_5',\n",
       " 'D4_rolling_mean_120',\n",
       " 'V9_dist_to_rolling_mean_5',\n",
       " 'S8_dist_to_rolling_mean_120',\n",
       " 'M9_diff_1',\n",
       " 'P6_pctchg_1',\n",
       " 'S11',\n",
       " 'D4_rolling_std_120',\n",
       " 'S2_momentum_10',\n",
       " 'D4_rolling_var_120',\n",
       " 'E16',\n",
       " 'M11_dist_to_momentum_5',\n",
       " 'V9_diff_10',\n",
       " 'M4_rolling_mean_60',\n",
       " 'M4_z_60',\n",
       " 'P6_dist_to_rolling_mean_60',\n",
       " 'M11_pctchg_1',\n",
       " 'M4_pctchg_1',\n",
       " 'S8',\n",
       " 'P7_lag_1',\n",
       " 'V9_rolling_var_20',\n",
       " 'S2_rolling_min_5',\n",
       " 'M9_dist_to_rolling_mean_120',\n",
       " 'M4_lag_5',\n",
       " 'P13_diff_1',\n",
       " 'M4_dist_to_momentum_5',\n",
       " 'P6_rolling_mean_10',\n",
       " 'M11_lag_5',\n",
       " 'V13_dist_to_momentum_5',\n",
       " 'S8_skewness',\n",
       " 'V13_lag_5',\n",
       " 'M11',\n",
       " 'M4_lag_10',\n",
       " 'P13_z_10',\n",
       " 'P12',\n",
       " 'macro_corr_M4_D4_30_macro_low_ratio_60',\n",
       " 'S4',\n",
       " 'M4_dist_to_rolling_mean_10',\n",
       " 'S2_dist_to_momentum_5',\n",
       " 'P8_dist_to_rolling_mean_120',\n",
       " 'S2_momentum_20',\n",
       " 'V13_dist_to_rolling_mean_10',\n",
       " 'macro_corr_M4_P6_30_macro_high_ratio_20',\n",
       " 'P1_diff_10',\n",
       " 'E17',\n",
       " 'M4_diff_1',\n",
       " 'D5_cumsum',\n",
       " 'E8_rolling_std_5',\n",
       " 'S12_lag_10',\n",
       " 'P8_z_120',\n",
       " 'M4_macro_rolling_sum_10',\n",
       " 'M11_skewness',\n",
       " 'P7_dist_to_momentum_20',\n",
       " 'M11_z_20',\n",
       " 'S2_z_10',\n",
       " 'E8_macro_rolling_sum_5',\n",
       " 'P2',\n",
       " 'D4_dist_to_rolling_mean_120',\n",
       " 'V9_lag_1',\n",
       " 'P3',\n",
       " 'P7_dist_to_momentum_5',\n",
       " 'M9_z_10',\n",
       " 'V13_skewness',\n",
       " 'M11_momentum_5',\n",
       " 'V13_rolling_std_60',\n",
       " 'M11_cumsum',\n",
       " 'M11_diff_10',\n",
       " 'M11_dist_to_rolling_mean_20',\n",
       " 'M11_rolling_median_120',\n",
       " 'P7_pctchg_1',\n",
       " 'P6_diff_1',\n",
       " 'M9_kurtosis',\n",
       " 'P8_dist_to_rolling_mean_60',\n",
       " 'V13_dist_to_momentum_10']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "371555a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature consistency check: TRUE\n"
     ]
    }
   ],
   "source": [
    "# function to compare final_selected_features with X.columns.tolist(), to veruify they match\n",
    "def verify_feature_consistency(selected_features, model_features):\n",
    "    \"\"\"\n",
    "    Verify that the selected features match the model features.\n",
    "    \n",
    "    Args:\n",
    "        selected_features: List of features selected during feature selection\n",
    "        model_features: List of features used in the model training\n",
    "    Returns:\n",
    "        bool: True if they match, False otherwise\n",
    "    \"\"\"\n",
    "    return set(selected_features) == set(model_features)\n",
    "\n",
    "# apply the verification\n",
    "features_match = verify_feature_consistency(final_selected_features, X.columns.tolist())\n",
    "\n",
    "# print the result\n",
    "print(f\"Feature consistency check: {'TRUE' if features_match else 'FALSE'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "74c96fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -0.003038\n",
       "1      -0.009114\n",
       "2      -0.010243\n",
       "3       0.004046\n",
       "4      -0.012301\n",
       "          ...   \n",
       "6285   -0.016339\n",
       "6286    0.004761\n",
       "6287   -0.016470\n",
       "6288   -0.007177\n",
       "6289   -0.008327\n",
       "Name: market_forward_excess_returns, Length: 6290, dtype: float32"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b2089",
   "metadata": {},
   "source": [
    "### ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b9a20934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  CatBoost Base Model \n",
    "# ================================================================\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Generate dynamic random state for models\n",
    "model_seed = int(time.time() * 1000) % 100000\n",
    "# print(f\"Using dynamic random seed for models: {model_seed}\")\n",
    "\n",
    "# Initialize CatBoostRegressor with BEST features and dynamic random state\n",
    "ml_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=model_seed,  # Dynamic instead of fixed 42\n",
    "    verbose=False,\n",
    "    # loss_function='RMSE',\n",
    "    # verbose=100,\n",
    "    # random_seed=42\n",
    ")\n",
    "\n",
    "# Use the correctly selected features (X instead of undefined X_best)\n",
    "# ml_model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50)\n",
    "ml_model.fit(X, y);\n",
    "\n",
    "# # Make predictions\n",
    "# cat_predictions = ml_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "47c65c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # LightGBM Training\n",
    "# # ================================================================\n",
    "# import lightgbm as lgb\n",
    "\n",
    "# # Generate dynamic random state for models\n",
    "# model_seed = int(time.time() * 1000) % 100000\n",
    "# print(f\"Using dynamic random seed for models: {model_seed}\")\n",
    "\n",
    "# # LightGBM Parameters\n",
    "# params = {\n",
    "#     'objective': 'regression',\n",
    "#     'metric': 'rmse',\n",
    "#     'learning_rate': 0.05,\n",
    "#     'num_leaves': 63,\n",
    "#     'feature_fraction': 0.8,\n",
    "#     'bagging_fraction': 0.8,\n",
    "#     'bagging_freq': 5,\n",
    "#     'seed': model_seed,\n",
    "#     'n_jobs': -1,\n",
    "#     'verbose': -1\n",
    "# }\n",
    "\n",
    "# # Initialize and train LightGBM model\n",
    "# ml_model = lgb.LGBMRegressor(**params)\n",
    "# ml_model.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bfe6b707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd here any additional ML model evaluation or prediction code as needed.\\n'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add here any additional ML model evaluation or prediction code as needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aacf0f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd here any additional ML model evaluation or prediction code as needed.\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add here any additional ML model evaluation or prediction code as needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "adb0294d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd here any additional ML model evaluation or prediction code as needed.\\n'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add here any additional ML model evaluation or prediction code as needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ccb15c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd here any additional ML model evaluation or prediction code as needed.\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add here any additional ML model evaluation or prediction code as needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "830085ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdd here any additional ML model evaluation or prediction code as needed.\\n'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add here any additional ML model evaluation or prediction code as needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273de23f",
   "metadata": {},
   "source": [
    "### NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "08334e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network with BEST features  \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use only first GPU if multiple\n",
    "\n",
    "# Import TensorFlow after setting environment variables\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')  # Only show errors\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\"\"\"\n",
    "The cell contains the neural network model definition and training code, \n",
    "which fits the model on the selected features from the enhanced feature selection process.\n",
    "\"\"\"\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(len(selected_features),)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2), \n",
    "    Dense(1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Set TensorFlow random seed for reproducibility within this run\n",
    "tf.random.set_seed(model_seed)\n",
    "nn_model.fit(X_scaled, y, epochs=100, validation_split=0.2, verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4e6d783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete with dynamically selected features\n",
      "CatBoost trained on 150 features\n",
      "Neural Network trained on 150 features\n",
      "Selected features: ['V13_dist_to_rolling_mean_5', 'V13_diff_1', 'S8_normalized', 'V9_diff_1', 'M4_dist_to_rolling_mean_60', 'V13_momentum_5', 'M4_diff_5', 'S2', 'I2', 'V13_diff_5']...\n"
     ]
    }
   ],
   "source": [
    "print(\"Model training complete with dynamically selected features\")\n",
    "print(f\"CatBoost trained on {X.shape[1]} features\")\n",
    "print(f\"Neural Network trained on {X.shape[1]} features\")\n",
    "print(f\"Selected features: {selected_features[:10]}...\" if len(selected_features) > 10 else f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5acef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7e08be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# #  CatBoost Base Model (GridSearch + TimeSeriesSplit)\n",
    "# # ================================================================\n",
    "\n",
    "# print(\"Training CatBoost model with TimeSeries CV...\")\n",
    "\n",
    "# tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# # check here random_state = 42 for reproducibility!\n",
    "# cbc = CatBoostRegressor(loss_function='RMSE', verbose=0, random_state=42)\n",
    "\n",
    "# param_grid = {\n",
    "#     'depth': [4, 6],\n",
    "#     'learning_rate': [0.05, 0.1],\n",
    "#     'iterations': [300, 500],\n",
    "#     'l2_leaf_reg': [2, 5]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(\n",
    "#     estimator=cbc,\n",
    "#     param_grid=param_grid,\n",
    "#     cv=tscv,\n",
    "#     scoring='neg_mean_squared_error',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# grid.fit(X, y)\n",
    "# best_cbc = grid.best_estimator_\n",
    "# print(f\" Best Params: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8ed795b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# #  Neural Network Model (Feedforward Regressor)\n",
    "# # ================================================================\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# def build_nn(input_dim):\n",
    "#     model = keras.Sequential([\n",
    "#         layers.Input(shape=(input_dim,)),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.Dropout(0.2),\n",
    "#         layers.Dense(64, activation='relu'),\n",
    "#         layers.Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "#     return model\n",
    "\n",
    "# nn_model = build_nn(X_scaled.shape[1])\n",
    "# es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # last 20% time-based validation\n",
    "# date_cut = train[\"date_id\"].quantile(0.8)\n",
    "# train_idx = train[\"date_id\"] <= date_cut\n",
    "# val_idx = train[\"date_id\"] > date_cut\n",
    "\n",
    "# X_train, y_train = X_scaled[train_idx], y[train_idx]\n",
    "# X_val, y_val = X_scaled[val_idx], y[val_idx]\n",
    "\n",
    "# nn_model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "#              epochs=100, batch_size=256, verbose=0, callbacks=[es])\n",
    "# print(\" Neural Network trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5ef74b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Submission saved to 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "This code splits the data into training, validation, and test sets\n",
    "\"\"\"\n",
    "\n",
    "# Attention to random_state=42 for reproducibility\n",
    "\n",
    "# Define train-validation-test split without random_state\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5\n",
    ")\n",
    "\n",
    "# Extract validation indices\n",
    "val_idx = X_val.index\n",
    "\n",
    "# ==================================================================\n",
    "# Ensemble Prediction (0.8 × ML Model + 0.2 × NN)\n",
    "# ==================================================================\n",
    "\n",
    "# ================================================================\n",
    "# Fix here the ensemble weights as needed\n",
    "# ================================================================\n",
    "# ensemble_cat_pct = 0.8\n",
    "# ensemble_nn_pct = 0.2\n",
    "\n",
    "ensemble_cat_pct = 1.0\n",
    "ensemble_nn_pct = 0.0\n",
    "# ================================================================\n",
    "\n",
    "# Validation predictions\n",
    "val_cat = ml_model.predict(X_val)\n",
    "val_nn = nn_model.predict(scaler.transform(X_val)).ravel()\n",
    "\n",
    "# Combine predictions\n",
    "val_ensemble = ensemble_cat_pct * val_cat + ensemble_nn_pct * val_nn\n",
    "\n",
    "# Create validation DataFrame with predictions\n",
    "val_df = train.loc[val_idx].copy()\n",
    "val_df[\"pred\"] = val_ensemble\n",
    "\n",
    "# Test predictions\n",
    "test_cat = ml_model.predict(X_test)\n",
    "test_nn = nn_model.predict(scaler.transform(X_test)).ravel()\n",
    "\n",
    "# Combine test predictions\n",
    "test_ensemble = ensemble_cat_pct * test_cat + ensemble_nn_pct * test_nn\n",
    "\n",
    "# ==================================================================\n",
    "# This is a temporary submission file creation for test predictions\n",
    "# ==================================================================\n",
    "# Save test predictions with sequential id matching the test_ensemble length\n",
    "submission = pd.DataFrame({'id': range(len(test_ensemble)), 'target': test_ensemble})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved to 'submission.csv'\")\n",
    "\n",
    "# # Save test predictions using date_id as the identifier\n",
    "# submission = pd.DataFrame({'id': test['date_id'], 'target': test_ensemble})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission saved to 'submission.csv'\")\n",
    "\n",
    "# # Save test predictions with sequential id\n",
    "# submission = pd.DataFrame({'id': range(len(test_ensemble)), 'target': test_ensemble})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9e4d8e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe above code implements a machine learning pipeline that splits the dataset into training, \\nvalidation, and test sets, performs predictions using an ensemble of two models \\n(CatBoost and a neural network), and saves the final test predictions to a CSV file for submission.\\n\\nData Splitting\\nThe dataset is divided into three subsets: training, validation, and test sets. \\nThe train_test_split function from sklearn is used twice:\\n\\nFirst Split: The data is split into a training set (X_train, y_train) and a temporary set \\n(X_temp, y_temp) with 70% of the data allocated to training and 30% to the temporary set.\\nSecond Split: The temporary set is further split into validation (X_val, y_val) \\nand test sets (X_test, y_test) with an equal 50-50 split. \\nThis results in 15% of the original data for validation and 15% for testing.\\nThe validation indices (val_idx) are extracted from X_val for later use in creating \\na DataFrame with predictions.\\n\\nEnsemble Prediction\\nThe ensemble combines predictions from two models:\\n\\nCatBoost Model (ml_model): Predictions are made directly on the validation and test sets.\\nNeural Network Model (nn_model): Predictions are made after scaling the validation \\nand test sets using a pre-fitted scaler. The .ravel() method ensures the predictions \\nare flattened into a 1D array.\\nThe ensemble combines the predictions using weighted averaging:\\n\\n80% weight for CatBoost predictions.\\n20% weight for neural network predictions.\\nValidation Predictions\\nThe combined predictions for the validation set (val_ensemble) are stored \\nin a new DataFrame (val_df) that includes the original validation data \\n(retrieved using val_idx) and the ensemble predictions under the column \"pred\". \\nThis allows for further analysis or evaluation of the ensemble\\'s performance on the validation set.\\n\\nTest Predictions and Submission\\nThe ensemble predictions for the test set (test_ensemble) are saved in a DataFrame \\n(submission) with two columns:\\n\\n\"id\": The unique identifier for each test sample, assumed to be present in the test DataFrame.\\n\"target\": The ensemble predictions.\\nThe submission DataFrame is exported to a CSV file named submission.csv, \\nwhich can be used for competition submissions or further analysis. \\nA confirmation message is printed to indicate the file has been saved successfully.\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The above code implements a machine learning pipeline that splits the dataset into training, \n",
    "validation, and test sets, performs predictions using an ensemble of two models \n",
    "(CatBoost and a neural network), and saves the final test predictions to a CSV file for submission.\n",
    "\n",
    "Data Splitting\n",
    "The dataset is divided into three subsets: training, validation, and test sets. \n",
    "The train_test_split function from sklearn is used twice:\n",
    "\n",
    "First Split: The data is split into a training set (X_train, y_train) and a temporary set \n",
    "(X_temp, y_temp) with 70% of the data allocated to training and 30% to the temporary set.\n",
    "Second Split: The temporary set is further split into validation (X_val, y_val) \n",
    "and test sets (X_test, y_test) with an equal 50-50 split. \n",
    "This results in 15% of the original data for validation and 15% for testing.\n",
    "The validation indices (val_idx) are extracted from X_val for later use in creating \n",
    "a DataFrame with predictions.\n",
    "\n",
    "Ensemble Prediction\n",
    "The ensemble combines predictions from two models:\n",
    "\n",
    "CatBoost Model (ml_model): Predictions are made directly on the validation and test sets.\n",
    "Neural Network Model (nn_model): Predictions are made after scaling the validation \n",
    "and test sets using a pre-fitted scaler. The .ravel() method ensures the predictions \n",
    "are flattened into a 1D array.\n",
    "The ensemble combines the predictions using weighted averaging:\n",
    "\n",
    "80% weight for CatBoost predictions.\n",
    "20% weight for neural network predictions.\n",
    "Validation Predictions\n",
    "The combined predictions for the validation set (val_ensemble) are stored \n",
    "in a new DataFrame (val_df) that includes the original validation data \n",
    "(retrieved using val_idx) and the ensemble predictions under the column \"pred\". \n",
    "This allows for further analysis or evaluation of the ensemble's performance on the validation set.\n",
    "\n",
    "Test Predictions and Submission\n",
    "The ensemble predictions for the test set (test_ensemble) are saved in a DataFrame \n",
    "(submission) with two columns:\n",
    "\n",
    "\"id\": The unique identifier for each test sample, assumed to be present in the test DataFrame.\n",
    "\"target\": The ensemble predictions.\n",
    "The submission DataFrame is exported to a CSV file named submission.csv, \n",
    "which can be used for competition submissions or further analysis. \n",
    "A confirmation message is printed to indicate the file has been saved successfully.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "617a5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe random_state=42 parameter in the train_test_split function is used to ensure reproducibility \\nof the data splitting process. Here\\'s a detailed explanation:\\n\\nWhy Use random_state?\\nThe train_test_split function in scikit-learn splits the dataset into subsets \\n(e.g., training, validation, and test sets) randomly. By default, \\nthe randomness is determined by a random seed that changes every time the function is called. \\nThis means that without specifying random_state, the split will be different each time you run \\nthe code, leading to different subsets of data.\\n\\nWhat Does random_state=42 Do?\\nThe random_state parameter sets the seed for the random number generator used in the \\nsplitting process. By setting random_state=42, you ensure that the random number generator \\nproduces the same sequence of random numbers every time the code is executed. \\nThis guarantees that the data split is consistent across runs, making the results reproducible.\\n\\nWhy 42 Specifically?\\nThe choice of 42 is arbitrary and has no special significance in the context of machine learning. \\nIt is often used as a convention or a \"magic number\" in examples and tutorials, \\npopularized by the book The Hitchhiker\\'s Guide to the Galaxy, where 42 is \"the answer to the \\nultimate question of life, the universe, and everything.\" You can use any integer value \\nfor random_state to achieve reproducibility.\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The random_state=42 parameter in the train_test_split function is used to ensure reproducibility \n",
    "of the data splitting process. Here's a detailed explanation:\n",
    "\n",
    "Why Use random_state?\n",
    "The train_test_split function in scikit-learn splits the dataset into subsets \n",
    "(e.g., training, validation, and test sets) randomly. By default, \n",
    "the randomness is determined by a random seed that changes every time the function is called. \n",
    "This means that without specifying random_state, the split will be different each time you run \n",
    "the code, leading to different subsets of data.\n",
    "\n",
    "What Does random_state=42 Do?\n",
    "The random_state parameter sets the seed for the random number generator used in the \n",
    "splitting process. By setting random_state=42, you ensure that the random number generator \n",
    "produces the same sequence of random numbers every time the code is executed. \n",
    "This guarantees that the data split is consistent across runs, making the results reproducible.\n",
    "\n",
    "Why 42 Specifically?\n",
    "The choice of 42 is arbitrary and has no special significance in the context of machine learning. \n",
    "It is often used as a convention or a \"magic number\" in examples and tutorials, \n",
    "popularized by the book The Hitchhiker's Guide to the Galaxy, where 42 is \"the answer to the \n",
    "ultimate question of life, the universe, and everything.\" You can use any integer value \n",
    "for random_state to achieve reproducibility.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "932d6d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped weights stats: 0.0 4.124704041658811e-06 1.0475797370322353 1.9997369501860132 2.0\n",
      "Strategy raw Sharpe: 5.47711575865524\n",
      "Adjusted Sharpe: 5.438658343003995\n",
      "Vol penalty: 1.0070711218135766 Return penalty: 1.0 Return gap: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected evaluation: use mapped weights and official formula =====\n",
    "def compute_strategy_stats(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute strategy daily returns and Sharpe (annualized).\n",
    "    weights: array-like positions in [0,2]\n",
    "    forward_returns, risk_free_rate: arrays aligned\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays\n",
    "    w = np.asarray(weights)\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "\n",
    "    # Strategy return per day: rf*(1 - w) + w * forward_returns\n",
    "    # Strategy excess over rf:\n",
    "    strat_ret = rf * (1.0 - w) + w * fr\n",
    "    strat_excess = strat_ret - rf   # == w * (fr - rf)\n",
    "    # annualized sharpe\n",
    "    mean = np.nanmean(strat_excess)\n",
    "    std = np.nanstd(strat_excess)\n",
    "    sharpe = (mean / (std + 1e-12)) * np.sqrt(252) if std > 0 else 0.0\n",
    "    # annualized vol of strategy returns\n",
    "    vol_ann = std * np.sqrt(252)\n",
    "    return {\n",
    "        'sharpe': sharpe,\n",
    "        'vol_ann': vol_ann,\n",
    "        'mean_daily_excess': mean,\n",
    "        'std_daily_excess': std,\n",
    "        'strat_ret_series': strat_ret,\n",
    "        'strat_excess_series': strat_excess\n",
    "    }\n",
    "\n",
    "def sharpe_penalty_official(weights, forward_returns, risk_free_rate):\n",
    "    \"\"\"\n",
    "    Compute adjusted Sharpe like the official metric:\n",
    "    - compute strategy sharpe\n",
    "    - compute market vol and strategy vol, form vol_penalty = 1 + max(0, strategy_vol/market_vol - 1.2)\n",
    "    - compute return_gap penalty like (max(0, (market_mean_excess - strat_mean_excess) * 100 * 252))**2 / 100 etc.\n",
    "    Returns adjusted_sharpe (float) and components.\n",
    "    \"\"\"\n",
    "    # strategy stats\n",
    "    stats = compute_strategy_stats(weights, forward_returns, risk_free_rate)\n",
    "    strat_excess = stats['strat_excess_series']\n",
    "    strat_sharpe = stats['sharpe']\n",
    "    strat_vol = stats['vol_ann']\n",
    "    # market stats\n",
    "    fr = np.asarray(forward_returns)\n",
    "    rf = np.asarray(risk_free_rate)\n",
    "    market_excess = fr - rf\n",
    "    market_mean_excess = ( (1 + market_excess).prod() ) ** (1.0 / len(market_excess)) - 1 if len(market_excess)>0 else 0.0\n",
    "    # fallback simpler mean if product fails\n",
    "    # but safer to use mean:\n",
    "    market_mean_excess = np.nanmean(market_excess)\n",
    "    market_std = np.nanstd(fr)\n",
    "    market_vol = market_std * np.sqrt(252) if market_std>0 else 1e-9\n",
    "\n",
    "    # volatility penalty\n",
    "    excess_vol = max(0.0, (strat_vol / (market_vol + 1e-12)) - 1.2)\n",
    "    vol_penalty = 1.0 + excess_vol\n",
    "\n",
    "    # return gap penalty (use squared scaled gap similar to demo code)\n",
    "    strat_mean_excess = np.nanmean(strat_excess)\n",
    "    return_gap = max(0.0, (market_mean_excess - strat_mean_excess) * 100 * 252)  # percent annualized gap\n",
    "    return_penalty = 1.0 + (return_gap**2) / 100.0\n",
    "\n",
    "    adjusted_sharpe = strat_sharpe / (vol_penalty * return_penalty + 1e-12)\n",
    "    return {\n",
    "        'adjusted_sharpe': adjusted_sharpe,\n",
    "        'strat_sharpe': strat_sharpe,\n",
    "        'vol_penalty': vol_penalty,\n",
    "        'return_penalty': return_penalty,\n",
    "        'strat_vol': strat_vol,\n",
    "        'market_vol': market_vol,\n",
    "        'return_gap': return_gap\n",
    "    }\n",
    "\n",
    "# ===== Use it on validation properly mapping raw preds to weights =====\n",
    "\n",
    "# val_ensemble is your raw ensemble prediction (unmapped)\n",
    "# First map to weights using your mapping function (or revised mapping)\n",
    "def robust_signal_to_weight(sig, lower=0.0, upper=2.0):\n",
    "    \"\"\"\n",
    "    Map raw signals to weights robustly using percentile clipping and stable scaling.\n",
    "    If distribution is degenerate, fallback to standard scaling.\n",
    "    \"\"\"\n",
    "    sig = np.asarray(sig)\n",
    "    lo = np.nanpercentile(sig, 5)\n",
    "    hi = np.nanpercentile(sig, 95)\n",
    "    if np.isclose(hi, lo):\n",
    "        # fallback: z-score and sigmoid mapping\n",
    "        sig_z = (sig - np.nanmean(sig)) / (np.nanstd(sig) + 1e-12)\n",
    "        # map z to [0,2] via logistic\n",
    "        w = 2.0 / (1.0 + np.exp(-sig_z))\n",
    "    else:\n",
    "        w = (sig - lo) / (hi - lo + 1e-12) * (upper - lower) + lower\n",
    "    return np.clip(w, lower, upper)\n",
    "\n",
    "# compute mapped weights\n",
    "val_weights = robust_signal_to_weight(val_ensemble)   # or pass val_cat/val_nn separately\n",
    "\n",
    "# compute official adjusted sharpe and components\n",
    "res = sharpe_penalty_official(val_weights, val_df['forward_returns'].to_numpy(), val_df['risk_free_rate'].to_numpy())\n",
    "\n",
    "print(\"Mapped weights stats:\", np.nanmin(val_weights), np.nanpercentile(val_weights,5), np.nanmedian(val_weights), np.nanpercentile(val_weights,95), np.nanmax(val_weights))\n",
    "print(\"Strategy raw Sharpe:\", res['strat_sharpe'])\n",
    "print(\"Adjusted Sharpe:\", res['adjusted_sharpe'])\n",
    "print(\"Vol penalty:\", res['vol_penalty'], \"Return penalty:\", res['return_penalty'], \"Return gap:\", res['return_gap'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cc6efc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m197/197\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 743us/step\n",
      "Optimized Ensemble Weights:\n",
      "{'best_ml_percentage': 1.0, 'best_nn_percentage': 0.0, 'best_rmse': 0.0038843041275603246}\n"
     ]
    }
   ],
   "source": [
    "# function to optimize ensemble weights\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "opt_ensemble_ml_pct = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]  # Example percentages for CatBoost in ensemble\n",
    "opt_ensemble_nn_pct = [round(1 - pct, 2) for pct in opt_ensemble_ml_pct]\n",
    "\n",
    "def optimize_ensemble_weights(ml_preds, nn_preds, y_true, ml_percentages, nn_percentages):\n",
    "    \"\"\"\n",
    "    Optimize ensemble weights for CatBoost and Neural Network predictions.\n",
    "    \n",
    "    Args:\n",
    "        ml_preds: Predictions from CatBoost model\n",
    "        nn_preds: Predictions from Neural Network model\n",
    "        y_true: True target values\n",
    "        ml_percentages: List of percentages for CatBoost in ensemble\n",
    "        nn_percentages: List of percentages for Neural Network in ensemble\n",
    "    \n",
    "    Returns:\n",
    "        dict: Best weights and corresponding RMSE\n",
    "    \"\"\"\n",
    "    best_rmse = float('inf')\n",
    "    best_weights = (0.0, 0.0)\n",
    "    \n",
    "    for ml_pct, nn_pct in zip(ml_percentages, nn_percentages):\n",
    "        ensemble_preds = ml_pct * ml_preds + nn_pct * nn_preds\n",
    "        rmse = mean_squared_error(y_true, ensemble_preds, squared=False)\n",
    "        \n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_weights = (ml_pct, nn_pct)\n",
    "    \n",
    "    return {\n",
    "        'best_ml_percentage': best_weights[0],\n",
    "        'best_nn_percentage': best_weights[1],\n",
    "        'best_rmse': best_rmse\n",
    "    }\n",
    "\n",
    "# apply the optimization\n",
    "ensemble_optimization_results = optimize_ensemble_weights(ml_model.predict(X), nn_model.predict(X_scaled).ravel(), y, opt_ensemble_ml_pct, opt_ensemble_nn_pct)\n",
    "\n",
    "# print the results\n",
    "print(f\"Optimized Ensemble Weights:\")\n",
    "print(ensemble_optimization_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) 1b +cat \n",
    "Mapped weights stats: 0.0 9.551500239995839e-05 0.9904980683117174 1.996815120109926 2.0\n",
    "Strategy raw Sharpe: 6.077702579657624\n",
    "Adjusted Sharpe: 5.766583097942549\n",
    "Vol penalty: 1.05395213706715 Return penalty: 1.0 Return gap: 0.0\n",
    "\n",
    "2) 1a +cat \n",
    "Mapped weights stats: 0.0 0.00010174980984208535 0.9694203817637703 1.9985528252921856 2.0\n",
    "Strategy raw Sharpe: 6.228093968694775\n",
    "Adjusted Sharpe: 5.894612189284663\n",
    "Vol penalty: 1.0565739982030415 Return penalty: 1.0 Return gap: 0.0\n",
    "\n",
    "3) 1a +1b +cat\n",
    "Mapped weights stats: 0.0 4.124704041658811e-06 1.0475797370322353 1.9997369501860132 2.0\n",
    "Strategy raw Sharpe: 5.47711575865524\n",
    "Adjusted Sharpe: 5.438658343003995\n",
    "Vol penalty: 1.0070711218135766 Return penalty: 1.0 Return gap: 0.0\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c23c741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# #  Competition-Compliant Inference Function\n",
    "# # ================================================================\n",
    "# _ml_model = best_cbc\n",
    "# _nn_model = nn_model\n",
    "# _scaler = scaler\n",
    "# _feat_cols = features\n",
    "\n",
    "# \"\"\"\n",
    "#     Check if is really necessary exchange from pl to pd and back to pl?\n",
    "#     pl.DataFrame (we convert to pandas inside)\n",
    "# \"\"\"\n",
    "# def predict(pl_df):\n",
    "#     \"\"\"Competition inference function.\"\"\"\n",
    "#     pdf = pl_df.to_pandas().fillna(0.0)\n",
    "#     for f in _feat_cols:\n",
    "#         if f not in pdf.columns:\n",
    "#             pdf[f] = 0.0\n",
    "#     Xp = pdf[_feat_cols].values\n",
    "#     Xp_scaled = _scaler.transform(Xp)\n",
    "#     pred_cat = _ml_model.predict(pdf[_feat_cols])\n",
    "#     pred_nn = _nn_model.predict(Xp_scaled, verbose=0).ravel()\n",
    "#     preds = ensemble_cat_pct * pred_cat + ensemble_nn_pct * pred_nn\n",
    "#     lo, hi = np.percentile(preds, [5, 95])\n",
    "#     weights = np.clip((preds - lo) / (hi - lo + 1e-9) * 2.0, 0, 2)\n",
    "#     return pd.DataFrame({\"prediction\": weights.astype(\"float32\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4afa19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# #  Competition-Compliant Inference Function\n",
    "# # ================================================================\n",
    "# _ml_model = best_cbc\n",
    "# _nn_model = nn_model\n",
    "# _scaler = scaler\n",
    "# _feat_cols = features\n",
    "# _history_returns = list(train.loc[val_idx, 'forward_returns'].iloc[-VOL_WINDOW:].tolist())\n",
    "\n",
    "# def predict(pl_df: pl.DataFrame) -> float:\n",
    "#     \"\"\"Competition inference function - returns single float allocation.\"\"\"\n",
    "#     global _history_returns\n",
    "    \n",
    "#     # Convert Polars to Pandas and handle missing values\n",
    "#     pdf = pl_df.to_pandas().fillna(0.0)\n",
    "    \n",
    "#     # Ensure all required features are present\n",
    "#     for f in _feat_cols:\n",
    "#         if f not in pdf.columns:\n",
    "#             pdf[f] = 0.0\n",
    "    \n",
    "#     # Get features in correct format\n",
    "#     X_features = pdf[_feat_cols].values\n",
    "#     X_scaled = _scaler.transform(X_features)\n",
    "    \n",
    "#     # Make predictions from both models\n",
    "#     pred_cat = _ml_model.predict(pdf[_feat_cols])[0]  # Get first prediction\n",
    "#     pred_nn = _nn_model.predict(X_scaled, verbose=0).ravel()[0]  # Get first prediction\n",
    "    \n",
    "#     # Ensemble prediction\n",
    "#     pred = ensemble_cat_pct * pred_cat + ensemble_nn_pct * pred_nn\n",
    "    \n",
    "#     # Estimate rolling volatility for scaling\n",
    "#     vol_est = np.std(_history_returns) if len(_history_returns) > 1 else 1e-3\n",
    "    \n",
    "#     # Scale prediction to allocation with volatility adjustment\n",
    "#     allocation = float(np.clip((best_k * pred) / (vol_est + 1e-9), 0, 2))\n",
    "    \n",
    "#     # Update history for rolling volatility estimation\n",
    "#     if 'lagged_forward_returns' in pl_df.columns:\n",
    "#         try:\n",
    "#             _history_returns.append(float(pl_df['lagged_forward_returns'][0]))\n",
    "#         except:\n",
    "#             _history_returns.append(0.0)\n",
    "#     else:\n",
    "#         _history_returns.append(0.0)\n",
    "    \n",
    "#     # Keep only last VOL_WINDOW entries\n",
    "#     _history_returns = _history_returns[-VOL_WINDOW:]\n",
    "    \n",
    "#     return allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ca07cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NEXT STEPS, IMPORTANT FOR IMPROVEMENT:\n",
    "\n",
    "Stronger feature scaling\n",
    "\n",
    "PCA optional\n",
    "\n",
    "Rolling retrain or time-based CV for robustness out of sample\n",
    "\n",
    "Optimization of the mix (CatBoost vs NN) to dynamically find the optimal weight based on your adjusted Sharpe\n",
    "Eventually to be extended to more models in the ensemble\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a2cf25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================================================================\n",
    "# # Kaggle Evaluation Server / Local Submission\n",
    "# # ================================================================\n",
    "\n",
    "# if KAGGLE_ENV:\n",
    "#     # Kaggle competition environment\n",
    "#     server = kdeval.DefaultInferenceServer(predict)\n",
    "    \n",
    "#     if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#         server.serve()\n",
    "#     else:\n",
    "#         server.run_local_gateway((str(DATA_DIR),))\n",
    "        \n",
    "# else:\n",
    "#     # Local environment - generate submission file\n",
    "#     print(\"🔧 Local mode - generating submission file...\")\n",
    "    \n",
    "#     # Generate predictions for test set\n",
    "#     test_pred_cat = best_cbc.predict(X_test)\n",
    "#     test_pred_nn = nn_model.predict(scaler.transform(X_test), verbose=0).ravel()\n",
    "#     preds = ensemble_cat_pct * test_pred_cat + ensemble_nn_pct * test_pred_nn\n",
    "    \n",
    "#     # Apply same scaling logic as validation\n",
    "#     test_exposures = np.clip(best_k * preds, 0, 2)\n",
    "    \n",
    "#     # Apply smoothing like in the working example\n",
    "#     alpha = 0.8\n",
    "#     smoothed_allocation = []\n",
    "#     prev = 0.0\n",
    "#     for x in test_exposures:\n",
    "#         s = alpha * x + (1 - alpha) * prev\n",
    "#         smoothed_allocation.append(s)\n",
    "#         prev = s\n",
    "#     smoothed_allocation = np.array(smoothed_allocation)\n",
    "    \n",
    "#     # Create submission\n",
    "#     submission = pd.DataFrame({\n",
    "#         'date_id': test['date_id'],\n",
    "#         'prediction': smoothed_allocation.astype('float32')\n",
    "#     })\n",
    "    \n",
    "#     submission.to_csv('submission_ensemble.csv', index=False)\n",
    "#     print(\" Saved submission_ensemble.csv\")\n",
    "#     print(f\" Prediction range: [{smoothed_allocation.min():.4f}, {smoothed_allocation.max():.4f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
